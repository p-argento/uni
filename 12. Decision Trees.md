[[DM Classification]]

# Decision Trees Introduction
---
Split until the splitting attributes directly leads to a specific class, in a sense that it does not need to be split again to find a specific class.
For example, if 'home owner=yes' then the class is for sure 'no'.

![[Pasted image 20231122113533.png|400]]

> Applied typically for tabular data.
	When the slip is not binary, who tells me what is the right split?
	There could be more than one tree that fits the same data.
	Use greedy algorithm, but it is not possible to grasp the real optimum.


# Decision Tree Induction
---
There are many algorithms to build the tree:
1. Hunt
2. CART
3. ID3, C4.5
4. SLIQ,SPRINT

## Hunt's Algorithm



## Design Issues of Decision Tree Induction



## Different Splits
Depends on attribute types
1. binary
2. nominal
3. ordinal
4. continuous
Depends on number of ways to split
1. 2-way split
2. multi-way split

## Finding the Best Split
1. Compute impurity measure (P) before splitting
2. Compute impurity measure (M) after splitting
	1. in particular, the weighted impurity of each children
3. Choose the attribute test condition that produces
	1. highest gain (G=P-M)
	2. (or) lowest impurity after splitting (M)

## Measures of Impurity
Which one
1. GINI
2. Entropy
	1. Information Gain
	2. Gain ratio
3. Misclassification Error

## Comparison among Impurity Measures



## Stopping Criteria for Tree Induction



