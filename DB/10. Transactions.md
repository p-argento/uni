This is about the third part
1. permanent memory manager, buffer manager and storage structures
2. query optimizer and access plan manager
3. transaction management and concurrency manager <--

## Intro
Meaning how to deal with failures and concurrency.
Aborting means not only deleting a transaction, but also deleting all of its effects.

A transaction is a sequence of operations with the following properties (ACID).
The crucial part is the effect.
1. atomicity
	1. if a transaction fails, it has no effect on the db
	2. (or) only if a transaction is a success, then it has a success
		1. meaning that if there is an effect, the transaction was successful
	3. (or) there is success or there is no effect
	4. it means that a transaction is completed or it has never happened
	5. one of the implementations is the LOG
2. durability
	1. if the dbms has a crash after a transaction was completed, all the effects of the transactions are still there
	2. means that completed transactions are resilient to failures
	3. one of the implementations is the REDO 
3. isolation
	1. when many transactions are executed concurrently, then the final effect must be the same as if it were executed alone
4. consistency (less important)
	1. means that the db implements some contraints about consistence
	2. the fact that the db will maintain some properties stated at the beginning

Atomicity and durability have to do with failures.
But atomicity is related to failure BEFORE commit (no effect retained), while durability is related to failure after commit (no effect is lost).
Implementing atomicity is very easy, you move the transaction from the buffer only after commit, meaning "please be lazy". And durability is the opposite, meaning "please be eager". So it is not possible to use the same buffer and obtain both of them.

It is important to remember that the db does not operate on disk, but on buffer.

We distinguish
1. transaction failure
	1. illegal transaction that must be terminated
	2. for example updating a record with negative age
	3. most common type of failure and easier to deal with
2. system failure (or crash)
	1. means that the hardware or software have some problem and you cannot be sure that the content of the buffer is coherent
	2. the system must restart
	3. however the persistent store is safe
3. media failure (or disaster)
	1. a system failure that is so sever that affects persistent store, or that you cannot be sure of the content of the disk
	2. very rare, but you must be able to recover

How to protect the system from failures?
1. periodic backup
	1. Take a periodic backup of the content of the disk.
2. log file
	1. whenever you do something on the buffer, record every operation, usually kept in a different location than the buffer, so that if the buffer is lost, then you can still recover the log
	2. in case of error, the log is used to rebuild a coherent state from the last backup

What is written on the log file?
1. (T, begin)
	1. like transaction 34 begins
2. for each (T, write, P, BeforeImage, AfterImage)
	1. transaction T has done a write operation on page P (page id on disk), use the value BeforeImage before the operation and AfterImage
	2. BeforeImage and AfterImage are useful for Atomicity and Durability
3. (T,commit) or (T, abort)
	1. the act of writing commit is what makes the transaction persistent
	2. wait for commit before giving the money

## restart
How to restart after a system failure?
We use the UNDO-REDO Protocol to ensure atomicity and durability
1. Read the log and divide
	1. redo list, meaning the transactions with commit
	2. undo list, meaning the transactions without commit
		1. they must be undone
2. going backwards in the log from the last one, UNDO all the operations of all transactions in the undo list
	1. means rewrite on the page P the value of BeforeImage
	2. you cannot know if it was already flushed or not, it might be useless, but it is not a problem to undo an operation that has not been done
		1. this fact is called "idempotent.", in mathematics and computer science, an operation is called idempotent if performing it multiple times has the same effect as performing it once
3. run a REDO phase, start from the beginning and move forwards to redo all the operations
	1. going forward is crucial

For this reason, every systems take a checkpoint, meaning that it flushes all the dirty pages to disk. With this idea, the redo phase is much faster.
The simplest way of taking a checkpoint is "stop the world", meaning that the buffer manager stops the operations.
So we usually do the flushing together with the usual operations, but a problem arise. What happens to the operations done between "BeginCheckpoint(BC)" and "EndCheckpint(EC)"? For sure, what's before BC is sure to be on disk.
Then why do care about the EC? Because if there's no EC it means that there is a problem and the checkpoint is not effective. 

![[Pasted image 20240521224107.png|400]]

If you find the EC, like in t1, it means that all the operations completely before BC are on disk.
But what if the commit of a transaction t2 is after the EC. It goes to the REDO list, because the commit is found in the log.
And t3, that has no commit, is in the UNDO list. What about the operations that are part of transaction t3? All the operations must be undone. We undo since we are in doubt.
t4 is undo for sure.
t5, began after the EC and finishes after the EC, but we have the commit in the log because the fail happened later. So it is in the REDO.

So restart has 3 phases
1. (create the lists) scan the log in the time between the BC and the FAIL
	1.  if you find a trace of an operation but do NOT find a commit, put it in the UNDO list
	2. for every transaction of which I find an operation and a commit, put it in the REDO list
2. (undo) scan backwards the log, undoing the operations in the UNDO list,
	1. at the end of this phase, I solved atomicity
3. (redo) scan forward, redoing the operations in the REDO list
	1. at the end of this phase, I solved durability

When you redo a transaction, you only redo only its operations after the BC.
When you undo a transaction, you undo all of its operations.

Both undo and redo are executed in the buffer, for reasons of speed.
If there is a crash during the restart, no problem, because undo-redo are idempotent.

In the end, the persistent memory is composed by both the memory and the log.

Kinds of failures and recovery
1. transaction failure
	1. UNDO Log is enough
	2. it is undone in the buffer, but what is read is the buffer and eventually the undone operations will go into disk.
2. system failure
	1. UNDO-REDO procedure
3. disaster
	1. instead of starting the UNDO-REDO in the last checkpoint, you start from the last backup

Remember that everything always happen on the buffer.
Working on the buffer allows to restart much faster.
And what happens is always in the log.

Undo, redo: which one for durability and which one for atomicity?
Undo for atomicity and redo for durability.

The backup is usually done with no running transactions (cold backup). It mean that we can skip the undo phase, and do just the redo phase. Otherwise, the transactions without the commit must be undone.


*watched until 26 min of 08-05*






## Protocols
The protocol described is the standard.
But some systems avoid the redo and undo by constraining the buffer.
*NoUndo-Redo Protocol*
If you pin every transaction that you modify, this is a NoUndo situation because there is no need to undo anything. The ones that are not pinned cannot go out of the buffer.
(in the undeo-redo, you unpin immediatly to flush the page and free the buffer)
It is the NoSteal (or Pin) Policy, based on pinning every buffer until commit.
When there is a restart there is no need to undo anything since the failure does not go into the disk.

The problem is that no transaction cannot modify more than the pages than the buffer memory can contain. It is just a problem in theory, because in practice you just 

It has a big size issue, because once you have pinned everything.
But what we want is to have something like "There is no such thing as data is too big".
That's why this approach is not used.

*NoRedo Protocol*
Called Force Policy, where flushing of buffers is forced before commit.
I am giving the buffer manager more space.
Here we have a time problem.

Faster restart of system failure, but at the cost of slower runtime with many ...

This may be used. Why?

*Undo-Redo*
Most dbms prefers this one.
Steal:
NoForce: the buffer manager is not forced to...


When you adopt a NoUndo you differ updates until after the commit, while on Undo you can do update.
"Write ahed log rule" means that you must first write the log and then you can unpin the buffer. Why? Otherwise you may lose the buffer. First flush the log then the buffer.
Some systems flush only the log on a specific disk.





# Concurrency control

Consider 2 ATMs running in parallel.
If two transactions read at the same time and update at the same time, then I will loose the first update.

Types of executions
1. serial execution
	1. all the operations of t1 comes first
	2. but it is impractical
2. interleaved execution

Why serial is impractical?
You need parallelism. Even in a physical bank there are people serving clients in parallel. How to avoid overdraft?
We want an effect that is the one of serial execution, but with a concurrent execution.
I need a manager that keeps track of all the executions.

## Strict 2PL Protocol

The Lock Protocol is exclusive. If i want to go the bathroom, I have the key and no one else can.
In CS, read and write are conflicting, and cannot be done at the same time.

Serialized ability comes after...



If you know that you will update, you need to use:
`SELECT FOR UPDATE`

