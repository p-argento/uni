This is about the third part
1. permanent memory manager, buffer manager and storage structures
2. query optimizer and access plan manager
3. transaction management and concurrency manager <--

Meaning how to deal with failures and concurrency.
Aborting means not only deleting a transaction, but also deleting all of its effects.

## ACID
A transaction is a sequence of operations with the following properties (ACID).
The crucial part is the effect.
1. atomicity
	1. if a transaction fails, it has no effect on the db
	2. (or) only if a transaction is a success, then it has a success
		1. meaning that if there is an effect, the transaction was successful
	3. (or) there is success or there is no effect
	4. it means that a transaction is completed or it has never happened
	5. one of the implementations is the LOG
2. durability
	1. if the dbms has a crash after a transaction was completed, all the effects of the transactions are still there
	2. means that completed transactions are resilient to failures
	3. one of the implementations is the REDO 
3. isolation
	1. when many transactions are executed concurrently, then the final effect must be the same as if it were executed alone
4. consistency (less important)
	1. means that the db implements some contraints about consistence
	2. the fact that the db will maintain some properties stated at the beginning

Atomicity and durability have to do with failures.
But atomicity is related to failure BEFORE commit (no effect retained), while durability is related to failure after commit (no effect is lost).
Implementing atomicity is very easy, you move the transaction from the buffer only after commit, meaning "please be lazy". And durability is the opposite, meaning "please be eager". So it is not possible to use the same buffer and obtain both of them.

It is important to remember that the db does not operate on disk, but on buffer.

## Failures
We distinguish
1. transaction failure
	1. illegal transaction that must be terminated
	2. for example updating a record with negative age
	3. most common type of failure and easier to deal with
2. system failure (or crash)
	1. means that the hardware or software have some problem and you cannot be sure that the content of the buffer is coherent
	2. the system must restart
	3. however the persistent store is safe
3. media failure (or disaster)
	1. a system failure that is so sever that affects persistent store, or that you cannot be sure of the content of the disk
	2. very rare, but you must be able to recover

How to protect the system from failures?
1. periodic backup
	1. Take a periodic backup of the content of the disk.
2. log file
	1. whenever you do something on the buffer, record every operation, usually kept in a different location than the buffer, so that if the buffer is lost, then you can still recover the log
	2. in case of error, the log is used to rebuild a coherent state from the last backup

What is written on the log file?
1. (T, begin)
	1. like transaction 34 begins
2. for each (T, write, P, BeforeImage, AfterImage)
	1. transaction T has done a write operation on page P (page id on disk), use the value BeforeImage before the operation and AfterImage
	2. BeforeImage and AfterImage are useful for Atomicity and Durability
3. (T,commit) or (T, abort)
	1. the act of writing commit is what makes the transaction persistent
	2. wait for commit before giving the money

## Restart
How to restart after a system failure?
We use the UNDO-REDO Protocol to ensure atomicity and durability
1. Read the log and divide
	1. redo list, meaning the transactions with commit
	2. undo list, meaning the transactions without commit
		1. they must be undone
2. going backwards in the log from the last one, UNDO all the operations of all transactions in the undo list
	1. means rewrite on the page P the value of BeforeImage
	2. you cannot know if it was already flushed or not, it might be useless, but it is not a problem to undo an operation that has not been done
		1. this fact is called "idempotent.", in mathematics and computer science, an operation is called idempotent if performing it multiple times has the same effect as performing it once
3. run a REDO phase, start from the beginning and move forwards to redo all the operations
	1. going forward is crucial

For this reason, every systems take a checkpoint, meaning that it flushes all the dirty pages to disk. With this idea, the redo phase is much faster.
The simplest way of taking a checkpoint is "stop the world", meaning that the buffer manager stops the operations.
So we usually do the flushing together with the usual operations, but a problem arise. What happens to the operations done between "BeginCheckpoint(BC)" and "EndCheckpint(EC)"? For sure, what's before BC is sure to be on disk.
Then why do care about the EC? Because if there's no EC it means that there is a problem and the checkpoint is not effective. 

![[Pasted image 20240521224107.png|400]]

If you find the EC, like in t1, it means that all the operations completely before BC are on disk.
But what if the commit of a transaction t2 is after the EC. It goes to the REDO list, because the commit is found in the log.
And t3, that has no commit, is in the UNDO list. What about the operations that are part of transaction t3? All the operations must be undone. We undo since we are in doubt.
t4 is undo for sure.
t5, began after the EC and finishes after the EC, but we have the commit in the log because the fail happened later. So it is in the REDO.

So restart has 3 phases
1. (create the lists) scan the log in the time between the BC and the FAIL
	1.  if you find a trace of an operation but do NOT find a commit, put it in the UNDO list
	2. for every transaction of which I find an operation and a commit, put it in the REDO list
2. (undo) scan backwards the log, undoing the operations in the UNDO list,
	1. at the end of this phase, I solved atomicity
3. (redo) scan forward, redoing the operations in the REDO list
	1. at the end of this phase, I solved durability

When you redo a transaction, you only redo only its operations after the BC.
When you undo a transaction, you undo all of its operations.

Both undo and redo are executed in the buffer, for reasons of speed.
If there is a crash during the restart, no problem, because undo-redo are idempotent.

In the end, the persistent memory is composed by both the memory and the log.

Kinds of failures and recovery
1. transaction failure
	1. UNDO Log is enough
	2. it is undone in the buffer, but what is read is the buffer and eventually the undone operations will go into disk.
2. system failure
	1. UNDO-REDO procedure
3. disaster
	1. instead of starting the UNDO-REDO in the last checkpoint, you start from the last backup

Remember that everything always happen on the buffer.
Working on the buffer allows to restart much faster.
And what happens is always in the log.

Undo, redo: which one for durability and which one for atomicity?
Undo for atomicity and redo for durability.

The backup is usually done with no running transactions (cold backup). It mean that we can skip the undo phase, and do just the redo phase. Otherwise, the transactions without the commit must be undone.


*watched until 26 min of 08-05*

[[DB exercise on transactions]]


## Protocols
The protocol described is the standard.
But some systems avoid the redo and undo by constraining the buffer.

### NoUndo-Redo Protocol
If you pin every transaction that you modify, this is a NoUndo situation because there is no need to undo anything. The ones that are pinned cannot go out of the buffer.
(in the undeo-redo, you unpin immediatly to flush the page and free the buffer)
It is the NoSteal (or Pin) Policy, based on pinning every buffer until commit.
When there is a restart there is no need to undo anything since the failure does not go into the disk.

The problem is that no transaction cannot modify more than the pages than the buffer memory can contain. It is just a problem in theory, because in practice you just 

It has a big size issue, because once you have pinned everything.
But what we want is to have something like "There is no such thing as data is too big".
That's why this approach is not used.

### NoRedo Protocol
Called Force Policy, where flushing of buffers is forced before commit.
I am giving the buffer manager more space.
Here we have a time problem.

Faster restart of system failure, but at the cost of slower runtime with many ...

This may be used. Why?

### Undo-Redo
Most dbms prefers this one.
NoForce: the buffer manager is not forced to flush.


When you adopt a NoUndo you differ updates until after the commit, while on Undo you can do update.
"Write ahed log rule" means that you must first write the log and then you can unpin the buffer. Why? Otherwise you may lose the buffer. First flush the log then the buffer.
Some systems flush only the log on a specific disk.





# Concurrency control

Consider 2 ATMs running in parallel.
If two transactions read at the same time and update at the same time, then I will loose the first update.

When different transactions work on the same data at the same time.
In order to avoid this problem, first we need to define the risk and then build a concurrency manager.

Types of executions
1. serial execution
	1. all the operations of t1 comes first
	2. but it is impractical
2. interleaved (or parallel) execution
	1. means that the operations of the transactions are done at the same time.

Definition.
A concurrent execution of a set of transactionsÂ {$T_1,...,T_j$} is a serial if, for every pair of transactions $T_i,T_j$ all the operations of $T_i$ are executed before any of the operations of $T_j$ or vice versa.
Remember that a set has no order a priori.

Why serial execution is impractical?
You need parallelism. Even in a physical bank there are people serving clients in parallel. How to avoid overdraft?
We want an effect that is the one of serial execution, but with a concurrent execution.
I need a manager that keeps track of all the executions.

Definition.
A concurrent execution of a set of transactions ${T_1,...,T_n}$ is serializable if it has the same effect on the database as some serial execution of the same transaction.

I need a concurrency manager (or scheduler).
The transactions happen at the same time(?) but the *effect* (this is the keyword) is serialized. We want the speed of concurrency, but the effect of serializability.
The DB Manager ensures that, even with multiple transactions, the effect is serialized. All that it needs is begin-transaction and end-transaction.

Establish a protocol between the transactions T and the scheduler S.
A protocol is an abstract way of describing a family of concrete algorithms, basically a set of rules. If the protocol obey these rules, than we can prove some theorems.

The following two rules are called the "Lock Protocol".
1. whenever a transaction wants to read or write a page, before doing that, the transaction asks the permission to the scheduler.
	1. the permission is called "lock", it is a metaphor for the permission of going to the bathroom, that maybe already occupied by someone else.
	2. If i want to go the bathroom, I have the key and no one else can.
2. the scheduler will never give conflicting permissions to different transactions.
	1. the scheduler has a compatibility matrix, meaning that it checks if the two operations are compatible
	2. an example of this matrix (but not necessarily the only one) is
		1. read permissions are not conflicting
		2. read and write are conflicting
		3. write and write are conflicting
		4. but actually DBMS have many more operations

Then, we have the "2 Phase Lock Protocol" (or 2PL Protocol).
Here, you also have another rule.
3. once a transaction get the permission, this transaction will not release the permission until commit or abort.

So, the rules of the protocol are
1. permission before acting
2. never give conflicting permissions
3. never release permission before commit

You get serializability because the first transaction will release the lock only when finished. If the same transaction asks read and write on the same page, this is not conflicting. They are conflicting only if two transactions are acting on the same page (or other data object).

What is the difference between a protocol and an algorithm?

The scheduler have 3 possibilities to answer the transaction's permission.
1. just say no (no-implementation)
	1. you can choose the behaviour of the transaction, for example "active waiting", meaning that in the meantime other things can be done and then the permission will be asked again later.
2. freeze the transaction (standard-implementation)
	1. the transaction enter a "waiting-state", when the lock is available again, it is awakened (or notified) and the permission is asked again.
3. whenever a permission is denied, the transaction is aborted (killing-implementation)
	1. brutal and definitly standard
These are all implementations of the second rule.
DO NOT confuse the implementation with the rule.


