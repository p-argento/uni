## DBMS Architecture

![[Pasted image 20240521093251.png]]

Most DBMS share the same architecture.
It is divided in
1. relational engine
2. storage engine

The storage engine is composed by
	1. access method manager (like open/scan a file/index)
	2. storage structures manager (like hash tables, indexes)
	3. buffer manager (similar to the OS)
	4. concurrency manager (solve concurrency problems)
	5. transaction manager (assure atomicity and durability of transactions)

On top of this storage engine, SQL is built.
It is executed in the relational engine, composed by
1. query manager
	1. query optimizer, a tool that transform a query into an optimize query (the kernel of sql)
	2. access plan manager, execute it using access methods

We will study in order
1. permanent memory manager, buffer manager and storage structures
2. query optimizer and access plan manager
3. transaction management and concurrency manager

# 1. persistent memory manager, buffer manager and storage structures
## Persistent Memory Manager

We start from memory, we distinguish
1. persistent memory (slow, big, robust)
2. main memory (fast, small, volatile)
Whenever there is a system crash, the content of the main memory is lost.
If you want to read, analyze any data, it must be done in the main memory.
Data must be first loaded from persistent memory to main memory.

Persistent memory comes into 2 shapes.
1. Hard Disk Drives (HDD) with rotating disk.
2. Solid State Drives (SDD) that is more expensive and fast.
Most databases in the world are on disk. But they are incredibly slow.
To do an entire spin, it take 5 ms, basically forever.

Main memory is random access, meaning that you move 1 byte at a time, taking microseconds.
While in HDD, you need to read a page (4kbyte) at a time, taking 10 ms.

Whenever you analyze an algo that runs on persistent storage, you need to measure in pages. How many pages do I have to read in order to do something?

Instead of persistent memory or HDD or SSD, we will just say "disk".

The permanent memory manager is just implementing the basic operations on different OS.

## Buffer Manager

![[Pasted image 20240521101528.png]]

In order to operate on a page of data, you must first load the page from disk into main memory. But they have different orders of magnitude.
The buffer decides which pages to load and when.
The frames are data structures that have PageID and FrameID.
For each frame, the buffer manger contains
1. PinCount
	1. is the number of clients who are currently using that page
	2. when a transaction it is asked, the buffer manager decides to free a frame to load a new page if it was not already there
	3. like a pin to say "do not move this page", meaning that it can be removed only if no one is using it
2. dirty pin
	1. means that somebody as modified this page, so it is different from the one present on disk
	2. when it happens, the buffer must first "flush", updating the disk, then the page is available for eviction (removing from buffer)
3. page

The methods are
1. getAndPinPage
2. unpinPage
3. setDirty
4. flushPage

We study
1. replacement algo
2. flush algo

The most common "replacement algo" is Least Recently Used (LSU) meaning that the buffer manager keeps a timestamp for each page and it will choose to remove the older if needed.

The "flushing policy" is what should the buffer manager do when the dirty bit is one.
1. (lazy policy)i can flush only when i need space 
	1. it minimizes the number of flashes
2. (eager policy) as soon as i see a dirty bit, i flush
	1. i like to keep my buffer clean
3. (checkpoint approach) every 5 minutes (or different), the buffer manager scans all the pages, and, if there's a dirty bit, it flushes that page.
	1. this is the most common

The essential point is that you do not operate directly on disk, but it must be first moved into the buffer.
Note that you are updating a copy of the page that is in the volatile memory, and those changes might be lost. We will see the LOG manager, that is eager.

Accessing the buffer has zero cost.
In the algos, we assume that there are no pages pinned in the buffer, meaning that we need to load all the pages.

How do we represent tables on disk?
We assume that every record is stored in a page, like 100 records in a page.
We assume that every record is uniquely identified by a Record ID, 
RID=(PageID, Position within the page)
Once you know the RID, you know how to find that record. This RID is persistent.

## Storage Structures
Whenever you have a table, you must decide how you will store this table on disk.
Types
1. Heap organization
	1. whenever you add a new record, it is added at the end of the file
	2. it is the only organizer attribute-neutral, meaning that it does not privilege any attribute
	3. best for a lot of insertions
2. sequential organization
	1. sorted by an important attribute (like the surname)
	2. best if never update but do a lot of range searches
3. hash organization
	1. best if main operation is equality searches
4. tree organization
	1. intermediate, quite good every time

A data organization is a rule that tells the storage manager where to place a new record. Each one has its own strength, no optimal solution.
After you have chosen one optimal, then you can add more secondary organizations.

We use a cost model to decide.
We measure time in terms of pages, "Npag(R)" is the unit of measure.
For each organization, we will ask how much does it cost to perform
1. equality and range search
	1. means retrieve all data where name=giorgio or whose age is between smt
3. update, insertion (this is the main focus), delete

### Heap organization.
This is the most common organization used by dbms.
It is the fastest organization for insertion, since new records are just put at the end of the file. It is (almost) the only one where you read 1gb of data for 1gb of files, while for other organizations you need a little bit of empty space, like 1.2 gb.

Heap is the worst for equality search. And since the equality is probably the most common operations, why are they so popular? Because equalities can be speed up using indexes. The most common approach is using the heap and then add indexes. Adding indexes is not ideal as having the best organization, but it is almost as good.
The advantage is to build an index only later and only if a problem arise. If no problem arise, maybe there is not even the need for a better organization or an index.

It is ideal when
1. insertion is the dominant  operation
	1. like in log files, where we just keep track, but almost never search
2. we almost never do equality and range search, but we do massive search
	1. meaning that we read from top to end, like for computing the average of all values, also called table scan
	2. the heap gives the cheapest table scan, when you read all the records

The cost of equality is
1. (average) NPag(R) / 2
2. (worst) Npag(R)
The average is only for a unique key, otherwise it is always worst because we need to scan the whole table R.

The cost of inserting a tuple is just 2Pag(R).
Why 2? Read the file and write it.
We can do even better, pinning a page that is used often in the buffer, and then unpin it for flushing only when the page is full. Then, start a new blank page pinned for the new entries. Now the cost is 1 write for 100 records, so we pay NPag/NRec, supposed that every page contains 100 records.

The second faster organization for insertion is hash, where the cost is 2.2


### Sequential Organization
Keep data sorted on an attribute.
Excellent for range search.
If you keep data sorted, finding the range is very easy because data are sequential. This works of course only on a specific attribute.

It is also quite useful for equalities.
You can interpolate within a range and try to guess the position.
Of course this depends on the distribution and multiple attempts might be required.

It has the same cost of heap for massive search.

This is very used in OLAP. But almost never in transanctions system.
Insertion is incredibly expensive, because to insert in the middle of a file, you need to move down all the other records.

It is called "static", because it is good for data that is very rarely updated.
Both heap and sequential are slow, one for search and the one for insertion.

### Hashing
Choose an attribute, a size for the file (say 100), define a hash function (that gives back a number between 0 and the size). Then compute the hashing on that attribute for every record to decide where to store that record, in which page.
A good hash function has the property that even if the input has a very bad distribution, the output is a balanced distribution.

The basic idea is that if you have 100.000 records and every page has the capacity of 100 records, then you choose a density that is lower than 1, like 0.8 (or 80%).
$d = N/ (M\cdot c)$
$M=N/(d\cdot c)$ and not $M=N/c$
Thanks to this generosity of number of pages, every page will contain $d\cdot c$ meaning that there is still enough space, avoiding the problem of overflow, which means reading two pages instead of one. Even with a uniform distribution, every page might not get the same number of records.

By keeping the density 80% and overflow probability of 10%, then the average of pages that I need to read plus the overflow is just 1.1
The cost is essentially 1.

To sum up, hashing is perfect for equalities. Computing the hash function, I can immediately get the page where the record is. The cost is 1, so optimal, cannot get better than this.
And for range search? The hash organization does NOT help in anyway. I should compute the hash of every record, but we do not know them, so it is better to use TableScan. However, for TableScan, this is even worse than heap, because here we need to use more space (1.25) for every page, meaning longer scan. 

Insertion cost is 2, which is excellent. Compute hash function, read page, write page.
However, if insertion is common, then we could finish the space on the pages. Then it might be needed to rebuild everything with bigger pages (say 200 records) and recompute the hashing. Not the best organization then. So, if data is not static, this is not a good idea.


### Tree

![[Pasted image 20240530112329.png]]

A B-Tree is a generalization of a binary tree, where in every node you have something like 500 values, then in the following node we have the values that are between the previous values. The same idea of a binary tree, but the tree is not binary.
Every node as a big number of children. In this way, you can have lot of numbers accessed with a small number of levels.

When you organize your tree, you usually keep the root in the buffer, so that the tree is available at zero cost.

How many pointers per page? It depends on the size of the page on disk. Basically, we want every node to correspond to a page on disk.
In the leaves of the btree you put the keys and the entire record, while in the intermediate node you put only the values of the key.
In the picture, 16* means the value 16 of the key plus all the other attributes.

It supports both equalities and range search.
It is similar to sequential organization, but the difference is that the tree is dynamic.
When you need to insert a new record, it can be done easily, and if necessary, you can just create a new page and redistribute come of the records. (not very clear)
Observe that on average each page is full at 80%, but it is never sparse,  because there is at least 66%.
Once the parent node is full, you can just create a new parent.
It grows not from the leaves, but from the root. The root are always balanced.
This technique is dynamic.
If you have lot of deletions, you can do the opposite, merging.

So, insertion is not optimal. The cost is 3, while with heap was 2.
For read, the cost for equality search is 2, while with hash was 1.
For range, it was ...
In massive search, there is 20% overhead.
In every game, it always arrive second, but it is always very close.
Maybe not optimal, but always good enough, especially range search.

### Secondary organization: Indexes
Remember that all these structures are built on a single attribute, but we always have more than one. So, we can use some secondary organization.
You must decide one primary, and later on you can have many secondary organizations.

An index is essentialy a table with just. 2 columns, the value of an attibute and the pointer to the record (RID) that allows to find the record. Important, the index is the data structure composed by many pairs of attribute's value and pointer (K,RID).

How to store the index on disk? It is organized as a BTree or as a Hash Table. The goal of an index is to make search faster, this is why we use want to use primary organizations that are fast in search. The standard choice is BTree, because it is dynamic, but if we do not care about range searches (like in fiscal codes), then hash tables might be used. This means the index is sorted with respect to the attribute.

Using an index of course is slower than using directly the BTree, because you need one step more to get that record.
Now, since space is quite cheap, you might be tempted to build an index for every attribute. However, during insertion or deletion you will need to update all the indexes created, and this means many more operations on disk.
Usually, every dbms automatically defines an index for every primary key. Why? Because the only reason not to have an index is slowing down insertion. But in case of the primary key, it actually means that we have to check first that there are no other keys and this can be done using the index, instead of the more expensive TableScan.

An interesting point is that every dbms also build an index on every foreign key.
Why? Assume the StudentIDs as foreign key in the Exam table, before deleting a student, we have 3 options:
1. no action
2. set null
3. cascade -> delete also the record in exams table related to that student
For this reason, whenever you delete a student, with an index on foreign keys, it is easier to find the records that are affected (if they exists). This means that it makes delete much faster.

Moreover, it is useful to use the index only when the selectivity factor of the query is low. The selectivity factor of a query is the percentage of records that are returned by that query. The smaller the better, also called strong, meaning that it cuts away a lot of stuff. Indexes are very useful with a strong selectivity factor.

Assume Npag=100, NRec=10.000 and a query where Sf=10%, then ExpectedRecords=1.000
Usually the buffer is much smaller than Npag.
There are two possibilities to answer a query
1. use the index
	1. when the number of records that you expect is lower than the number of pages NPag of the file
2. not use the index
	1. if you expect more records than pages, then the TableScan is just better
So indexes are very useful for queries with a very small selectivity factor.

### Sorting Files
All DBMS use the SortMerge algorithm.
They use a specific version of SortMerge that is very efficient.
Usually the cost is $Nlog_2N$
But we want something better, so instead of merging two records at a time, we merge 1.000 records, meaning $Nlog_{1000}N$ which is around $2\cdot NPag$
This means that we can consider it to be linear.
Whenever you implement something on a db, you really want it to be linear.



# 2. query optimizer and access plan manager

The query manager is composed by
1. query optimizer
2. access plan manager

![[Pasted image 20240521113056.png]]


Since we do not expect users to write their queries in a way that suggests the best strategy for ﬁnding the answer, it is responsibility of the *Query Manager* module of the system to ﬁgure out how to ﬁnd the most efﬁcient way to get the data the user wants.

Query Processing
1. syntactically and semantically correctness
2. transformation into a relational algebra query
3. logical transformation to standardize and simplify the query
4. select a strategy and the algorithms to perform the operations

The result of this process is the *physical query plan*: a tree of physical operators that implement algorithms to execute the relational algebra operators.

Each relational algebra operator can be implemented using several algorithms (physical operators).

For example, the join can be implemented using:
1. nested loops
2. page nested loops
3. index nested loops

## Access Plan

[[DB-query-execution.pdf]]


![[Pasted image 20240508144835.png]]
![[Pasted image 20240508144846.png]]
![[Pasted image 20240508144900.png]]
![[Pasted image 20240508144915.png]]

![[Pasted image 20240530130500.png]]