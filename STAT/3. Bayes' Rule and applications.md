## Correction of exercises at home

To prove that it not true, it is sufficient to find a counterexample.

First exercise.
![[Pasted image 20240223121451.png]]

Basically, under the assumption that the both coins have the same result in C, we can prove that the consequence is not true.
It shows that *pairwise independence is weaker than independent*.

Second exercise.

![[Pasted image 20240222142618.png]]
![[Pasted image 20240223122424.png]]

The events must be independent, not just pairwise independence.

## Application to ML Classifiers

![[Pasted image 20240223123053.png]]

Adding B does not change the probability of a certain event.
In data mining, B can be irrelevant for predicting a certain event.
However, testing in isolation is not enough, we must consider all the subset of available features to understand if it is predictive or not.

# Bayes Rule

## Example
![[Pasted image 20240223123807.png]]

$$\text{Chain Rule:}\quad P(C\cap+)=P(+|C)\cdot P(C)$$
$$\text{Law of total probability:}\quad P(+)=P(+|C)\cdot P(C)+P(+|C^c)\cdot P(C^c)$$
But $P(C)$ is not always know. In the example is the probability of having Covid.
This is true especially in DM.

![[Pasted image 20240223124546.png]]

## Bayes' Rule

![[Pasted image 20240223125301.png]]
The rule we obtained in the example is called Bayes Rule.
![[Pasted image 20240223124810.png]]

It can be expressed also without applying the law of total probability to the denominator. Meaning$$
P(C_i|A)=\frac{P(A|C_i)\cdot P(C_i)}{P(A)}
$$So:
1. $P(C_i)$ is called prior probability (unknown in the example)
2. $P(C_i|A)$ is called posterior (after testing positive, what is the probability of being positive)
	1. not easy to calculate

## Binary Classifiers in ML
![[Pasted image 20240223125316.png]]

Y hat is the predicted class, Y is the true class.

Usually accuracy, precision and recall are not known.
So we calculate those values on data that we have.




