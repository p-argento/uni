## Representational learning

![[Pasted image 20240627193229.png|300]]

Representation learning methods
1. allow a machine to be fed with raw data
2. automatically discover the representation needed for detection or classification

Multilayer Neural Network.
1. can solve any type of classification task involving nonlinear decision boundaries
2. hidden layers are intermediate layers between input and output layers
3. more activation functions are used such as sigmoid,  hyperbolic tangent,...

Level of abstraction.
The model will figure out the right level of abstraction for the information that is good to get the right output.
Deep neural networks can build up increasingly more complex levels of abstraction, like lines, parts, regions.

![[Pasted image 20240628171422.png]]

## Activation functions

![[Pasted image 20240628170545.png]]

instead of the logistic function, the most common activation functions are
1. hyperbolic tangent (TANH)
	1. which is similar to logistic but shifted to the range -1+1
2. rectified linear unit (RELU)
	1. linear with a cutoff at zero
	2. typically used inside hidden layers
3. soft exponential linear unit (SOFT-MAX)
	1. a soft version of the relu
	2. log(exp(x)+1)

![[Pasted image 20240628172104.png]]

The shape of the derivative is important because we want to use the gradient descent, and therefore make sure that the function is derivable in the entire domain.

## Backpropagation

We cannot use the percepetron model in the intermediate layers, because we do not know how to compute the errors. They were in fact e=y-f(w,x) where y is the groundtruth.
But in intermediate layers we do not know the true value of y, it is known only for the final output layer.

How to backpropagate the error into the hidden nodes?
We use a specific version of Gradient Descent.

Gradient Descent for Multi-Layer NN

The error function to minimize is $$E=\frac 1 2 \sum_{i=1}^N\bigg(y_i-f(\sum_j w_jx_{ij})\bigg)^2$$
that is basically a Sum of Squared Residuals (SSR).
It will be the Loss Function from which we can find a global minimum solution.

The weight update is $$w_j^{(k+1)}=w_j^{k}-\lambda \frac{\delta E}{\delta w_j}$$
The activation function's slope can be, for example, the sigmoid function, where the slope is obtained as partial derivative by the GD. $$w_j^{(k+1)}=w_j^{k}+\lambda \frac{\delta E}{\delta w_j}$$

![[Pasted image 20240628174643.png]]


Error is computed at the output and propagated back to input by chain rule to compute the contribution of each weight to the loss.
It is a 2-step process
1. forward pass
	1. computing the network output $o_i$
2. backward pass
	1. computing the loss function gradients and update


...


## Loss Function

*not so clear*

The Error function (also Loss Function or Cost Function) allows to compare candidate solutions using a simple scalar. But keep in mind that it is a simplification of a complex system.
It is therefore important that the loss function is designed to faithfully represent the goal. Each task may need a specific loss function.

The problem can be
1. regression
	1. target
		1. real-value quantity
	2. output layer
		1. is ONE node with a linear activation unit
	3. loss function
		1. mean squared error (MSE)
2. classification
	1. target
		1. belonging to one of K classes
	2. output layer
		1. is ONE node with a sigmoid activation unit
			1. K=2 binary cross entropy?
		2. K output nodes in a softmax layer
			1. K>2 categorical cross entropy
	3. loss function
		1. cross entropy (ie negative log-likelihood)

![[Pasted image 20240628184142.png]]

## Design of a NN

It is important to set
1. number of nodes in input layer
	1. one input node per binary/continuous attribute
	2. k or $log_2k$ nodes for each categorical attribute with k values
2. number of nodes in output layer
	1. one output for binary class problem
	2. k or $log_2k$ nodes for k-class problem
3. number of hidden layers and nodes in hidden layer
	1. remember that too much might mean overfitting
4. initial weights and biases

Observe that augmenting the number of dimensions at the beginning with more neurons in hidden layers is like increasing the number of dimensions in the svm to get non-linear insights.

Furthermore, be aware of some issues during training
1. GD may converge to local minimum, not a global one
2. sensitive to noise in training
3. can handle redundant attributes
4. model building can be time consuming (testing is not)
5. avoid overfitting

## NN Training

Split dataset into
1. training set
	1. use it to update weights
	2. records are repeated in random order
2. validation set
	1. use it to decide when to stop training
	2. monitoring the error and the number of epochs
3. test set
	1. use it to test the performance of the NN
	2. should not be used as part of the NN development

Preprocessing alert.
Since NN are considering vectors entirely, you MUST use standard normalization.

Weight initialization.
Choice of initial weight values is important as this is the starting position in the space of the weights, which could be far away from global minimum.
1. aim to sle














