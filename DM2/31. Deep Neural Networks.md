## Representational learning

![[Pasted image 20240627193229.png|300]]

Representation learning methods
1. allow a machine to be fed with raw data
2. automatically discover the representation needed for detection or classification

Multilayer Neural Network.
1. can solve any type of classification task involving nonlinear decision boundaries
2. hidden layers are intermediate layers between input and output layers
3. more activation functions are used such as sigmoid,  hyperbolic tangent,...

Level of abstraction.
The model will figure out the right level of abstraction for the information that is good to get the right output.
Deep neural networks can build up increasingly more complex levels of abstraction, like lines, parts, regions.

![[Pasted image 20240628171422.png]]

## Activation functions

![[Pasted image 20240628170545.png]]

instead of the logistic function, the most common activation functions are
1. hyperbolic tangent (TANH)
	1. which is similar to logistic but shifted to the range -1+1
2. rectified linear unit (RELU)
	1. linear with a cutoff at zero
	2. typically used inside hidden layers
3. soft exponential linear unit (SOFT-MAX)
	1. a soft version of the relu
	2. log(exp(x)+1)

![[Pasted image 20240628172104.png]]

The shape of the derivative is important because we want to use the gradient descent, and therefore make sure that the function is derivable in the entire domain.

## Learning for Hidden Layers

We cannot use the percepetron model in the intermediate layers, because we do not know how to compute the errors. They were in fact e=y-f(w,x) where y is the groundtruth.
But in intermediate layers we do not know the true value of y, it is known only for the final output layer.

How to backpropagate the error into the hidden nodes?
We use a specific version of Gradient Descent.

Gradient Descent for Multi-Layer NN

The error function to minimize is $$E=\frac 1 2 \sum_{i=1}^N\bigg(y_i-f(\sum_j w_jx_{ij})\bigg)^2$$
that is basically a Sum of Squared Residuals (SSR).
It will be the Loss Function from which we can find a global minimum solution.

The weight update is $$w_j^{(k+1)}=w_j^{k}-\lambda \frac{\delta E}{\delta w_j}$$
The activation function's slope can be, for example, the sigmoid function, where the slope is obtained as partial derivative by the GD. $$w_j^{(k+1)}=w_j^{k}+\lambda \frac{\delta E}{\delta w_j}$$

![[Pasted image 20240628174643.png]]


Error is computed at the output and propagated back to input by chain rule to compute the contribution of each weight to the loss.
It is a 2-step process
1. forward pass
	1. computing the network output $o_i$
2. backward pass
	1. computing the loss function gradients and update

















