# Representational learning

![[Pasted image 20240627193229.png|300]]

Representation learning methods
1. allow a machine to be fed with raw data
2. automatically discover the representation needed for detection or classification

Multilayer Neural Network.
1. can solve any type of classification task involving nonlinear decision boundaries
2. hidden layers are intermediate layers between input and output layers
3. more activation functions are used such as sigmoid,  hyperbolic tangent,...

Level of abstraction.
The model will figure out the right level of abstraction for the information that is good to get the right output.
Deep neural networks can build up increasingly more complex levels of abstraction, like lines, parts, regions.

![[Pasted image 20240628171422.png]]

# Training concepts

## Activation functions

![[Pasted image 20240628170545.png]]

instead of the logistic function, the most common activation functions are
1. hyperbolic tangent (TANH)
	1. which is similar to logistic but shifted to the range -1+1
2. rectified linear unit (RELU)
	1. linear with a cutoff at zero
	2. typically used inside hidden layers
3. soft exponential linear unit (SOFT-MAX)
	1. a soft version of the relu
	2. log(exp(x)+1)

![[Pasted image 20240628172104.png]]

The shape of the derivative is important because we want to use the gradient descent, and therefore make sure that the function is derivable in the entire domain.

## Backpropagation

We cannot use the percepetron model in the intermediate layers, because we do not know how to compute the errors. They were in fact e=y-f(w,x) where y is the groundtruth.
But in intermediate layers we do not know the true value of y, it is known only for the final output layer.

How to backpropagate the error into the hidden nodes?
We use a specific version of Gradient Descent.

Gradient Descent for Multi-Layer NN

The error function to minimize is $$E=\frac 1 2 \sum_{i=1}^N\bigg(y_i-f(\sum_j w_jx_{ij})\bigg)^2$$
that is basically a Sum of Squared Residuals (SSR).
It will be the Loss Function from which we can find a global minimum solution.

The weight update is $$w_j^{(k+1)}=w_j^{k}-\lambda \frac{\delta E}{\delta w_j}$$
The activation function's slope can be, for example, the sigmoid function, where the slope is obtained as partial derivative by the GD. $$w_j^{(k+1)}=w_j^{k}+\lambda \frac{\delta E}{\delta w_j}$$

![[Pasted image 20240628174643.png]]


Error is computed at the output and propagated back to input by chain rule to compute the contribution of each weight to the loss.
It is a 2-step process
1. forward pass
	1. computing the network output $o_i$
2. backward pass
	1. computing the loss function gradients and update


...


## Loss Function

*not so clear*

The Error function (also Loss Function or Cost Function) allows to compare candidate solutions using a simple scalar. But keep in mind that it is a simplification of a complex system.
It is therefore important that the loss function is designed to faithfully represent the goal. Each task may need a specific loss function.

The problem can be
1. regression
	1. target
		1. real-value quantity
	2. output layer
		1. is ONE node with a linear activation unit
	3. loss function
		1. mean squared error (MSE)
2. classification
	1. target
		1. belonging to one of K classes
	2. output layer
		1. is ONE node with a sigmoid activation unit
			1. K=2 binary cross entropy?
		2. K output nodes in a softmax layer
			1. K>2 categorical cross entropy
	3. loss function
		1. cross entropy (ie negative log-likelihood)

![[Pasted image 20240628184142.png]]

## Hyperparameters

It is important to set
1. number of nodes in input layer
	1. one input node per binary/continuous attribute
	2. k or $log_2k$ nodes for each categorical attribute with k values
2. number of nodes in output layer
	1. one output for binary class problem
	2. k or $log_2k$ nodes for k-class problem
3. number of hidden layers and nodes in hidden layer
	1. remember that too much might mean overfitting
4. initial weights and biases

Observe that augmenting the number of dimensions at the beginning with more neurons in hidden layers is like increasing the number of dimensions in the svm to get non-linear insights.

Furthermore, be aware of some issues during training
1. GD may converge to local minimum, not a global one
2. sensitive to noise in training
3. can handle redundant attributes
4. model building can be time consuming (testing is not)
5. avoid overfitting

# Training details
## Pre-processing

Split dataset into
1. training set
	1. use it to update weights
	2. records are repeated in random order
2. validation set
	1. use it to decide when to stop training
	2. monitoring the error and the number of epochs
3. test set
	1. use it to test the performance of the NN
	2. should not be used as part of the NN development

Preprocessing alert.
Since NN are considering vectors entirely, you MUST use standard normalization.

## Weight initialization

Choice of initial weight values is important as this is the starting position in the space of the weights, which could be far away from global minimum.
1. aim to select weight values which produce midrange function signal ???
2. select weight values randomly from uniform probability distribution
	1. try different random initialization to assess robustness and have more opportunities to find optimal results
3. normalize weight values so number of weighted connections per unit produces midrange function signal

## Learning modes

1. sequential mode (on-line, stochastic, or per-record)
	1. weights updated after each record is presented
		1. more updates means quicker convergence but also learning less stable
2. batch mode (off-line or per-epoch)
	1. weights updated after all records are presented
		1. can be very slow and lead to trapping in early local minima
3. minibatch mode (blend of the previous)
	1. weights updated after a few records
		1. from ten to thousands, typically 100, this hyperparameter affects performance
		2. best of both and good for the GPU

## Convergence criteria

Learning is obtained by repeatedly supplying training data and adjusting by backpropagation.
Typically 1 training set presentation is 1 epoch.

We need a stopping criteria to define *convergence*.
1. euclidean norm of the gradient vector reaches a sufficiently small value
2. absolute rate of change in the average squared error per epoch is sufficiently small
3. generalization performance reaches a peak
	1. validation for generalization performance ???

## Early Stopping

This concept is VERY important and MUST be used in the project. It is the way to avoid overfitting of NN.
Running too many epochs may overtrain the network and result in overfitting, performing poorly in generalization.
Then
1. keep a holdout validation set and test accuracy after every epoch
	1. maintain weights for best performing network on the validation set and stop training when error increase beyond this
2. always let the network run for some epochs before deciding to stop, then backtrack to best result
	1. the "patience parameter" is used to go back to the model before it starts overfitting the data
		1. in sklearn, it is called "tolerance" for n_iter_no_change

![[Pasted image 20240630173256.png]]



## Model selection

This is more important than the number of nodes and layers.
> Try at least 3 models in the project.

Remember.
1. too few hidden units prevent the network from learning adequately fitting the data and learning the concept
2. too many hidden units leads to overfitting, unless you regularize heavily
	1. meaning dropout, weight decay, weights penalties..

Cross validation should be used to determine an appropriate number of hidden units
1. by using the optimal validation error to select the model with optimal number of hidden layers and nodes


## Norm Regularization

Constrain the learning model to avoid overfitting and help improving generalization.

Add "penalization terms" *to the loss function* that punish the model for excessive use of resources.
1. limit the number of weights that is used to learn a task
2. limit the total activation of neurons in the network

![[Pasted image 20240630184125.png]]

Observe that lambda was called C in the SVM.

Common penalty terms are
![[Pasted image 20240630184306.png]]

## Dropout regularization

It means randomly disconnecting units from the network during training. Meaning that during each training iteration, some nodes are randomly set to zero. The consequence is that these nodes will not contribute in the forward pass and the weights will not be updated in the backward pass.

In the training phase.
1. regulated by unit dropping hyperparameter
	1. it the probability P of dropping a neuron during an iteration
2. prevents unit coadaptation
	1. build more robust networks that do not rely on single neurons for particular concepts, avoiding also overfitting
	2. meaning that neurons in different parts of the network are learning the same concept
3. committee machine effect
	1. because of dropout, the model predicts as if it was composed by different combination of neurons with some shared weights, making it looking like a committee

In the test phase, we can predict
1. without dropout
	1. since the dropout only happens during training, in the prediction phase the weights must be scaled by a dropout rate
	2. this ensures that the output is the same as during training
2. with dropout
	1. performing multiple passes and averaging the results
	2. this predictions can be used to estimate uncertainty using confidence intervals
	3. known as Monte Carlo Dropout

There is a variant of the dropout of neurons, called "dropconnect", which drops individual connections (the weights) instead of entire neurons.

![[Pasted image 20240630184629.png]]


## Momentum Regularization

This is another way to regularize and to avoid overfitting.
Adding a term to the weight update equation to store an exponentially weight history of previous weights changes.

This reduces the problems of instability while increasing the rate of convergence.
1. if weight changes tend to have the same signs
	1. momentum terms increases
	2. gradient decrease speed up convergence on shallow gradient
2. if weight changes tend to have opposing signs
	1. momentum term decreases
	2. gradient descent slows doen to reduce oscillations (stabilizing)

In addition, it can help escape being trapped in a local minimum.



## Optimization Algorithms

Which optimization algorithm to choose?
1. Stochastic Gradient Descent (SGD)
	1. easy and efficient
	2. difficult to pick up the best learning rate
	3. unstable convergence
	4. often used with momentum
2. RMSprop
	1. adaptive learning rate method
		1. reduces it using a moving average of the squared gradient
	2. fastens convergence by having quicker gradients when necessary
3. Adagrad
	1. like RMSprop but with element-wise scaling of the gradient
4. ADAM
	1. like Adagrad but adds an exponentially decaying average of past gradients like momentum
	2. this is the most widely used and the default for pytorch


## Advanced NN approaches

1. convolutional NN
	1. typically applied for the classification of images and time series
	2. instead of having only "fully connected" layers adopt "convolutional layers"
2. Recurrent NN
	1. typically applied in Natural Language Processing, like BERT
	2. they have an "internal state" that is updated as a sequence is processed



# Convolutional NN
[statquest](https://www.youtube.com/watch?v=HGwBXDKFk9I)

## Idea
It is specific for images and time series classification.
The key ideas are
1. reduce number of input nodes for the nn
2. tolerate small shifts in the pixels
3. take advantage of the correlated pixels nearby (still don't know how)
My point is that the the convolution looks like a pre-processing to simplify the image and improve generalization of the nn.

## Algorithm
Given an image and a filter (aka kernel)
1. convolve the filter with the image to obtain a feature map
	1. it means sliding the smaller matrix called filter over the image computing dot products (and adding bias) to fill the matrix called feature map
	2. the stride of the sliding can be one or more (parameter), but the dimensions of the matrices and the stride must match
	3. a padding (border of zeros) could be added to the image
2. the feature map is run through an activation function
	1. usually a RELU to remove negative values
3. the updated feature map is run through Pooling
	1. usually MaxPooling or AvgPooling, they are specific filters that instead of the dot product compute just the max or avg of a specific submatrix, without overlapping, to create an even smaller matrix
4. the matrix after pooling is given as input of a standard NN
	1. actually the vector obtained from the matrix, it will give the final classification as output


## Params
1. filter (or kernel)
2. stride
3. padding
4. bias
5. activation function

![[Pasted image 20240721190730.png]]

# Recurrent NN









