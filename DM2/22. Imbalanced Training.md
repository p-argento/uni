## Outline
1. Imbalanced Classes
2. undersampling
	1. random undersampling
	2. neighbor-based approach
		1. tomek links
		2. edited nearest neighbor
		3. condensed nearest neighbor
		4. cluster centroids
3. oversampling
	1. random oversampling
	2. smote
	3. adasyn
4. balancing at the algo level
	1. adjusting class weight
	2. meta-cost sensitive classifier
	3. adjusting decision threshold

## Imbalanced Classes
Most classification methods assume classes are reasonably balanced. But in reality it is quite common to have a rare (and interesting) class. For example, the number of credit cards defrauded or a specific condition on medical screenings.

A classifier that always predict the most common class has an accuracy of 99%. For this reason, we should rely more on precision, recall and f1-score.
Always compare the accuracy with the trivial classifier.
If classes are perfectly balanced, a trivial classifier (e.g. majority) will yield an accuracy of ~100/N %

How to handle imbalanced data?
1. balance the training set (most popular)
	1. under-sampling the majority class
	2. over-sampling the minority class
		1. a doctor might not trust syntetic data
2. balance at the algorithm level
	1. adjusting class weights
		1. making the algo more sensitive to rare classes
	2. adjusting decision threshold
	3. designing a new algorithm
3. switch to Anomaly Detection -> [[24. Anomaly Detection]]
4. do nothing and hope to be lucky

## Undersampling



























---


## New slide

![[Pasted image 20240325095753 1.png]]

In the project.
Select an imbalance problem from tabular data and use at least one oversampling and one undersampling