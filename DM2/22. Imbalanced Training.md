## Outline
1. Imbalanced Classes
2. undersampling
	1. random undersampling
	2. neighbor-based approach
		1. tomek links
		2. edited nearest neighbor
		3. condensed nearest neighbor
		4. cluster centroids
3. oversampling
	1. random oversampling
	2. smote
	3. adasyn
4. balancing at the algo level
	1. adjusting class weight
	2. meta-cost sensitive classifier
	3. adjusting decision threshold

## notebook outline
[Python imblearn library](https://imbalanced-learn.org/stable/)

1. data preparation
	1. data partitioning
	2. classification
	3. PCA
2. undersampling
	1. RandomUnderSampler
	2. CondensedNearestNeighbor
	3. TomekLinks
	4. EditedNearestNeighbors
	5. Cluster Centroids
3. Oversampling
	1. RandomOverSampler
	2. SMOTE
	3. ADASYN
4. Balancing at the Algorithm Level
	1. Class weights
	2. Meta-cost sensitive Classifier
	3. Corrected Decision Tree



# Imbalanced Classes
Most classification methods assume classes are reasonably balanced. But in reality it is quite common to have a rare (and interesting) class. For example, the number of credit cards defrauded or a specific condition on medical screenings.

A classifier that always predict the most common class has an accuracy of 99%. For this reason, we should rely more on precision, recall and f1-score.
Always compare the accuracy with the trivial classifier.
If classes are perfectly balanced, a trivial classifier (e.g. majority) will yield an accuracy of ~100/N %

How to handle imbalanced data?
1. balance the training set (most popular)
	1. under-sampling the majority class
	2. over-sampling the minority class
		1. a doctor might not trust synthetic data
2. balance at the algorithm level
	1. adjusting class weights
		1. making the algo more sensitive to rare classes
	2. adjusting decision threshold
	3. designing a new algorithm
3. switch to Anomaly Detection -> [[24. Anomaly Detection]]
4. do nothing and hope to be lucky
	1. try as a first approach to have a baseline model

In the literature there are many solution combining SMOTE with Undersampling approaches and using them in sequence in order to oversample the minority class while undersampling the majority one and mitigating the negative effects of these approaches.

# Undersampling

We divide the undersampling of the majority class in
1. random undersampling
2. neighbor-based approaches
	1. Condensed Nearest Neaighbors
	2. Tomek Links

When do we do undersampling?
It should be done after the partition training/validation/set.
First, rebalance the training set and train the model.
Second, test the model on the imbalanced test set.
Third, test the model on the balanced test set.
Compare the results, if the last step is bad, it means that the rebalance it too specific for the training set.

## Random Undersampling

Under-sample the majority classes by randomly picking samples with or without replacement.
Since it is random, it might be worth it to repeat it many times and compare the results to get the most representative data.

![[Pasted image 20240612160958.png]]

## Tomek Links

It is a instance-based approach, meaning that there's the issue of computing $n^2$ distances.

The idea is to remove points belonging to the majority class that are close to the minority class. It will be easier to generalize.

Tomek Links uses a rule to select the pair of observation (a,b) that respect these properties.
1. a nearest neighbor is b
2. b nearest neighbor is a
3. a and b belong to a different class (majority or minority)

Tomek Links are used to find samples of data from the majority class that is having the lowest distance with the minority class data and then remove it.

![[Pasted image 20240612160748.png]]

## Edited Nearest Neighbor (ENN)

The idea is to remove instances of the majority class for which a KNN classifier fails. Basically helping the KNN to generalize.

The problem is that a lot of distances must be computed.

The ENN method is
1.  find the knn of each observation
2. check if the observations in majority class are misclassified
3. delete from the dataset the misclassified observations in the majority class

## Condensed Nearest Neighbor (CNN)

This is the most popular. Use this instead of the previous ones.

The idea is to perform a smart undersampling by removing majority points having as knn a minority point.

NEW SLIDE below

![[Pasted image 20240325095753 1.png]]

> In the project. Select an imbalance problem from tabular data and use at least one oversampling and one undersampling.

CNN reduces the dataset X for classification by constructing a subset of records which are able to classify the original dataset using knn (typically 1nn).
Note that CNN does NOT return the misclassified points, but a subset of X (called STORE).


## Undersampling by Cluster Centroids

It is a smart version of random undersampling.
Under-sampling by generating centroids based on clustering methods.

Under-samples the majority class by replacing a cluster of majority samples by the cluster centroid of a K-Means algorithm.

K-Means keeps K majority samples by fitting it with K cluster to the majority class and using the coordinates of the K cluster centroids as the new majority samples.

# Oversampling

We explore
1. Random Oversampling
2. Synthetic Minority Oversampling Technique (SMOTE)
3. Adaptive Syntetic (ADASYN) sampling approach

## Random Oversampling

Over-sample the minority class by picking samples at random with replacement. Repeat the training several times, but consider that it might not help too much in the training because we are just creating points on top of others.

## SMOTE Oversampling

It is one of the best algorithm.
Synthetic Minority Oversampling Technique (SMOTE) does oversample the minority class by adding points through interpolation.

After connecting the points, there could be still empty areas inside creating problems.

![[Pasted image 20240612171740.png]]

It operates in the "feature space" rather than in the "data space", and effectively forces the decision region of the minority class to become more general.
The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of k minority class nearest neighbors.

Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. (by default k=5).
E.g., if the amount of over-sampling needed is 200%, only two neighbors from the five are chosen and one sample is generated in  the direction of each.

Take the difference between the feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, meaning pick a random point in the segment connecting two points of the minority class. Add it to the feature vector.

![[Pasted image 20240612172153.png]]

In SMOTE, there is the problem of concave shapes, like the ones that were causing problems with k-means.
The alternatives are
1. SMONTEC (over-sample for continuous and categorical features)
2. BorderlineSMOTE (over-sample using the bordeline variant)
3. SVMSMOTE (over-sample using the SVM variant)
4. ADASYN (over-sample using ADASYN)

## ADASYN

The steps are
1. Calculate the ratio of minority to majority examples
	1. use d=min/maj
2. calculate the total number of synthetic minority to generate
	1. use G=(maj-min)\*beta
	2. where beta is the ratio of minority, meaning that if beta=1 it is required a perfectly balanced dataset
3. find the knn of each minority sample and calculate the ratio of maj for the neigborhood k
	1. use r=maj_i / k
	2. normalize by dividing r_i for the sum of all the r_i
4. (adaptive step) calculate the number of syntethic samples to generate per neighborhood
	1. use G_i=G_r
	2. they must keep the density, meaning more points in a more dense area
5. generate G_i samples for each neighborhood by taking two minority samples (x_i,y_i) within the neighborhood and generate a synthetic one as SMOTE
	1. ie s_i=x_i+(y_i-x_i)*\lambda
	2. where lambda is a random number between 0 and 1

![[Pasted image 20240612173952.png]]

# Balancing at the algorithm level

## Adjusting the class weights

The classifier can be trained considering different costs to be paid for misclassification errors on minority classes. This is generally done using a "class weight".

Each outcome with respect to a confusion matrix can be associated to a weight in a corresponding weight (or cost) matrix.
Thus, the objective of the classification algorithm is to find the model that minimizes the total costs.

![[Pasted image 20240612174746.png]]

## Adjusting the Decision Threshold

Several classification methods compute scores in terms of probability of belonging to a class, and then assign class.

What if we generalize the schema into:
1. Score p > THR% → class = Y
2. Otherwise → class = N





## Meta-Cost Sensitive Classifier

There was a library, but it is not maintained anymore.











