
Algorithms for Categorical / Transactional Clustering
1. K-Modes
2. ROCK
3. CLOPE
4. TX-Means

## Market Basket Data
Consider Market Basket Data, where each transaction represent one customer, and each transaction contains set of items purchased by the customer.

Clustering customers reveals customers with similar buying patterns putting them into the same cluster.

In a market basket database, attributes are non-numeric, transactions are viewed as records with boolean attributes corresponding to a single item. It means 1 if the transaction contains the item, 0 otherwise.
My idea is that it looks like one-hot-encoding.

However, using distance metrics as similarity measure for categorical data is NOT appropriate. Two transactions can be closer than others but without having a single item in common.

## K-Modes

Instead of the euclidean distance, like K-Means, K-Modes uses the *number of mismatches* between the attributes of two objects.

![[Pasted image 20240723143905.png|300]]

- $m$ is the number of attributes, meaning the items that can be purchased

K-Modes uses the *mode* as the representative object of a cluster. Given the set of objects in the cluster C, the mode is get computing the max frequency for each attribute.

![[Pasted image 20240723144113.png|300]]

Observe that everything that can be discretized can be captured by this algo. 

The *algorithm* is the same as K-Means but with the mode
1. randomly select the initial objects as modes
2. scan of the data to assign each object to the closer cluster identified by the MODE
3. re-compute the mode of each clluster
4. repeat the steps 2 and 3 until no object changes the assigned cluster

The main problem with this family of algorithms is the *initialization*. The solution is to run the algorithm multiple times and pick the solution with the least SSE.

## ROCK
means RObust Clustering using linK

This is the oldest algorithm for transactional clustering, proposed in the 80s, but very slow. The core concept is LINK.

It is a *hierarchical algorithm* (aka agglomerative) for clustering transactional data (market basket databases), it can be used directly on the set of transactions.

ROCK uses links to cluster instead of the classical distance notion. It uses the notion of neighborhood between pair of objects to identify the number of links between two objects, like DBSCAN.

The input is
1. a set S of data points
2. the number of k clusters to be found
	1. even if it is a hierarchical clustering, the dendrogram is stopped earlier because of high computational cost 
3. the similarity threshold (called theta)
The output is
1. groups of clustered data

The ROCK *algorithm* is divided into three major parts
1. draw a random sample from the data set
2. perform a hierarchical agglomerative clustering algorithm
3. label data

First step. Draw a random sample from the data set.
1. this is needed to speed up calculations and ensure scalability to very large data sets
2. the inital sample is used to form clusters, then the remainig data on dataset is assigned to these clusters

Second step. Perform a hierarchical agglomerative clustering algorithm.
1. places each single data point into a separate cluster
2. compute the similarity measure for all pairs of clusters
3. merge the two clusters with the highest similarity
4. verify a stop condition (reached k-clusters)
	1. if not go to step 2
(these steps are common to all hierachical agglomerative clustering algorithms but with different definition to the similarity measaures)

Third step. Label data.
1. the remaining data points are assigned to clusters
2. done selecting a random sample Li from each cluster Ci, then we assign each point p to the cluster for which it has the strongest linkage with Li

There are three *important concepts*
1. similarity
2. neighbors
3. links

Similarity. As a similarity function, ROCK uses the Jaccard Coefficient, meaning that $$sim(A,B)=\frac{|A\cap B|}{|A\cup B|}$$
Neighbors. $$\text{A and B are neighbors if sim(A,B)} \geq \theta$$Where $\theta$ is a hyperparameter defined by the user, but there is no way to find the best one. If very low, clusters will contain very different transactions.

Links. A link defines the number of common neighbors between two objects, meaning $$link(A,B)=|neighbor(A)\cap neighbor(B)|$$Think of it like social networks. We are similar if we have many friends in common, meaning having similar transactions in common. I do not look at similarities, but at the world around them.

Higher values of link(A,B) means higher probability that A and B belong to the same cluster. 
Similarity is local while link captures global information.
Remember that, like in dbscan, every point is a neighbor of itself.

![[Pasted image 20240723161059.png]]

## CLOPE
means Clustering with sLOPE

It is transactional clustering for high dimensional data.
The key is that it has a global vision of the transactions. It is efficient for a large number of distinct items, but not number of transactions.

It uses a global criterion function that tries to *increase the intra-cluster overlapping* of transaction items.
This is done by increasing the height-to-width ratio (H/W) of the cluster histogram, which means that we aim at higher rectangular shapes (see image), because it means more overlapping inside the same cluster.

![[Pasted image 20240723154022.png]]

For evaluating the goodness of a clustering, the criterion function is the gradient of $$G(C)=\frac{H(C)}{W(C)}=\frac{S(C)}{W(C)^r}$$
![[Pasted image 20240723154639.png]]

Where $r$ is called repulsion and it is an hyperparameter, usually set to 2. It is the equivalent of theta in ROCK and the radius in dbscan, meaning that it is the threshold for the similarity. It is difficult to hypertune.

Input.
1. dataset
2. repulsion r
3. maximum number of clusters

In summary.
1. for each transaction, add it to a new cluster or to an existing one such that the profit is maximized
2. for each transaction, try to move it to another cluster and do it if maximizes the profit
3. repeat until all the transactions remain in the same cluster

## TX-Means
means Transactional X-Means.

It is a top-down hierarchical algorithm, basically the transactional version of X-Means.

![[Pasted image 20240723161016.png|300]]

It is a *parameter-free* clustering algorithm able to efficiently partition transactional data automatically.
It is suitable for the case where clustering must be applied on a massive number of different datasets. For example, when a large set of users need to be analyzed individually and each of them has generated a long history of transactions.

TX-Means automatically estimates the number of clusters. Additionally, it provides the representative transaction of each cluster, which summarizes the pattern captured by that cluster.

Key points of the algorithm.
1. recursively, the dataset is split in 2 (*bisecting*), until a stop condition is met.
2. the distance function is the Jaccard Coefficient (that is overlap-based)
3. as stopping criterion, an adapted version of BIC is used, since we deal with transactions
4. in the end, for every leaf we get a representative basket (think of it as the average basket)

An important aspect is the scalability of the X-Means, thanks to a *sampling strategy*.
1. random selection of a subset of baskets
2. run TX-Means on the subset and obtain clusters and representative baskets
3. assign the remaining baskets to the clusters using a nearest neighbor approach with respect to the representative baskets









