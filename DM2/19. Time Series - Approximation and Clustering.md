# Approximation
## Why approximation
We represent the ts into a smaller and simpler space.
We loose some part of the data, but still remains understandable.
Approximation is a special form of dimensionality reduction specifically designed for TS.

The difference between approximation and compression is that
- approximation is still understandable
- compression may not be understandable

### Approximation example
We can decompose the data into many sine waves using the Fourier Transform.
The Fourier Coefficients are just a column of numbers.
Note that this is not reduced yet.

Now, we can truncate most of the small coefficients with little effect, leaving only the most significative waves.

![[Pasted image 20240323174538.png|300]]

### Types of TS Approximations

![[Pasted image 20240323174644.png]]

We have

| approx                                              | DWT | Python |
| --------------------------------------------------- | --- | ------ |
| 1. Discrete Fourier Transform (DFT)                 | NO  |        |
| 2. Discrete Wavelet Transform (DWT)                 | NO  | NO     |
| 3. Singular Value Decomposition (SVD)               | NO  | ?      |
| 4. Piecewise Linear Approximation (PLA)             |     | NO     |
| 5. Piecewise Aggregate Approximation (PAA)          |     |        |
| 6. Adaptive Piecewise Constant Approximation (APCA) |     | NO     |
| 7. Symbolic Aggregate Approximation (SAX)           |     |        |


1-3 loose the temporal information.
4-7 keep the temporal information.

## 1. Discrete Fourier Transform (DFT)
It represents the ts as a a linear combination of sines and cosines.
Only N/2 coefficients are kept. In fact, sine waves requires two numbers, for the phase w and the amplitude A,B.
Low amplitude coefficients contribute little to information and can be discarded for saving storage space.
A ts represented with DTF is said to be in its frequency domain.


| pros                                                  | cons                                        |
| ----------------------------------------------------- | ------------------------------------------- |
| - good ability to compress natural signals like sound | - problems with different length sequences  |
| - fast nlogn algo                                     | - do NOT support weighted distance measures |

![[Pasted image 20240324153429 1.png]]
## 2. Discrete Wavelet Transform (DWT)
(NOT available in Python)

It represents the ts as a lineare combination of wavelet basis functions .
Keep only the first N coefficients.
Wavelets are represented data in terms of the sum and difference of a prototype function, called the "mother wavelet" or "analyzing wavelet".

While DFT captures a global representation, the DWT is localized in time. It means that some of the wavelet coefficients represent small local subsections of the data.


| pros                            | cons                                                                                           |
| ------------------------------- | ---------------------------------------------------------------------------------------------- |
| - good for stationarity signals | - ONLY defined if sequence length is an integral power of 2                                    |
| - fast linear time algo         | - in the wavelet, the left side of the signal is approximated at the expense of the right side |
|                                 | - do NOT support weighted distance measure                                                     |

![[Pasted image 20240324153439 1.png]]
## 3. Singular Value Decomposition (SVD)
(SKIP FOR NOW)



## 4. Piecewise Linear Approximation (PLA)
(NOT AVAILABLE IN PYTHON)

It represents the ts as a sequence of straight lines.
If lines are connected, then we have K/2 lines.
If lines are disconnected, then we have K/3.

There are many algos for segmenting ts.
How to choose the optimal number of segments K?
It is a tradeoff between accuracy and compactness with no general solution.

Each line segment can be represented with a pair of numbers:
1. length
2. left_height
	1. which is the starting point of the segment, while the right_height can be inferred by looking at the next segment

![[Pasted image 20240324154146 1.png]]

## 5. Piecewise Aggregate Approximation (PAA)
(MOST POPULAR)

It represents the ts as a sequence of box basis functions with each box of the same size.
It approximates a TS by dividing it into equal-length segments.
Then the mean value of the points in a segment is stored.
The vector of these values becomes the data reduced representation.

| pros                                  | cons |
| ------------------------------------- | ---- |
| - extremely fast                      |      |
| - supports NON-euclidean measures     |      |
| - support weighted Euclidean distance |      |

![[Pasted image 20240324154620 1.png]]
## 6. Adaptive Piecewise Constant Approximation (APCA)
(NOT available in Python)

It is a kind of mix between PLA and PAA.
It allows the segments to have arbitrary lengths, so two numbers for each segment are needed (the mean and the length).
It has the advantage of being able to place a single segment in an area of low activity and many segments in areas of high activity.
In addition, the structure of the data must be considered.

| pros                                   | cons |
| -------------------------------------- | ---- |
| - fast linear time                     |      |
| - supports NON-euclidean measures      |      |
| - supports weighted euclidean distance |      |

![[Pasted image 20240324155917 1.png]]

### TS Segmentation
Note that a ts can be segmented using
1. predefined length w
2. predefined number of segments k
3. change point detection methods
	1. More details: Selective review of offline change point detection methods. Truong, C., Oudre, L., & Vayatis, N. (2020). Signal Processing, 167, 107299.
## 7. Symbolic Aggregate Approximation (SAX)

Basically, it takes PAA as a first step, and then approximates even more.
It converts data into a discrete format with a small alphabet size (a,b,c,..).
Every letter (representing a part of the representation) contributes about the same amount of information about the shape of the ts.
In the end, there will be letters with the same frequency, while for example in the DFT the first letters are more significant.

Steps.
1. (PAA) aggregate w coefficients taken from the mean of each of the w equal-sized segments
2. determine breakpoint to divide the ts into equiprobable regions, one for each one of the letters of the alphabets specified by the user
3. each region is assigned a symbol, mapping PAA coefficient to the symbols corresponding to the regions in which they reside
	1. the symbols are assigned bottom-up, meaning that the lowest region will be "a", the following "b" and so on (look at the image)

It is important that the breakpoints are determined such that the probability of a segment falling into any of the regions is approximately the same (if the symbols are not equi-probable, some of the substrings would be more probable than others, and we would inject a probabilistic bias in the process)

![[Pasted image 20240324160319 1.png]]
# Clustering
## Clustering Time Series
It is based on the similarity between time series.
As always, within the cluster the TS should be similar, while we want dissimilarity between clusters.
It is an unsupervised task.

The 2 general methods for TS Clustering are
1. Partitional Clustering
2. Hierarchical Clustering

## 1. Hierarchical Clustering
It computes pairwise distance, then merges similar clusters bottom-up.
No need for the number of clusters.
Useful tool for data evaluation, by creating a dendrogram.

Limited to small datasets, due to its quadratic complexity.
It might be a good idea to do approximation first to reduce the complexity. 

![[Pasted image 20240324165030 1.png]]

## 2. Partitional Clustering
Typically use K-means (or one of the variants).
Meaning that the objective function minimizes the sum of squared intra- clusters errors.
K-means is the most popular, but the number of clusters K must be specified.

Approximation may not be requested.
While the distance function plays a fundamental role in terms of quality and efficiency. Both ED and (constrained) DTW should be tried.


## Types of TS Clustering
(taxonomy invented by Guidotti)

Types
1. Whole clustering
	1. traditional, using the shape and then other features
2. Feature-based clustering
	1. extract features or motifs and use them
3. Compression-based clustering
	1. run clustering on compressed ts
4. Subsequence clustering
	1. use subset of the ts obtained with a sliding window (similar to moving average)