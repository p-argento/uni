GD is a very effective and widely usable mathematical technique to find the best parameters in many and various tasks such as
1. linear regression parameters
2. logistic regression squiggle
3. neural networks weights
4. t-SNE clusters

It is an optimization algorithm.
You can use GD to find minimum (or maximum, then it is called Gradient Ascent) of many different functions.
GD does not really care what is the function that it minimizes, it just does what it was asked for.
Using GD, you must know how to tell if one value od the parameter is better that the other.
You must provide GD some function to minimize/maximize and GD will deal with finding its optimum value.

References
1.  Lemaréchal, C. (2012). "Cauchy and the Gradient Method" (PDF). Doc Math Extra: 251–254.
2. An overview of gradient descent optimization algorithms [pdf](http://arxiv.org/abs/1609.04747)Ruder, Sebastian. “An Overview of Gradient Descent Optimization Algorithms.” arXiv, June 15, 2017. [http://arxiv.org/abs/1609.04747](http://arxiv.org/abs/1609.04747).

(all the rest of the slides are just screenshots from [statquest video](https://www.youtube.com/watch?v=sDv4f4s2SB8&t=572s), so I will just watch it and take notes on the kindle)




