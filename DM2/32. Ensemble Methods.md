# Outline

1. Introduction to Ensembles
	1. the wisdom of the crowds
	2. types of ensembles
2. Bagging
	1. Bootstrap Sampling
	2. Bagging idea 
	3. Random Forests
4. Boosting
	1. Boosting idea
	2. AdaBoost


Types of ensembles
1. Manipulating data distribution
	1. Bagging (Bootstrap AGGregatING)
	2. Boosting
		1. AdaBoost
2. Manipulating input features
	1. Random Forest
3. Manipulating class labels
	1. Error-correcting output coding

Notebook
1. Data preparation
2. Random Forest
	1. Cross Validation
	2. Tuning Hyper-Parameters
3. Bagging
	1. with decision tree
	2. with SVC
	3. with RandomForestClassifier
4. Boosting
	1. AdaBoost
		1. with DecisionTreeClassifier
		2. with RandomForestClassifier

# Introduction to Ensembles

...

## Bootstrap
All the ensembles are based on the Boostrap sampling.
Bootstrap is a re-sampling statistical technique with re-entry to approximate the sample distribution of a statistic.
In each boostrap extraction, each record in the original dataset has 1/n probability of being extracted.
It can be extracted more than once or zero.


# Bagging idea
Means Bootstrap AGGregatING.

Bagging steps
1. bootstrap sampling (meaning sampling with replacement)
2. build classifier on each boostrap sample
3. use majority voting among the results of the classifiers

The idea is that creating samples with repetition we should get for example a classifier that is super expert in classifying the class yellow that is common in one of the samples because of the boostrap, and so on with other classifiers.

For example, in 


## (sklearn) BaggingClassifier
from sklearn.ensemble import BaggingClassifier

```python
from sklearn.svm import SVC
clf = BaggingClassifier(estimator=SVC(C=1000), 
                        n_estimators=10, 
                        random_state=0)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average=None))
print(classification_report(y_test, y_pred))
```

if `estimator=None` is a DecisionTree

# Random Forest

While Bagging is generalistic, meaning that can be used with any classifier, Random Forest is specific for trees.

Each decision tree is built on a bootstrap sample based on the values of an independent set of random vectors.
It means that each decision tree considers a random smaller subset of features.

## Differences with Bagging and AdaBoost

Random Forest is an extension of Bagging that further introduces randomness in the model building process.
The goal is to reduce correlation among trees.
The randomness is obtained not only with bootstrap, but also with the random selection of a subset of features for each decision tree.
Given n records and m attributes,
1. in RF, each decision tree is built on m' randomly chosen attributes, so each sample is n x m'
2. in Bagging, each decision tree is built on the same m attributes

> Actually, it looks like in the standard implementations, the random subset of features is selected for each split, not each tree.

Unlike AdaBoost, in Random Forests, the random vectors are generated from a fixed probability distribution.



# Boosting

It is an iterative procedure to adaptively change distribution of training data by focusing more on previously classified records.

Initially, all the records are assigned equal weights.
Unlike bagging, weights may change at the end of each boosting round.

It means that the previous model impacts how I select the records for the next model.

## Difference with Bagging


Bagging does NOT use bootstrap sampling. Instead, it adjusts the weights of data points based on the errors of previous models, without resampling.











