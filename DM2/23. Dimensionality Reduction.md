## Outline
1. intro to dimensionality reduction
2. feature selection
	1. Variance Threshold
	2. Univariate Feature Selection (ie ANOVA F-value)
	3. RFE
3. feature projection
	1. Random Subspace Projection (RSP)
	2. PCA
	3. Classic MDS
	4. Sammon Mapping
	5. Isometric Feature Mapping (IsoMap)
	6. t-SNE

## Notebook Outline
1. Feature Selection
2. Feature Projection





# Introduction to Dimensionality Reduction

Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables.
Approaches can be divided in
1. feature selection
	1. meaning remove some of the features
2. feature projection
	1. meaning obtain new features combining existing ones

# 1. Feature Selection

Why? Classification and regression can be done more in the reduced space rather than in the original space. This is because we can use faster algorithms, denser dataset and there is no curse of dimensionality.

Select a subset of the features according to different strategies
1. filter strategy
	1. ie information gain
2. wrapper startegy
	1. ie search guided by accuracy
3. embedded strategy
	1. selected features added or removed while building the model based on prediction errors

We discuss
1. Variance Threshold
2. Univariate Feature Selection (ie ANOVA F-value)
3. Recursive Feature Elimination (RFE)

## Variance Threshold

It removes all the features whose variance does not meet some threshold. By default, it removes all zero-variance features, meaning those that have the same value in all samples.

## Univariate Feature Selection

It selects the best features (or how many features to use?) based on univariate statistical tests. For instance, it removes all but the k highest scoring features.

An example of statistical test is the ANOVA F-value between label/feature.

![[Pasted image 20240613122009.png]]

where
- $Y_i$ denotes the sample mean in the $i^{th}$ group
- n is the number of observations in the $i^{th}$ group
- $\bar{Y}$ denotes the overall mean of the data
- $Y_{ij}$ is the observation in the  $i^{th}$ group out of K groups
- K denotes the number of groups
- N is the overall sample size

F-value is large if the numerator is large, which is unlikely to happen if the population of the groups all have the same value

## Recursive Feature Elimination (RFE)

Compared to variance threshold, the RFE requires training a model. There exists also a RFE-CV which uses the cross validation to improve the model used.

It removes redundant features first, meaning those that are highly correlated. Then, it uses more advanced techniques to remove less useful features.

Given an external estimator that assigns weights to features, RFE selects features by recursively considering smaller and smaller sets of features.
The estimator can assign for example the coefficients of a linear model, or the feature importance of a decision tree.

Steps
1. train the estimator on the initial set of features and obtain the importance of each features
2. prune the least important features from the current set of features
3. recursively repeat on the pruned set until the desired number of features is reached.

The same technique but without recursion is called SelectFromModel in sklearn.

# 2. Feature Selection
aka Feature Extraction

It transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear or non-linear. Typically nonlinear cannot be inverted.

Approaches discussed
1. Random Subspace Projection (RSP)
2. Principal Component Analysis (PCA)
3. Multidimensional Scaling
	1. Sammon
	2. IsoMap
	3. t-SNE 

More approaches
1. Non-negative matrix factorization (NMF)
2. Linear Discriminant Analysis (LDA)
3. Autoencoder

## Random Subspace Projection (RSP)

High-dimensional data is projected into low-dimensional space using a random matrix R whose columns have unit length.

No attempt to find a criterion for optimization.
It preserve the structure of the data, meaning the distances between points.
It is computationally cheap but NOT stable and NOT meaningful.

![[Pasted image 20240613131036.png]]

## Principal Component Analysis (PCA)

...


















## Multi-Dimensional Scaling (MDS)

MDS is a family of different algorithms designed to map data into a very low configuration, eg k=2 or K=3.
It cannot be inverted.

Given a pairwise dissimilarity matrix, the goal of MDS is to learn a mapping of data into a lower dimensionality such that the relative distances are preserved. It means that if two points are close in the feature space, it should be close also in the latent factor space.

Algorithm
1. given a pairwise dissimilarity matrix D and the dimensionality k
2. find a mapping such that $d_{ij}=||x_i-x_j||$ for all points in D
3. use gradient descend to minimize the function 
   $J(x) = \sum_{i} \sum_{j} d_1(d_{ij}, d_2(x_i,x_j))$

![[Pasted image 20240613131731.png]]

Distance, dissimilarity and similarity (or proximity) are defined for any pair of objects in any space.
In mathematics, a distance function between two objects is called "metric", satisfying:
1. d(x, y) ≥ 0
2. d(x, y) = 0 iff x = y
3. d(x, y) = d(y, x)
4. d(x, z) ≤ d(x, y) + d(y, z)
If the last condition does not hold, then d is a distance function but it is NOT a metric.
In other words, a distance measure is not strictly metric if it is positive-definite and symmetric, but it not required to satisfy the triangle inequality.

Depending on the distances adopted to calculate D and the distance function used for d1 and d2, we distinguish
1. Classic MDS
	1. adopts euclidean distance for every calculus
2. Metric MDS
	1. adopts metrics as distances
3. Non-Metric MDS
	1. deals with ranks of distances instead of their values

## Summon Mapping

It is a generalization of Metric MDS.
It introduces a weighting system that normalizes the squared-errors in pairwise distances by using the distance in the original space.
$J(x) = \sum_{i} \sum_{j} d_1(d_{ij}, d_2(x_i,x_j))/d_{ij}$
As a result, it preserves the small dij, giving them a greater degree of importance in the fitting procedure than for larger values of dij.

In contrast with Classic MDS, Sammon Mapping better preserves inter-distances for smaller dissimilarities, while proportionally squeezes the inter-distances for larger dissimilarities.

![[Pasted image 20240613153818.png]]

Basically, the normalization term $1/d_{ij}$ ensures that smaller distances in the original space have a higher weight in the objective function that is minimized. This avoids that larger distances, together with their larger errors, dominate the optimization process. While sometimes the smaller distances are those that really matters.

## Isometric Feature Mapping (IsoMap)

It is a MDS method.
Preserves the intrinsic geometry of the data.
Uses the geodesic manifold distances between all pairs.
IsoMap Handles non-linear manifold.

![[Pasted image 20240613154847.png]]

Algorithm.
1. determine neighboring points within a fixed radius based on the input space distance (euclidean)
	1. these neighborhood relations are represented as weighted graph G over the data points
2. estimate the "geodesic distance" between all pairs of points on the manifold
	1. by computing their shortest path distances on the graph G
	2. geodesic distance means counting the number of links to reach a certain point
3. construct an embedding of the data in a k dimensional euclidean space that best preserves the manifold geometry

## t-Distributed Stochastic Neighbor Embedding (t-SNE)

What are the differences between PCA and t-SNE?
They are the two most used dimensionality reduction techniques.

| PCA                                                                  | t-SNE                                                                          |
| -------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| tries to find a global structure                                     | tries to preserve local structure                                              |
| low dimensional subspace                                             | local dimensional neighborhood should be the same as the original neighborhood |
| can lead to local inconsistencies (far away points become neighbors) | distance and neighbors preservation                                            |
|                                                                      | used almost only for visualization                                             |
![[Pasted image 20240613160026.png]]

t-SNE is a subfamily of SNE, meaning Stochastic Neighbor Embedding. The intuition of SNE is to measure the pairwise similarities between high-dimensional and low-dimensional objects.

![[Pasted image 20240613160330.png]]

In SNE, we encode high dimensional neighborhood information as a distribution.
The intuition is to randomly walk between data points , jumping to a close point will happen with higher probability.
Find low dimensional points such that their neighborhood distribution is similar.

How do you measure the distance between distributions? The most common measure is KL divergence.

For neighborhood distributions.
1. consider the neighborhood around an input data point xi
2. imagine that we have a Gaussian distribution centered around xi
3. the probability that xi chooses some other datapoint xj as its neighbor is in proportion with the density under the Gaussian
4. a point closer to xi will be more likely than one further away.

Probabilities.
![[Pasted image 20240613161012.png]]
This probability $p_{j|i}$ is the probability that the point xi chooses xj as its neighbor.
The parameter $\sigma$ sets the size of the neighbourhood, think of it as a sort of radius. If very low sigma, all the probability is in the newest neighbor, while with a very high sigma the weights are uniform. *???*
We set a sigma differently for each data point.
$\sigma$ is a parameter tuned by the SNE approach and results depend heavily on it as it defines the neighbourhood we are trying to preserve.
The final distribution over pairs is symmetrized, because we want the distance ij to be the same as ji, meaning that pij = 1/2N(pi|j + pj|i)

Perplexity.
For each distribution p

...





















