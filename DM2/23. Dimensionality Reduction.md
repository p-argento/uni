## Outline
1. intro to dimensionality reduction
2. feature selection
	1. Variance Threshold
	2. Univariate Feature Selection (ie ANOVA F-value)
	3. RFE
3. feature projection
	1. Random Subspace Projection (RSP)
	2. PCA
	3. Classic MDS
	4. Sammon Mapping
	5. Isometric Feature Mapping (IsoMap)
	6. t-SNE

# Introduction to Dimensionality Reduction

Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables.
Approaches can be divided in
1. feature selection
	1. meaning remove some of the features
2. feature projection
	1. meaning obtain new features combining existing ones

# 1. Feature Selection

Why? Classification and regression can be done more in the reduced space rather than in the original space. This is because we can use faster algorithms, denser dataset and there is no curse of dimensionality.

Select a subset of the features according to different strategies
1. filter strategy
	1. ie information gain
2. wrapper startegy
	1. ie search guided by accuracy
3. embedded strategy
	1. selected features added or removed while building the model based on prediction errors

We discuss
1. Variance Threshold
2. Univariate Feature Selection (ie ANOVA F-value)
3. Recursive Feature Elimination (RFE)

## Variance Threshold

It removes all the features whose variance does not meet some threshold. By default, it removes all zero-variance features, meaning those that have the same value in all samples.

## Univariate Feature Selection

It selects the best features (or how many features to use?) based on univariate statistical tests. For instance, it removes all but the k highest scoring features.

An example of statistical test is the ANOVA F-value between label/feature.















