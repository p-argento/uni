## Outline
1. intro to dimensionality reduction
2. feature selection
	1. Variance Threshold
	2. Univariate Feature Selection (ie ANOVA F-value)
	3. RFE
3. feature projection
	1. Random Subspace Projection (RSP)
	2. PCA
	3. Classic MDS
	4. Sammon Mapping
	5. Isometric Feature Mapping (IsoMap)
	6. t-SNE

# Introduction to Dimensionality Reduction

Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables.
Approaches can be divided in
1. feature selection
	1. meaning remove some of the features
2. feature projection
	1. meaning obtain new features combining existing ones

# 1. Feature Selection

Why? Classification and regression can be done more in the reduced space rather than in the original space. This is because we can use faster algorithms, denser dataset and there is no curse of dimensionality.

Select a subset of the features according to different strategies
1. filter strategy
	1. ie information gain
2. wrapper startegy
	1. ie search guided by accuracy
3. embedded strategy
	1. selected features added or removed while building the model based on prediction errors

We discuss
1. Variance Threshold
2. Univariate Feature Selection (ie ANOVA F-value)
3. Recursive Feature Elimination (RFE)

## Variance Threshold

It removes all the features whose variance does not meet some threshold. By default, it removes all zero-variance features, meaning those that have the same value in all samples.

## Univariate Feature Selection

It selects the best features (or how many features to use?) based on univariate statistical tests. For instance, it removes all but the k highest scoring features.

An example of statistical test is the ANOVA F-value between label/feature.

![[Pasted image 20240613122009.png]]

where
- $Y_i$ denotes the sample mean in the $i^{th}$ group
- n is the number of observations in the $i^{th}$ group
- $\bar{Y}$ denotes the overall mean of the data
- $Y_{ij}$ is the observation in the  $i^{th}$ group out of K groups
- K denotes the number of groups
- N is the overall sample size

F-value is large if the numerator is large, which is unlikely to happen if the population of the groups all have the same value

## Recursive Feature Elimination (RFE)

Compared to variance threshold, the RFE requires training a model. There exists also a RFE-CV which uses the cross validation to improve the model used.

It removes redundant features first, meaning those that are highly correlated. Then, it uses more advanced techniques to remove less useful features.

Gi













