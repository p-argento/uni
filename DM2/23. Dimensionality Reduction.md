## Outline
1. intro to dimensionality reduction
2. feature selection
	1. Variance Threshold
	2. Univariate Feature Selection (ie ANOVA F-value)
	3. RFE
3. feature projection
	1. Random Subspace Projection (RSP)
	2. PCA
	3. Classic MDS
	4. Sammon Mapping
	5. Isometric Feature Mapping (IsoMap)
	6. t-SNE

# Introduction to Dimensionality Reduction

Dimensionality reduction is the process of reducing the number of variables under consideration by obtaining a set of principal variables.
Approaches can be divided in
1. feature selection
	1. meaning remove some of the features
2. feature projection
	1. meaning obtain new features combining existing ones

# 1. Feature Selection

Why? Classification and regression can be done more in the reduced space rather than in the original space. This is because we can use faster algorithms, denser dataset and there is no curse of dimensionality.

Select a subset of the features according to different strategies
1. filter strategy
	1. ie information gain
2. wrapper startegy
	1. ie search guided by accuracy
3. embedded strategy
	1. selected features added or removed while building the model based on prediction errors

We discuss
1. Variance Threshold
2. Univariate Feature Selection (ie ANOVA F-value)
3. Recursive Feature Elimination (RFE)

## Variance Threshold

It removes all the features whose variance does not meet some threshold. By default, it removes all zero-variance features, meaning those that have the same value in all samples.

## Univariate Feature Selection

It selects the best features (or how many features to use?) based on univariate statistical tests. For instance, it removes all but the k highest scoring features.

An example of statistical test is the ANOVA F-value between label/feature.

![[Pasted image 20240613122009.png]]

where
- $Y_i$ denotes the sample mean in the $i^{th}$ group
- n is the number of observations in the $i^{th}$ group
- $\bar{Y}$ denotes the overall mean of the data
- $Y_{ij}$ is the observation in the  $i^{th}$ group out of K groups
- K denotes the number of groups
- N is the overall sample size

F-value is large if the numerator is large, which is unlikely to happen if the population of the groups all have the same value

## Recursive Feature Elimination (RFE)

Compared to variance threshold, the RFE requires training a model. There exists also a RFE-CV which uses the cross validation to improve the model used.

It removes redundant features first, meaning those that are highly correlated. Then, it uses more advanced techniques to remove less useful features.

Given an external estimator that assigns weights to features, RFE selects features by recursively considering smaller and smaller sets of features.
The estimator can assign for example the coefficients of a linear model, or the feature importance of a decision tree.

Steps
1. train the estimator on the initial set of features and obtain the importance of each features
2. prune the least important features from the current set of features
3. recursively repeat on the pruned set until the desired number of features is reached.

The same technique but without recursion is called SelectFromModel in sklearn.

# 2. Feature Selection
aka Feature Extraction

It transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear or non-linear. Typically nonlinear cannot be inverted.

Approaches discussed
1. Random Subspace Projection (RSP)
2. Principal Component Analysis (PCA)
3. Multidimensional Scaling
	1. Sammon
	2. IsoMap
	3. t-SNE 

More approaches
1. Non-negative matrix factorization (NMF)
2. Linear Discriminant Analysis (LDA)
3. Autoencoder

## Random Subspace Projection (RSP)

High-dimensional data is projected into low-dimensional space using a random matrix R whose columns have unit length.

No attempt to find a criterion for optimization.
It preserve the structure of the data, meaning the distances between points.
It is computationally cheap but NOT stable and NOT meaningful.

![[Pasted image 20240613131036.png]]

## Principal Component Analysis (PCA)

...


## Multi-Dimensional Scaling (MDS)

MDS is a family of different algorithms designed to map data into a very low configuration, eg k=2 or K=3.
It cannot be inverted.

Given a pairwise dissimilarity matrix, the goal of MDS is to learn a mapping of data into a lower dimensionality such that the relative distances are preserved. It means that if two points are close in the feature space, it should be close also in the latent factor space.

Algorithm
1. given a pairwise dissimilarity matrix D and the dimensionality k
2. find a mapping such that $d_{ij}=||x_i-x_j||$ for all points in D
3. use gradient descend to minimize the function 
   J(x) = ∑in ∑jn d1(dij, d2(xi, xj))

![[Pasted image 20240613131731.png]]

Distance, dissimilarity and similarity (or proximity) are defined for any pair of objects in any space.
In mathematics, a distance function between two objects is called "metric", satisfying:
1. d(x, y) ≥ 0
2. d(x, y) = 0 iff x = y
3. d(x, y) = d(y, x)
4. d(x, z) ≤ d(x, y) + d(y, z)
If the last condition does not hold, then d is a distance function but it is NOT a metric.

Depending on the distances adopted to calculate D and the distance function used for d1 and d2, we distinguish
1. Classic MDS
	1. adopts euclidean distance for every calculus
2. Metric MDS
	1. adopts metrics as distances
3. Non-Metric MDS
	1. deals with ranks of distances instead of their values

## Summon Mapping




