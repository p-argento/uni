## Outline
1. introducing anomaly and outliers detections
2. visual approaches
	1. boxplot
	2. automatic boxplot
	3. hbos
3. statistical approaches
	1. Grubb's Test
	2. Likelihood Approach
4. depth-based approaches
	1. "convex hull" (no code)
	2. elliptical envelope
5. deviation-based approaches
	1. LMDD
6. distance-based approaches (or proximity-based)
	1. specified distance (Knorr, Ng 1998)
		1. index-based, nested-loop based, grid-based
	2. distance to kth nn
7. density-based approaches
	1. relative density *no code*
	2. local outlier factor (LOF)
	4. Conncectivity-based Outlier Factor (COF)
	5. Influenced Outlierness (INFLO) *no code*
	6. DBSCAN
8. clustering-based approaches *no code*
	1. distance of points from closest centroid
9. high-dimensional approaches
	1. Angle-based Outlier Degree (ABOD)
	2. Grid-based Subspace Outlier Detection *no code*
10. ensemble-approaches
	1. Feature Bagging (FeaBag)
	2. Lightweight OnlineDetector of Anomalies (LODA)
11. model-based approaches
		1. Isolation forest
	1. extended isolation forest

> Use pyod as the library for outliers detection, together with sklearn.

# Outline of notebook

1. Visual Approaches
	1. BoxPlots
	2. Automatic BoxPlots
	3. HBOS
2. Statistical Approaches
	1. Grubbs' Test
	2. Likelihood Approach
3. Depth-based Approaches
	1. Elliptical Envelope (EllEnv)
4. Deviation-based Approaches
	1. LMDD
5. Distance-based Approaches
	1. Knn
	2. RKnn
6. Density-based
	1. LOF
	2. COF
7. Clustering-based
	1. DBSCAN
8. High-dimensional Approaches
	1. ABOD
9. Ensemble Approaches
	1. Feature Bagging (FeaBag)
	2. LODA
10. Model-based
	1. Isolation Forest


# Anomalies and outliers

## Definition of outliers

Definition of Hawkins 1980.
"An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism."

We assume data is generated from a mechanism, for example social network data is generated by the mechanism of social network. An outlier is data generated by a different mechanism. Using a statistics-based intuition, we can then say that data objects follow a generating mechanism and abnormal objects deviate from this generating mechanism.

What are anomalies/outliers?
It is a set of data points that are considerably different than the rest of the data. They can be mistakes or important data.

Applications of outlier detection.
1. fraud detection (using abnormal buying patterns)
2. medicine (unusual symptomps)
3. public health

Causes of Anomalies
1. data from different classes (different fruits)
2. natural variation (unusual tall people)
3. data errors (200 pound children)

Distinction between noise and anomalies.
Noise and anomalies are related but distinct concepts.
Noise
1. is erroneous, perhaps random
2. doesn't necessarily produce unusual values
3. is NOT interesting (while anomalies are interesting)

## Issues in Anomaly Detection

First, the number of attributes.
Many anomalies are defined in terms of a single attribute. It can be hard to find an anomaly using all attributes, but it can exists.

Second, anomaly scoring.
Many anomaly detection techniques provide only a binary categorization (anomaly or not), while other approaches assign a score to all points, to measure the degree to which an object is an anomaly. For example, we could decide a threshold based on a distribution.
In the end, we often need a binary decision, still can be useful to have a score.

Third, efficiency and evaluation.
Should we find all anomalies at once or one at a time?
How do measure performance?

Examples of more detailed anomaly detection requests.
1. Given a data set D, find all data points x $\in$ D with anomaly scores greater than some threshold t 
2. Given a data set D, find all data points x $\in$ D having the top-n largest anomaly scores 
3. Given a data set D, containing mostly normal (but unlabeled) data  points, and a test point x, compute the anomaly score of x with respect to D

## Model types

We can build a model for the data and observe the results
1. supervised
	1. anomalies are treated as a rare class in a extremely imbalanced set
	2. we need to know the groundtruth for training (not easy in real life)
2. unsupervised
	1. anomalies are those points that don't fit well and distort the model
	2. for example in statistical distributions, clusters, regression, ...

In a supervised setting, in particular, if the groudtruth of anomalies is available, we can use a classification problem with ML approaches like ensembles, svm, dnn. But remember that the dataset would be very unbalanced and thus ad-hoc implementations should be adopted.

In a unsupervised setting, we can distinguish techinques in
1. proximity-based
	1. anomalies are points far away from other points
	2. (can be detected graphically)
2. density-based
	1. anomalies are low density points
	2. for example, noise points obtained with DBSCAN (but be careful because it was not built for this)
3. pattern-matching
	1. creating profiles of atypical but interesting events
	2. these algorithms are usually simple and efficient

## Approaches Taxonomy

More taxonomies for outlier detection approaches
1. global vs local
2. labeling vs scoring outliers
3. modeling properties

*Global vs Local Approaches.*
Consider the set of reference objects (meaning training set) relative to how the outlierness of a particular data object is determined. Some approaches can be somewhere in between.
The resolution of the reference set can range from only a single object (local) to the entire database (globally), or a user-defined input parameter.

| Global                                                                                         | Local                                                        |
| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| 1. the reference set contains all other data objects                                           | 1. the reference contains a small subset of data objects     |
| 2. the assumption is that there is only one normal mechanism                                   | 2. there is NO assumption on the number of normal mechanisms |
| 3. the problem is that other outliers are also in the reference set and may falsify the result | 3. the problem is how to choose a proper reference set       |

*Labeling vs Scoring approaches.*
Consider the output of an outlier detection algorithm.
Many scoring approaches focus on determining the top-n outliers, where n is user-defined. Scoring can be used together with a threshold to obtain binary output.

| Labeling                                             | Scoring                                                                                           |
| ---------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| Binary output                                        | Continuous attribute                                                                              |
| Data objects are labeled either as normal or outlier | Data objects can be sorted according to their score (meaning the probability of being an outlier) |

*Modeling properties ???*
Approaches classified by the properties of the underlying model
1. (statistical-based) apply a model to represent normal data points, so that  outliers are points that do not fit to that model
	1. examples of approaches
		1. probabilistic tests-based on statistical models
		2. depth-based approaches
		3. deviation-based approaches
2. (proximity-based) examine the spatial proximity of each object in the data space and if the proximity of an object considerably deviates from the proximity of other objects it is considered an outlier
	1. examples of approaches
		1. distance-based approaches
		2. density-based approaches
3. (angle-based) examine the spectrum of pairwise angles between a given point and all other points, outliers are points that have a spectrum featuring high fluctuations
	1. in other words, a point that is central in a dataset, if you use that point as a vertex of an angle using any other couple of points, if the angle they create is very different, it means that the points are central; otherwise, if the vertex is marginal, the angles will be similar

# 1. Visual Approaches

For example, boxplots or scatterplots.
However, they are not automatic and can be subjective. 

## Box Plots

How to use box-plots?
1. Calculate the IQR of a set of values as the difference between Q3 and Q1.
2. Then, draw the whiskers as the highest and lowest values within the range $[Q1-k*IQR; Q3+k*IQR]$ with k set usually to 1.5 (or 3 or 5 for stronger outliers).
3. Points outside the the range of whiskers are considered outliers and drawn as single points.

![[Pasted image 20240614163143.png]]

## Histogram-based Outlier Score (HBOS)

It means "Histogram-based Outlier Score" and it is a very efficient algo.

It assumes features independence and calculate the outlier score by building histograms.

Univariate histogram for each single feature.
1. categorical data
	1. simple counting
2. numerical data
	1. k bins having equal width
	2. N/k instances per bin having equal frequency
		1. frequency of records in a bin is used as a density estimation

Histograms are normalized to $[0,1]$ for each single feature.

HBOS for each record p is computed as a product of the inverse of the estimated density $hist(p)$.

  ![[Pasted image 20240614164158.png]]

# Statistical approaches

(in my opinion maybe better to call them probabilistic approaches like in pyod)
Using probabilistic definition of an outlier.
An outlier is an object that has a low probability with respect to a probability distribution model of the data.

Apply a statistical test that depends on
1. data distribution assumed (eg normal distribution)
2. parameters of distribution (eg mean, variance)
3. number of expected outliers (confidence limit)

The issues are
1. identifying the distribution of a data set
2. number of attributes
3. is the data a mixture of distributions?

The strengths and weaknesses of statistical approaches are

| pros                                                                                                       | cons                                                                                                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. firm mathematical foundation<br>2. can be very efficient<br>3. good result if the distribution is known | 1. in many cases, distribution is not known<br>2. for high dimensional data, it may be difficult to estimate the true distribution<br>3. mean and standard deviation are very sensitive to outliers and can easily distort the distribution |

## Grubbs' Test
(no libraries, see notebooks)

Detect outliers in univariate data.
Assume data comes from a normal distribution.
Detects one outlier at a time, remove it and repeat.

Grubbs' test statistic is
![[Pasted image 20240614172455.png]]

The null hypothesis H0 of no outliers is rejected, meaning that the point is an outlier, if
![[Pasted image 20240614172549.jpg]]

## Likelihood Approach

Assume the data set D contains samples from a mixture of two probabilities distributions
1. Majority distribution M
2. Anomalous distribution A

General Approach
1. initially assume all the data points belong to M
2. let $L_t(D)$ be the log likelihood of D at time t
3. for each point $x_t$ that belongs to M, move it to A
	1. let $L_{t+1}(D)$ be the new log likelihood
	2. compute the difference, $\Delta = L_t(D) – L_{t+1}(D)$
	3. if $\Delta >c$ , where c is a threshold, then $x_t$ is declared as an anomaly and moved permanently from M to A

More in detail.
![[Pasted image 20240614173648.png]]

# 3. Depth-based Approaches

This is a global approach, because it uses a global reference set.

General idea.
Search for outliers at the border of the data space but independent of statistical distributions.

Organize data objects in convex hull layers. The depth=1 is the outermost convex hull, that in bidimensional euclidean space, it is a convex polygon.

The assumption is that outliers are located at the border of the data space, while normal objects are in the center of the data space. Imagine that the majority of data lies in the middle, while outliers are objects on outer layers, meaning on the border.
However, if this assumption is not true, the method cannot work.

![[Pasted image 20240614181956.png]]

## Depth-based algo

Model of Tukey 1977.
Steps.
1. Points on the convex hull of the full data space have depth =1.
2. Points on the convex hull of the data set after removing all points with depth = 1 have depth =2
3. repeat until all points having depth <= k have been removed, meaning that they are reported as outliers

![[Pasted image 20240614182245.png]]

Convex hull computation is usually only efficient in 2D / 3D spaces.
Originally outputs a label but can be extend for scoring easily, taking depth as a scoring value.

The sample algorithms are
1. ISODEPTH -> Ruts and Rousseuw 1996
2. FDC -> Johnson et al. 1998

> try exercises on slides

## Elliptic Envelope

This is the second depth-based approach.
The assumptions are the same as before.
It creates an imaginary elliptical area around a given dataset.

Values that fall inside the envelope are considered normal data and anything outside the envelope is returned as an outlier.

The algorithm works best if data has a Gaussian distibution.

![[Pasted image 20240615150818.png]]

# 4. Deviation-based Approaches

Together with test-based approach and depth-based approach, this is the third type of approach build on top of the statistical definition of outlier. That is: apply a model to represent normal data points, so that  outliers are points that do not fit to that model.

It does not rely on a specific distribution of the data.
It is a global method.

General idea.
Given a set of data points (local group or global set).
Outliers are points that do not fit to the general characteristic of that set.
For example, the variance of the set is minimized when removing the outliers. For instance, removing bill gates the variance will vary a lot.

Basic assumption.
Outliers are the outermost points of the data set.

## LMDD
In [pyod](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD) is called Linear Model Deviation-base outlier detection (LMDD). Model by Arning et al. 1996.

The smaller number of points that, if removed, minimize the variance of the data.

Steps
1. Given a smoothing factor SF(I) that computes, for each I $\subseteq$ DB, how much the variance is decreased when I is removed from DB
2. with equal decrease in variance, a smaller exception set E is better
3. the outliers are the elements of E $\subseteq$ DB for which the following holds SF(E) $\geq$ SF(I) for all I $\subseteq$ DB

It is a similar idea like classical statistical approaches (k=1 distributions) but independent from the chosen kind of distribution.
Naive solution is in O(2n) for n data objects.
Heuristics like random sampling or best first search are applied.
Applicable to any data type (depends on the definition of SF).
Originally designed as a global method.
Outputs a label.

# 5. Distance-based Approaches

Together with density-based approaches, it uses the proximity-based definition of outlier. That is: examine the spatial proximity of each object in the data space and if the proximity of an object considerably deviates from the proximity of other objects it is considered an outlier.





# 6. Density-based Approaches





















