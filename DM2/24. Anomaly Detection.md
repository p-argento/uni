# Outline

Slides
1. introducing anomaly and outliers detections
2. visual approaches
	1. boxplot
	2. automatic boxplot
	3. hbos
3. statistical approaches
	1. Grubb's Test
	2. Likelihood Approach
4. depth-based approaches
	1. "convex hull" (no code)
	2. elliptical envelope
5. deviation-based approaches
	1. LMDD
6. distance-based approaches (or proximity-based)
	1. specified distance (Knorr, Ng 1998)
		1. index-based, nested-loop based, grid-based
	2. distance to kth nn
7. density-based approaches
	1. relative density *no code*
	2. local outlier factor (LOF)
	4. Conncectivity-based Outlier Factor (COF)
	5. Influenced Outlierness (INFLO) *no code*
	6. DBSCAN
8. clustering-based approaches *no code*
	1. distance of points from closest centroid
9. high-dimensional approaches
	1. Angle-based Outlier Degree (ABOD)
	2. Grid-based Subspace Outlier Detection *no code*
10. ensemble-approaches
	1. Feature Bagging (FeaBag)
	2. Lightweight OnlineDetector of Anomalies (LODA)
11. model-based approaches
		1. Isolation forest
	1. extended isolation forest

> Use pyod as the library for outliers detection, together with sklearn.

Exercises on
1. distance-based
2. density-based
3. depth-based

## Outline of notebook

1. Visual Approaches
	1. BoxPlots
	2. Automatic BoxPlots
	3. HBOS
2. Statistical Approaches
	1. Grubbs' Test
	2. Likelihood Approach
3. Depth-based Approaches
	1. Elliptical Envelope (EllEnv)
4. Deviation-based Approaches
	1. LMDD
5. Distance-based Approaches
	1. Knn
	2. RKnn
6. Density-based
	1. LOF
	2. COF
7. Clustering-based
	1. DBSCAN
8. High-dimensional Approaches
	1. ABOD
9. Ensemble Approaches
	1. Feature Bagging (FeaBag)
	2. LODA
10. Model-based
	1. Isolation Forest

I think the idea of the taxonomy comes from [here](https://imada.sdu.dk/u/zimek/publications/KDD2010/kdd10-outlier-tutorial.pdf)
![[Pasted image 20240616110307.png]]
But also ispired from [book slides](https://www-users.cse.umn.edu/~kumar001/dmbook/slides/chap9_anomaly_detection.pdf)
![[Pasted image 20240616112014.png]]


# Anomalies and outliers

## Definition of outliers

Definition of Hawkins 1980.
"An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism."

We assume data is generated from a mechanism, for example social network data is generated by the mechanism of social network. An outlier is data generated by a different mechanism. Using a statistics-based intuition, we can then say that data objects follow a generating mechanism and abnormal objects deviate from this generating mechanism.

What are anomalies/outliers?
It is a set of data points that are considerably different than the rest of the data. They can be mistakes or important data.

Applications of outlier detection.
1. fraud detection (using abnormal buying patterns)
2. medicine (unusual symptomps)
3. public health

Causes of Anomalies
1. data from different classes (different fruits)
2. natural variation (unusual tall people)
3. data errors (200 pound children)

Distinction between noise and anomalies.
Noise and anomalies are related but distinct concepts.
Noise
1. is erroneous, perhaps random
2. doesn't necessarily produce unusual values
3. is NOT interesting (while anomalies are interesting)

## Issues in Anomaly Detection

First, the number of attributes.
Many anomalies are defined in terms of a single attribute. It can be hard to find an anomaly using all attributes, but it can exists.

Second, anomaly scoring.
Many anomaly detection techniques provide only a binary categorization (anomaly or not), while other approaches assign a score to all points, to measure the degree to which an object is an anomaly. For example, we could decide a threshold based on a distribution.
In the end, we often need a binary decision, still can be useful to have a score.

Third, efficiency and evaluation.
Should we find all anomalies at once or one at a time?
How do measure performance?

Examples of more detailed anomaly detection requests.
1. Given a data set D, find all data points x $\in$ D with anomaly scores greater than some threshold t 
2. Given a data set D, find all data points x $\in$ D having the top-n largest anomaly scores 
3. Given a data set D, containing mostly normal (but unlabeled) data  points, and a test point x, compute the anomaly score of x with respect to D

## Model types

We can build a model for the data and observe the results
1. supervised
	1. anomalies are treated as a rare class in a extremely imbalanced set
	2. we need to know the groundtruth for training (not easy in real life)
2. unsupervised
	1. anomalies are those points that don't fit well and distort the model
	2. for example in statistical distributions, clusters, regression, ...

In a supervised setting, in particular, if the groudtruth of anomalies is available, we can use a classification problem with ML approaches like ensembles, svm, dnn. But remember that the dataset would be very unbalanced and thus ad-hoc implementations should be adopted.

In a unsupervised setting, we can distinguish techinques in
1. proximity-based
	1. anomalies are points far away from other points
	2. (can be detected graphically)
2. density-based
	1. anomalies are low density points
	2. for example, noise points obtained with DBSCAN (but be careful because it was not built for this)
3. pattern-matching
	1. creating profiles of atypical but interesting events
	2. these algorithms are usually simple and efficient

## Approaches Taxonomy

More taxonomies for outlier detection approaches
1. global vs local
2. labeling vs scoring outliers
3. modeling properties

*Global vs Local Approaches.*
Consider the set of reference objects (meaning training set) relative to how the outlierness of a particular data object is determined. Some approaches can be somewhere in between.
The resolution of the reference set can range from only a single object (local) to the entire database (globally), or a user-defined input parameter.

| Global                                                                                         | Local                                                        |
| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| 1. the reference set contains all other data objects                                           | 1. the reference contains a small subset of data objects     |
| 2. the assumption is that there is only one normal mechanism                                   | 2. there is NO assumption on the number of normal mechanisms |
| 3. the problem is that other outliers are also in the reference set and may falsify the result | 3. the problem is how to choose a proper reference set       |

*Labeling vs Scoring approaches.*
Consider the output of an outlier detection algorithm.
Many scoring approaches focus on determining the top-n outliers, where n is user-defined. Scoring can be used together with a threshold to obtain binary output.

| Labeling                                             | Scoring                                                                                           |
| ---------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| Binary output                                        | Continuous attribute                                                                              |
| Data objects are labeled either as normal or outlier | Data objects can be sorted according to their score (meaning the probability of being an outlier) |

*Modeling properties*
Approaches classified by the properties of the underlying modeling approach.
1. (model-based) apply a model to represent normal data points, so that  outliers are points that do not fit to that model
	1. examples of approaches
		1. probabilistic tests-based on statistical models
		2. depth-based approaches
		3. deviation-based approaches
2. (proximity-based) examine the spatial proximity of each object in the data space and if the proximity of an object considerably deviates from the proximity of other objects it is considered an outlier
	1. examples of approaches
		1. distance-based approaches
		2. density-based approaches
3. (angle-based) examine the spectrum of pairwise angles between a given point and all other points, outliers are points that have a spectrum featuring high fluctuations
	1. in other words, a point that is central in a dataset, if you use that point as a vertex of an angle using any other couple of points, if the angle they create is very different, it means that the points are central; otherwise, if the vertex is marginal, the angles will be similar
	2. example of approaches
		1. ABOD

# 1. Visual Approaches

For example, boxplots or scatterplots.
However, they are not automatic and can be subjective. 

## Box Plots

How to use box-plots?
1. Calculate the IQR of a set of values as the difference between Q3 and Q1.
2. Then, draw the whiskers as the highest and lowest values within the range $[Q1-k*IQR; Q3+k*IQR]$ with k set usually to 1.5 (or 3 or 5 for stronger outliers).
3. Points outside the the range of whiskers are considered outliers and drawn as single points.

![[Pasted image 20240614163143.png]]

## Histogram-based Outlier Score (HBOS)

It means "Histogram-based Outlier Score" and it is a very efficient algo.

It assumes features independence and calculate the outlier score by building histograms.

Univariate histogram for each single feature.
1. categorical data
	1. simple counting
2. numerical data
	1. k bins having equal width
	2. N/k instances per bin having equal frequency
		1. frequency of records in a bin is used as a density estimation

Histograms are normalized to $[0,1]$ for each single feature.

HBOS for each record p is computed as a product of the inverse of the estimated density $hist(p)$.

  ![[Pasted image 20240614164158.png]]

# Statistical approaches

(in my opinion maybe better to call them probabilistic approaches like in pyod)
Using probabilistic definition of an outlier.
An outlier is an object that has a low probability with respect to a probability distribution model of the data.

Apply a statistical test that depends on
1. data distribution assumed (eg normal distribution)
2. parameters of distribution (eg mean, variance)
3. number of expected outliers (confidence limit)

The issues are
1. identifying the distribution of a data set
2. number of attributes
3. is the data a mixture of distributions?

The strengths and weaknesses of statistical approaches are

| pros                                                                                                       | cons                                                                                                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. firm mathematical foundation<br>2. can be very efficient<br>3. good result if the distribution is known | 1. in many cases, distribution is not known<br>2. for high dimensional data, it may be difficult to estimate the true distribution<br>3. mean and standard deviation are very sensitive to outliers and can easily distort the distribution |

## Grubbs' Test
(no libraries, see notebooks)

Detect outliers in univariate data.
Assume data comes from a normal distribution.
Detects one outlier at a time, remove it and repeat.

Grubbs' test statistic is
![[Pasted image 20240614172455.png]]

The null hypothesis H0 of no outliers is rejected, meaning that the point is an outlier, if
![[Pasted image 20240614172549.jpg]]

## Likelihood Approach

Assume the data set D contains samples from a mixture of two probabilities distributions
1. Majority distribution M
2. Anomalous distribution A

General Approach
1. initially assume all the data points belong to M
2. let $L_t(D)$ be the log likelihood of D at time t
3. for each point $x_t$ that belongs to M, move it to A
	1. let $L_{t+1}(D)$ be the new log likelihood
	2. compute the difference, $\Delta = L_t(D) – L_{t+1}(D)$
	3. if $\Delta >c$ , where c is a threshold, then $x_t$ is declared as an anomaly and moved permanently from M to A

More in detail.
![[Pasted image 20240614173648.png]]

# 3. Depth-based Approaches

This is a global approach, because it uses a global reference set.

General idea.
Search for outliers at the border of the data space but independent of statistical distributions.

Organize data objects in convex hull layers. The depth=1 is the outermost convex hull, that in bidimensional euclidean space, it is a convex polygon.

The assumption is that outliers are located at the border of the data space, while normal objects are in the center of the data space. Imagine that the majority of data lies in the middle, while outliers are objects on outer layers, meaning on the border.
However, if this assumption is not true, the method cannot work.

![[Pasted image 20240614181956.png]]

## Depth-based algo

Model of Tukey 1977.
Steps.
1. Points on the convex hull of the full data space have depth =1.
2. Points on the convex hull of the data set after removing all points with depth = 1 have depth =2
3. repeat until all points having depth <= k have been removed, meaning that they are reported as outliers

![[Pasted image 20240614182245.png]]

Convex hull computation is usually only efficient in 2D / 3D spaces.
Originally outputs a label but can be extend for scoring easily, taking depth as a scoring value.

The sample algorithms are
1. ISODEPTH -> Ruts and Rousseuw 1996
2. FDC -> Johnson et al. 1998

> try exercises on slides

## Elliptic Envelope

This is the second depth-based approach.
The assumptions are the same as before.
It creates an imaginary elliptical area around a given dataset.

Values that fall inside the envelope are considered normal data and anything outside the envelope is returned as an outlier.

The algorithm works best if data has a Gaussian distibution.

![[Pasted image 20240615150818.png]]

# 4. Deviation-based Approaches

Together with test-based approach and depth-based approach, this is the third type of approach build on top of the statistical definition of outlier. That is: apply a model to represent normal data points, so that  outliers are points that do not fit to that model.

It does not rely on a specific distribution of the data.
It is a global method.

General idea.
Given a set of data points (local group or global set).
Outliers are points that do not fit to the general characteristic of that set.
For example, the variance of the set is minimized when removing the outliers. For instance, removing bill gates the variance will vary a lot.

Basic assumption.
Outliers are the outermost points of the data set.

## LMDD
In [pyod](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.lmdd.LMDD) is called Linear Model Deviation-base outlier detection (LMDD). Model by Arning et al. 1996.

The smaller number of points that, if removed, minimize the variance of the data.

Steps
1. Given a smoothing factor SF(I) that computes, for each I $\subseteq$ DB, how much the variance is decreased when I is removed from DB
2. with equal decrease in variance, a smaller exception set E is better
3. the outliers are the elements of E $\subseteq$ DB for which the following holds SF(E) $\geq$ SF(I) for all I $\subseteq$ DB

It is a similar idea like classical statistical approaches (k=1 distributions) but independent from the chosen kind of distribution.
Naive solution is in O(2n) for n data objects.
Heuristics like random sampling or best first search are applied.
Applicable to any data type (depends on the definition of SF).
Originally designed as a global method.
Outputs a label.

# 5. Distance-based Approaches

Together with density-based approaches, it uses the proximity-based definition of outlier. That is: examine the spatial proximity of each object in the data space and if the proximity of an object considerably deviates from the proximity of other objects it is considered an outlier.

In the [article review](https://dl.acm.org/doi/10.1145/1541880.1541882) the proximity-based approaches are called "NEAREST NEIGHBOR-BASED ANOMALY DETECTION TECHNIQUES".

They are similar to clustering-based techniques. The choice of the distance measure is critical to the performance of the technique. The key difference between the two techniques, however, is that clustering-based techniques evaluate each instance with respect to the cluster it belongs to, while nearest neighbor-based techniques analyze each instance with respect to its local neighborhood.

General idea.
Judge a point based on the distance(s) to its neighbors.

Basic Assumption.
Normal data objects have a dense neighborhood. Outliers are far apart from their neighbors, having a less dense neighborhood.

There are two categories.
1. techniques that use the distance of a data instance to its kth nearest neighbor as the anomaly score
	1. here called distance-based
	2. the outlier score of an object is the distance to its k-th nearest neighbor
2. techniques that compute the relative density of each data instance to compute its anomaly score
	1. here called density-based
	2. an object is an outlier if a specified fraction of the objects is more than a specified distance away (Knorr, Ng 1998)


*This section needs to be more clear. Try to look at the sources.*

## kNN
Outliers based on kNN-distance

General models for outlier scoring based on kNN distances.
1. the outlier score is the distance to the kth nearest neighbor
	1. Ramaswamy et al 2000
2. aggregate the distances of a point to all its 1NN, 2NN, ..., kNN as an outlier score
	1. Angiulli and Pizzuti 2002
3. using in-degree number
	1. see below

Be careful when choosing a low K, like K=2, because in the example below the two points are not considered outliers, while they clearly are.
![[Pasted image 20240616112955.png]]

Algorithm approaches
1. nested-loop
	1. naive: for each object, compute kNN with a sequential scan
	2. enhanced: use index structures for kNN queries
2. partition-based
	1. partition data into micro-clusters and aggregate information for each partition (eg minimum bounding rectangles)
		1. allows to prune micro clusters that cannot qualify when searching for the kNN of a particular point

## DB(ε,π)-outlier model
Distance-Based algorithms from [article](https://www.vldb.org/conf/1998/p392.pdf)
Knorr and Ng Algorithms

$DB(\epsilon,\pi)$ - outliers
Basic model of Knorr and Ng 1997.
Given a radius $\epsilon$ and a percentage $\pi$, a point p is considered an outlier if at most $\pi$ percent of all other points have a distance to p less than $\epsilon$, meaning it is close to few points.

![[Pasted image 20240616113522.png]]
![[Pasted image 20240616113514.png]]

More algorithms in the article of Knorr and Ng 1998.
1. Index-based
	1. compute distance range join using spatial index structure
	2. exclude point from further consideration if its epsilkon-neighborhood contains more than Card(DB) pi points
2. nested-loop based
	1. divide buffer in two parts
	2. use second part to scan/compare all points with the points from the first part
3. grid-based
	1. build grid such that any two points from the same grid cell have a distance of at most $\epsilon$ to each other
	2. points need only compare with points from neighboring cells

## RkNN
Outlier detection using in-degree number.

The idea is to construct the kNN graph for a data set, where the vertices are the data points and, if $q\in kNN(p)$, then there is a directed edge from p to q. If a vertex has an in-degree number $\leq$ threshold T, then it is an outlier.

The in-degree of a vertex in the kNN graph equals to the number of reverse kNNs (RkNN) of the corresponding point. The RkNNs of a point p are those data objects having p among their kNNs. The outliers are the points that are among the kNNs of less than T other points, meaning less than T RkNNs.

![[Pasted image 20240616114535.png]]

Note that the arrow of the distance might not be reciprocal. Think of it as a social network. The more follower, the less you are an outlier. In the image, A is not in the RkNN of anyone.

It is a local approach and outputs a label.
Depends on user-defined parameter k.

## Comment on distance-based approaches
Pro
1. simple
Cons
1. expensive O(n2)
2. sensitive to parameters
3. sensitive to variations in density
4. distance becomes less meaningful in high dimensional space


# 6. Density-based Approaches

Together with distance-based approaches, it uses the proximity-based definition of outlier. That is: examine the spatial proximity of each object in the data space and if the proximity of an object considerably deviates from the proximity of other objects it is considered an outlier.

In the article review, it is in the group of Nearest Neighbors-based techniques.

## Relative Density Outlier Score

The general idea is to compare the density around a point with the density around its local neighbors. The relative density of a point compared to its neighbors is computed as an outlier score. Approaches differ in how to estimate density.

The basic assumption is that the density around a normal data object is similar to the density around its neighbors. The density around an outlier is considerably different (lower) to the density around its neighbors.

The outlier score of an object is the inverse of the density around the object. Can be defined in terms of the k nearest neighbors as
1. the inverse of distance to kth neighbor
2. the inverse of the average distance to k neighbors
3. DBSCAN defintion

There could be problems if there are regions of different density, exactly like happens with dbscan.

![[Pasted image 20240616121751.png]]

## LOF
Local Outlier Factor (LOF)
`[Breunig et al. 1999], [Breunig et al. 2000]`

Motivation.
Distance-based outlier detection models have problems with different densities. How to compare the neighborhood of points from areas of different densities? The solution is to consider the relative density.

Looking at the image below, previous algorithms cannot work.
1. in the "DB(ε,π)-outlier model", the parameters ε,π cannot be chose so that o2 is an outlier but none of the points in cluster C1 (like q) is an outlier.
2. in the "outliers detection based on kNN-distance", kNN-distances of objects in C1 (like q) are larger than the kNN-distance of o2.

![[Pasted image 20240616124329.png]]

The LOF algorithm considers relative densities.
For each point, compute the density of its local neighborhood.
Compute local outlier factor (LOF) of a sample p as the average of the ratios of the density of sample p and the density of its nearest neighbors. Outliers are the points with largest LOF value.
In the example above, LOF will find both o1 and o2 as outliers.

Exactly like the advantage of OPTICS compared to DBSCAN, it avoids the problem with different densities.

Concepts.
![[Pasted image 20240616162440.png]]
1. reachability distance
	1. introducing a smoothing factor
![[Pasted image 20240616162534.png]]
2. local reachability distance (LRD) of point p
	1. inverse of the average reach-dist of the kNNs of p
![[Pasted image 20240616162622.png]]
3. local outlier factor (LOF) of point p
	1. average ratio of LDRs of neighbors of p and LRD of p
![[Pasted image 20240616162759.png]]

Properties.
1. if LOF ~ 1
	1. point is in a cluster (region with homogeneous density around the point and its neighbors)
2. if LOF >> 1
	1. point is an outlier

Discussion.
The choice of k (called MinPts in the original paper) specifies the reference set.
Originally implements a local approach, but the resolution depends on the user choice of k.
Outputs a scoring, assigning a LOF value to each point.


## COF
Connectivity-based outlier factor (COF).
Tan et al 2002.

Motivation.
In regions of low density, it may be hard to detect outliers.
Choose a low value for k is often not appropriate.
The solution is to treat "low density" and "isolation" differently.

For example.
![[Pasted image 20240616163300.png]]

It was introduced because although a high-density set can represent a pattern, not all patterns need to be high-density.

COF differs from LOF as it uses the chaining distance to calculate kNN. The chaining distances are the minimum of the total sum of the distances linking all neighbors. 
The connectivity is then calculated as the ratio between the average chaining distance of the record and the mean average chaining distance of the records in the kNN.
Chain relates to the graph representation of the dataset. It uses the connectivity between points.

![[Pasted image 20240616163854.png]]

## INFLO *
Influenced Outlierness.
Jin et al. 2006.
This is not in the notebook. Similar to LOF.

Motivation.
If clusters of different densities are not clearly separated, LOF will have problems.
The idea is to take symmetric neighborhood relationship into account. The Influence Space kIS(p) of a point p includes its kNNs (kNN(p)) and its reverse kNNs (RkNN(p)).

![[Pasted image 20240616164138.png]]

Model.
1. density is simply measured by the inverse of the kNN distance, meaning
	1. den(p)=1/kdistance(p)
2. influenced outlierness (INFLO) of a point p
	1. INFLO takes the ratio of the average density of objects in the neighborhood of a point p (kNN(p) U RkNN(p)) to p's density
 ![[Pasted image 20240616164309.png]]

Properties.
If INFLO ~ 1 -> point is in a cluster
If INFLO >> 1 -> point is an outlier

Discussion.
Similar to LOF.
Outputs an outlier score.
Originally proposed as a local approach, but resolution of the reference set kIS can be adjusted by the user setting parameter k.

## ??? MINING TOP-N LOCAL OUTLIERS ???
This slide disappeared from new versions.
But in the guidelines it is asked to find top-1% outliers.
![[Pasted image 20240616164556.png]]
![[Pasted image 20240616164609.png]]

## Comment on density-based
They are all very similar, so learn the core concepts and then learn the differences.
There are no models here, we are just computing the distances.

| Pros   | Cons                                                      |
| ------ | --------------------------------------------------------- |
| Simple | Expensive                                                 |
|        | sensitive to parameters                                   |
|        | density becomes less meaningful in high-dimensional space |





# 7. Clustering-based Approaches

They are similar to nearest neighbor-based techniques. The choice of the distance measure is critical to the performance of the technique. The key difference between the two techniques, however, is that clustering-based techniques evaluate each instance with respect to the cluster it belongs to, while nearest neighbor-based techniques analyze each instance with respect to its local neighborhood.

## DBSCAN



## CBLOF





# 8. High dimensional Approaches

## ABOD



## Grid-based Subspace Outlier Detection





# 9. Ensemble Approaches

## FeaBag



## LODA



# Model-based Approaches


## Isolation Forest
See more [here](https://arxiv.org/pdf/1811.02141)













