## Outline
1. introducing anomaly and outliers detections
2. model-based approaches
3. statistical approaches
	1. Grubb's Test
	2. Likelihood Approach
4. depth-based approaches
	1. "convex hull" of Tukey 1977
5. deviation-based approaches
	1. "Arning 1996"
6. distance-based approaches
	1. specified distance (Knorr, Ng 1998)
		1. index-based, nested-loop based, grid-based
	2. distance to kth nn
7. density-based approaches
	1. relative density
	2. local outlier factor (LOF)
	3. Mining Top-n Local Outliers ??
	4. Conncectivity-based Outlier Factor (COF)
	5. Influenced Outlierness (INFLO)
8. clustering-based approaches
	1. distance of points from closest centroid
9. high-dimensional approaches
	1. Angle-based Outlier Degree (ABOD)
	2. Grid-based Subspace Outlier Detection
10. ensemble-approaches
	1. Feature Bagging (FeaBag)
	2. Lightweight OnlineDetector of Anomalies (LODA)
11. model-based approaches
	1. Isolation forest
	2. extended isolation forest

# Anomalies and outliers

Definition of Hawkins 1980.
"An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism."

We assume data is generated from a mechanism, for example social network data is generated by the mechanism of social network. An outlier is data generated by a different mechanism. Using a statistics-based intuition, we can then say that data objects follow a generating mechanism and abnormal objects deviate from this generating mechanism.

What are anomalies/outliers?
It is a set of data points that are considerably different than the rest of the data. They can be mistakes or important data.

Applications of outlier detection.
1. fraud detection (using abnormal buying patterns)
2. medicine (unusual symptomps)
3. public health

Causes of Anomalies
1. data from different classes (different fruits)
2. natural variation (unusual tall people)
3. data errors (200 pound children)

Distinction between noise and anomalies.
Noise and anomalies are related but distinct concepts.
Noise
1. is erroneous, perhaps random
2. doesn't necessarily produce unusual values
3. is NOT interesting (while anomalies are interesting)

## Issues in Anomaly Detection
First, the number of attributes.
Many anomalies are defined in terms of a single attribute. It can be hard to find an anomaly using all attributes, but it can exists.

Second, anomaly scoring.
Many anomaly detection techniques provide only a binary categorization (anomaly or not), while other approaches assign a score to all points, to measure the degree to which an object is an anomaly. For example, we could decide a threshold based on a distribution.
In the end, we often need a binary decision, still can be useful to have a score.

Third, efficiency and evaluation.
Should we find all anomalies at once or one at a time?
How do measure performance?

Examples of more detailed anomaly detection requests.
1. Given a data set D, find all data points x $\in$ D with anomaly scores greater than some threshold t 
2. Given a data set D, find all data points x $\in$ D having the top-n largest anomaly scores 
3. Given a data set D, containing mostly normal (but unlabeled) data  points, and a test point x, compute the anomaly score of x with respect to D

# Model-based approaches

We can build a model for the data and observe the results
1. supervised
	1. anomalies are treated as a rare class in a extremely imbalanced set
	2. we need to know the groundtruth for training (not easy in real life)
2. unsupervised
	1. anomalies are those points that don't fit well and distort the model
	2. for example in statistical distributions, clusters, regression, ...

In a supervised setting, in particular, if the groudtruth of anomalies is available, we can use a classification problem with ML approaches like ensemles, svm, dnn. But remember that the dataset would be very unbalanced and thus ad-hoc implementations should be adopted.

In a unsupervised setting, we can distinguish techinques in
1. proximity-based
	1. anomalies are 
2. density-based
3. pattern-matching















