# Introduction to Time Series
## What is a Time Series?
It is a collection of observations made sequentially in time.
Generally at constant time intervals, which are on the x-axis.
Note that time instances can also be columns, instead of rows, and this is relevant especially when applying an operation to timestamps.

Some examples of TS are:
..


## Problems with TS
Some problems are
1. large amount of data
2. similarity not easy to estimate
3. different data formats
4. different sampling rates
5. noise, missing values, etc

## Tasks on TS
What we will do
1. Clustering
2. Motif Discovery
3. Rule Discovery
4. Classification
Other tasks not covered in the course
1. trends,seasonality
2. forecasting

All these tasks require *similarity matching*.

## Similarity
What is similarity?
It is the quality or state of being similar, likeness, resemblance, as a similarity of features.
In TS we recognize two kinds of similarity.
1. structural-based similarities
	1. meaning similarities at the structural level, the actual content
2. shape-based similarities
	1. meaning similarity at the level of the shape
	2. can not be enough to distinguish different TS

# 1. Structural-based Similarities

## Structural-based Similarities
Also called model-based similarity.
For very long time series, shape-based similarity give very poor results.
Therefore, we need to measure similarity based on high-level structure.

The idea is to turn the TS into a vector containing global features from the TS and use it to measure similarity or classify.
These features can be for example:
1. mean, variance, skewness, kurtosis
2. 1st derivative of mean, variance, etc
3. parameters of regression, forecasting, Markov Model

## Compression-based Dissimilarity
It is an alternative structure-based similarity.
It consist in using as features the result of a compression algorithm.
Instead of the Euclidean distance, the distance will be computed as
$$d(x,y)=CDM(x,y)=\frac{C(x,y)}{C(x)+C(y)}$$
$C(x)$ is the compressed version of x. Note that compressing helps saving space without losing the important information.

Then, if they are different, compressing together or separately will lead to a different result, meaning this ratio will be lower and therefore the distance smaller.

# 2. Shape-based Similarities
## Euclidean Distance
Let A and B be two objects from the university of possible objects. The distance (also dissimilarity) is denoted by D(A,B).
Properties of Distance Measures:
1. symmetry -> D(A,B)=D(B,A)
2. constancy -> D(A,A) = 0
3. positivity -> D(A,B)=0 iff A=B
4. triangular inequality -> D(A,B) $\leq$ D(A,C) + D(B,C)

Given two TS of the same length n, how to compute the Euclidean Distance?
![[Pasted image 20240317191447.png]]

Problem with the Euclidean Distance.
In addition to curse of dimensionality.
It is very sensitive to distortions in the data.
They are dangerous and must be removed using the appropriate transformations.
## Transformations: remove distortions for Euclidean Distance
If you want to apply traditional distance measures like Euclidean Distance.
They are
1. Offset Translation
2. Amplitude Scaling
3. Linear Trend
4. Noise

*1. Offset Translation*
Removing the mean.

*2. Amplitude Scaling*
Subtracting the mean and dividing by the standard deviation.
It is basically the Z-score normalization.
Note that it includes the offset translation.

*3. Linear Trend*
Removing linear trend.
Fitting the best fitting straight line to the time series, then subtract that line from the TS.
It is not mandatory and can be also wrong.
Really depends on what is necessary.

*4. Noise*
The most important transformation. Suggested for the project.
It needs a smoothing function that removes the noise.

A function for removing the noise can be the Moving Average (MA).
It is the mean value applied in certain time windows.
The smaller the window, the less smooth.
‚Ä¢ For example, if w=3 we have $ùë°_ùëñ = \frac 1 3 (ùë°_{ùëñ‚àí1} + ùë°_ùëñ + ùë°_{ùëñ+1})$


## Dynamic Time Warping
The ED greatly suffers from misalignment in data.
Two ts conceptually equivalent may be misaligned, meaning that peaks may be shifted, and therefore classified as distant using ED. 
Dynamic Time Warping uses Warped Axis ("assi deformati") to correct the misalignment.

![[Pasted image 20240323170152.png|400]]

See a cool explanation of how to transform images into ts [here](https://izbicki.me/blog/converting-images-into-time-series-for-data-mining.html)

### How to compute DTW
Given two ts Q and C, we create a matrix |Q|x|C| and fill it with the distance between every pair of point.
The ED is only the diagonal of this matrix. While outside of the diagonal there are temporary shifted pairs.
The cost of ED is then linear, while for DTW is quadratic (n^2).

*Note* that for computing the ED the two ts must be of the same length, while this is not important for DTW.

![[Pasted image 20240323170831.png]]

Every possible warping is a path through the matrix.
The best one is found using a recursive definition of DTW.
The idea is that the best path must pass through (i-1,j), (i-1,j-1) or (i,j-1).

![[Pasted image 20240323171312.png]]

??
![[Pasted image 20240323171436.png|400]]

### Dynamic Programming Approach
To find the best path, there are 3 steps
1. compute the matrix of all distances $d(q_i,c_j)$
2. compute the matrix of all path costs $\gamma(i,j)$
	1. computed using the formula above
3. find the path with the lowest value, meaning best alignment

In particular, the second step is
![[Pasted image 20240323171944.png]]

An example
![[Pasted image 20240323172040.png]]

The following is the representation of the a cumulative costs matrix
![[Pasted image 20240323172153.png]]


## Fast Approximations to DTW
Moving from m observations to m' , for example from days to weeks.  
In this way we need to calculate less differences.
Basically we use a downsampled or compressed representation.
Still it is possible to understand the shape.

![[Pasted image 20240323172437.png]]

## Global Constraints
This is the solution to the low speed of dtw.
They prevent also pathological warping.

Two main approaches
1. Sakoe-Chiba Band
2. Itakura Parallelogram

![[Pasted image 20240323172555.png]]

A global constraint constrains the indices of the warping path $w_k = (i,j)_k$ such that $j-r \leq i \leq j+r$, 
Where r is a term defining allowed range of warping for a given point in a sequence and also as a window that reduces the number of calculus.

It might be useless to have a high width
![[Pasted image 20240323172940.png]]



## Summary of Time Series Similarity
If you have SHORT time series
1. use DTW after searching over the warping window size
2. (use after approximation?)

If you have LONG time series
1. if you know something about data
	1. extract data
2. if you do NOT know about data
	1. try compression/approximation-based dissimilarity




