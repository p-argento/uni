# Introduction to Time Series
## What is a Time Series?
It is a collection of observations made sequentially in time.
Generally at constant time intervals, which are on the x-axis.
Note that time instances can also be columns, instead of rows, and this is relevant especially when applying an operation to timestamps.

Some examples of TS are:
..


## Problems with TS
Some problems are
1. large amount of data
2. similarity not easy to estimate
3. different data formats
4. different sampling rates
5. noise, missing values, etc

## Tasks on TS
What we will do
1. Clustering
2. Motif Discovery
3. Rule Discovery
4. Classification
Other tasks not covered in the course
1. trends,seasonality
2. forecasting

All these tasks require *similarity matching*.

## Similarity
What is similarity?
It is the quality or state of being similar, likeness, resemblance, as a similarity of features.
In TS we recognize two kinds of similarity.
1. structural-based similarities
	1. meaning similarities at the structural level, the actual content
2. shape-based similarities
	1. meaning similarity at the level of the shape
	2. can not be enough to distinguish different TS

# 1. Structural-based Similarities

## Structural-based Similarities
Also called model-based similarity.
For very long time series, shape-based similarity give very poor results.
Therefore, we need to measure similarity based on high-level structure.

The idea is to turn the TS into a vector containing global features from the TS and use it to measure similarity or classify.
These features can be for example:
1. mean, variance, skewness, kurtosis
2. 1st derivative of mean, variance, etc
3. parameters of regression, forecasting, Markov Model

## Compression-based Dissimilarity
It is an alternative structure-based similarity.
It consist in using as features the result of a compression algorithm.
Instead of the Euclidean distance, the distance will be computed as
$$d(x,y)=CDM(x,y)=\frac{C(x,y)}{C(x)+C(y)}$$
$C(x)$ is the compressed version of x. Note that compressing helps saving space without losing the important information.

Then, if they are different, compressing together or separately will lead to a different result, meaning this ratio will be lower and therefore the distance smaller.

# 2. Shape-based Similarities
## Euclidean Distance
Let A and B be two objects from the university of possible objects. The distance (also dissimilarity) is denoted by D(A,B).
Properties of Distance Measures:
1. symmetry -> D(A,B)=D(B,A)
2. constancy -> D(A,A) = 0
3. positivity -> D(A,B)=0 iff A=B
4. triangular inequality -> D(A,B) $\leq$ D(A,C) + D(B,C)

Given two TS of the same length n, how to compute the Euclidean Distance?
![[Pasted image 20240317191447.png]]

Problem with the Euclidean Distance.
In addition to curse of dimensionality.
It is very sensitive to distortions in the data.
They are dangerous and must be removed using the appropriate transformations.
## Transformations: remove distortions for Euclidean Distance
If you want to apply traditional distance measures like Euclidean Distance.
They are
1. Offset Translation
2. Amplitude Scaling
3. Linear Trend
4. Noise

*1. Offset Translation*
Removing the mean.
*2. Amplitude Scaling*
Subtracting the mean and dividing by the standard deviation.
It is basically the Z-score normalization.
Note that it includes the offset translation.

3. Linear Trend
4. Noise

## Dynamic Time Warping






## Comparing Euclidean Distance and DTW





## Fast Approximations to DTW



## Global Constraints
This is the solution to the low speed of dtw.

Two main approaches
1. Sakoe-Chiba Band
2. Itakura Parallelogram


## Summary of Time Series Similarity
If you have SHORT time series
1. use DTW after searching over the warping window size
2. (use after approximation?)

If you have LONG time series
1. if you know something about data
	1. extract data
2. if you do NOT know about data
	1. try compression/approximation-based dissimilarity




