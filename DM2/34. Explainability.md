# Outline

1. Introduction to XAI
	1. Motivations
	2. Examples
	3. Taxonomy of methods
	4. Types of explanations
2. Methods
	1. TREPAN
	2. LIME
	3. LORE
	4. SHAP
	5. Saliency Maps
	6. INTGRAD
	7. "MASK"
	8. Sentence Highlighting
3. Instace-based Explanations
	1. Prototypes
	2. Counterfactuals (?)
		1. Brute Force
		2. OPT problem
			1. ...
			2. DICE
			3. Heuristics
			4. SEDC
			5. GSG
		3. instace-based strategies
			1. NNCE
			2. CBCE
4. Conclusion



Taxonomy
1. Explainable by design methods (intrinsec explainability)
2. Black Box Explanantion Methods (post-hoc explainability)

## TREPAN







## LIME
means Local Interpretable Model-agnostic Explanation.

It is a local explainer developed in 2016.
The idea is that the global decision boundary is complex, but in the neighborhood of a single decision, the boundary is simple. In other words, a single decision can be explained by auditing the black box around the given instance and hence learning a local decision.

LIME reveals the black box decision through
1. feature importance
2. saliency map

It locally approximates the behaviour of a black box for the specific instance as a linear regressor, with Lasso or Ridge regularization.

Synthetic neighbors are generated from slight perturbations of the instance, usually remaining in the center of the normal distribution.
Each synthetic instance is weighted wrt the distance with the instance to explain.
Starting from synthetic records, a linear regressor is trained and returns the coefficient as explanantions.

In the case of an *image*, LIME algorithm is
1. turn the image x into a vector x' of interpretable superpixels expressing presence/absence.
2. generate a synthetic neighborhood Z by randomly perturbing x' and labels them with the black box
3. train a linear regressor (interpretable) and assign a weight to each superpixel

The perturbation is done by replacing superpixels with 0 (meaning black, grey, or white), to see the performance of the model and therefore check if they were important for the prediction.

## LORE
means LOcal Rule-based Explainer

It extends LIME adopting as local surrogate a decision tree classifier and by generating synthetic instances through a genetic procedure. The genetic algo accounts for both instances with the same labels and different ones.

It can be generalized to work on images and text using the same data representation adopted by LIME.
Additionally, LORE generates counterfactual rules.

It means that the two main differences with LIME are
1. decision trees instead of linear regression
	1. additional extraction of rules and counterfactual rules from the tree
2. genetic algorithm is used instead of perturbation for the generation of synthetic instances

The genetic algorithms, starting from a random sample undergo crossover (combining parts of two instances) and mutation (randomly changing some features) to create new instances.

The synthetic instances are labelled using the black box and are crucial for generating the counterfactual rules that further explain the decision.

The rules and counterfactual rules are extracted from the decision tree.

LORE is built to be used on tabular data.

## SHAP
SHAPely values
(SOTA, included in LightGBM)

The idea comes from statistics.
A prediction can be explained by assuming 












