Continuation of Decision Trees.

## Definition

It is a ML model for classification.
If the conditions are satisfied, then classify with a certain class.
Do not confuse with association rules: they were descriptive, while now they are predictive (and built to maximize the accuracy).

The classifier will classify records bby using a collection of "if...then..." rules.
Rule $r:(condition)\rightarrow y$
Where $(condition)$ is a conjunction of tests on attributes and $y$ is the class label.

The model will extract some rules for example.
![[Pasted image 20240219144106.png|500]]

Two goals.
1. How to extract the different rules?
2. How to apply rules? Which rule to use?
## Terminology

A rule r *covers* an instance x if the attributes of the instance satisfy the condition of the rule.

*Coverage* of a rule, is the fraction of records that satisfy the antecedent of a rule.
$$coverage=\frac{records\ covered}{total\ records}$$
The higher the coverage of a rule the more it is general and less specific.

*Accuracy* of a rule, is fraction of records that satisfy the antecedent that also satisy the consequent of a rule.
$$accuracy=\frac{covered\ and\ correctly\ predicted}{total\ records}$$

## How to apply the rules extracted?

We want rules that are
- *Mutually exclusive rules*
	- each record is covered by at most one rule
	- rules are independent
	- if rules not mutually exclusive
		- ordered rule set
		- voting schemes
- *Exhaustive rules*
	- each record is covered by at least one rule
	- accounts for every possible combination
	- if rules not exhaustive
		- default class for records that do not trigger any rules

*Ordered Rule set*
Rules ranked according to priority.
Assign class based on the highest ranked rule triggered.
If no rule fired, assign the default class.

Rule ordering.
- rule-based ordering
	- individual rules ranked based on quality
- class-based ordering
	- rules that belong to the same class appear together

## Building Classification Rules
Distinction
- indirect method
	- extract rules from other classification models
		- for example from decision trees, where all the rules are mutually exclusive and exhaustive by design
- direct method
	- extract rules directly from data
	- methods like RIPPER, CN2, ...

> Use additional libraries in the project if you want. 

The advantage of DT is that you do not need to discretize the data.

## RIPPER (direct method)
Sequential covering.

Algo.
1. start from an empty rule
2. repeat until stopping criterion
	1. grow a rule using a function to learn one rule
	2. remove records covered by the rule

![[Pasted image 20240219152105.png|500]]

Why removing record?
Because we want mutually exclusive rules.

*How to build one rule?*
FOIL's Informatioin Gain. Meaning (First Order Inductive Learner).
It is an early rule-based learning algorithm.

![[Pasted image 20240219152345.png|500]]

When adding new conditions, select the ones with highest gain.
The logic is the same of the decision trees.

*Complexity of the rule*
Take into consideration also the complexity of the rule, to avoid overfitting.
Use Minimum Description Length (MDL).
![[Pasted image 20240219153712.png|500]]

In sklearn, it is the ccp-alpha.

![[Pasted image 20240219152647.png|500]]


*What is the RIPPER?*
For 2-class problem
1. choose one of the classes as positive class and set the others negative
2. learn rules for positive class
3. negative class will be the default class

For multi-class problem.
1. order classes according to prevalence
2. learn the rule for the smallest class first, treat the rest as negative class
3. repeat

How to *grow one rule*?
1. start with empty rule
2. grow
	1. add conjuncts as long as they improve FOILS information gain
	2. stop when rule no longer covers negative examples
3. prune
	1. as measure, use incremental reduced error pruning 
		1. $v=(p-n)/(p+n)$
		2. where p (n) is the number of positive (negative) examples covered by the rule in the validation set 
	2. as method, delete any final sequence that maximizes $v$

How to *build a rule set*?
1. use sequential covering algorithm
	1. finds the best rule that covers the current set of positive examples
	2. eliminate both positive and negative examples covered by the rule
2. each time a new rule is added, compute the new description length
	1. stop adding rules when the model is overly complex
	2. basically when the new description length is d bits longer than the smallest description length obtained so far

We have all the rules. What now? We need to *optimize the rules*.
But is the rule really needed and not over-specified?
What happens if I have a complete vision of the dataset?
1. Consider 2 alternative rules using the whole dataset.
	1. replacement rule (r*), grow new rules from scratch
	2. revised rule (r'), adding new conditions
2. Compare the rule set for r against the rule set for r* and r'.
3. Choose the rule that minimizes MDL.

## Indirect Methods (rules from DT)

![[Pasted image 20240219154202.png|500]]

A tree with 5 leaves provide 5 rules.
Rules are usually extracted from unpruned decision trees.
But these rules are often reduntant, so we need a way to generalize.
For each rule r_A->y
1. consider an alternative rule r':A'->y
	1. where  A' is obtained by removing one of the conditions in A
2. compare the pessimistic error rate for r against all r'
3. prune if one...
4. remove duplicates
5. repeat ...


*Extract using class ordering*
Instead of ordering the rules, order subset of rules.

![[Pasted image 20240219154744.png|700]]

In DT.
For example, it is useless to ask "live in water" for birds, so the condition was removed.

In RIPPER.
Start from the class with less records.

> These methods are interpretable by design, so you can check all the rules in the dataset.

## Advantages of Rule-based Classifiers
What
1. similar to dt
	1. highly expressive
	2. easy to interpret
	3. comparable performance to decision trees
	4. can handle redundant attributes
2. better suited for handling imbalanced classes
3. cannot handle missing values
	1. while dt could


> check also here https://christophm.github.io/interpretable-ml-book/rules.html


