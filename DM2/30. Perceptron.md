## Concept of Artificial Neural Network

The model is an assembly of inter-connected nodes and weighted links.

Steps at each node.
1. sums multiple inputs according to the weights of its links.
2. then this output is compared against some threshold (also named bias)
	1. that is just like summing another weight always multiplied by 1
3. the result is the input of an activation function that gives the final output
	1. for example the sign function

![[Pasted image 20240627174959.png]]

Observe that the signal (input/output) can be
1. real value
2. unipolar (0/1)
3. bipolar (-1,+1)
4. NOT categorical (needs to be one-hot-encoded)

Adjusting the weights is a process that requires an optimization algorithm like Gradient Descent, aimining at minimize a cost function.

Basic activation functions, observe that it is a hyper-parameter, usually expressed with $\sigma$
1. linear activation function
	1. is just the identity function, called linear unit
2. sign activation function
	1. better for binary classification
	2. ![[Pasted image 20240627185126.png]]
3. sigmoid logistic activation function
	1. $\sigma(a)=\frac 1{1+exp(-a)}$

## Perceptron

It is a single layer network, that contains only input and output nodes.
The activation function is sign().

In the training the weight parameters are adjusted until the outputs of the perceptron become consistent with the true outputs of the training samples. (remember that this is a supervised method)

Training steps.
1. initialize the weights
2. repeat until stopping condition met
	1. for each training sample (x,y)
		1. compute the output with the current weights$$f(w^{(k)},x_i)$$2. update the weights (using GD)$$w^{k+1}=w^{(k)}+\lambda [y_i-f(w^{k},x_i)]x_i$$
where
1. $k$ is the iteration index
	1. but the hyperparameter is the number of epochs
		1. meaning k = epoch\*sample number
	2. an epoch is passed after training on the whole training set, for example epochs 10,000
	3. required for early stopping in order to avoid overfitting 
2. $\lambda$ is the learning rate
	1. hyperparameter between 0 and 1
	2. typically 0.1 or 0.01, but can be adaptive, gradually decreasing
3. $[y_i-f(w^{k},x_i)]$  is the error
	1. if this is different from zero, it means that the point is misclassified hence produces a change in the weight

In the example, the learning is sequential, but it is actually much better to randomize the order.

The problem of the perceptron.
It is that is a linear combination of input variables, meaning that the decision boundary is linear.
Non-linearly separable problems like the XOR.
This is why we need more layers.

*Exercises*.
1. prediction exercise
2. training exercise



