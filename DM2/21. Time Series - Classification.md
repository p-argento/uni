## How to classify Time Series
Generally assume that all ts have the same length m.
The goal is to find a function f that maps from the space of all ts to the space of class values. So, given a set X of n time series, each ts has m values and a class value c.

There are 2 main methods
1. KNN Classification
2. Shapelet-based Classification

# 1. KNN Classification
It is the most widely used and effective approach for time series classification (TSC).
It consists in using KNN on the raw ts.
DTW gives much better results than the ED.
However, it is very slow to calculate, given also the "lazyness" of the classifier.


# 2. Shapelet-based Classification
It is a better alternative compared to traditional KNN.
A shapelet is a subsequence of a predefined length n, such that the shape is characterizing one of the classes. So, a shapelet is a representative subsequence of the vector of distances that compose the ts.

## Shapelet Idea
The shapelets are used to transform a ts dataset and to use the transformed version as input for training a ml classifiers.
In other words, the shapelets extracted are used to obtain a transformed version of the initial dataset of time series, that will train a ML model, like a dt.

How to obtain the shapelet-tranformed dataset? Using a "shapelet transformer", which identifies candidate shapelets and then calculates the distance between each ts and each shapelets to select the most representative for each class.

Observe that the temporal information is not present anymore. This dataset will have a different dimension NxK, given the K shapelets found. It will have a highly discriminative power because, by design, this features are highly discriminative. It is tailored for ML classifiers.
Then, a ML classifer, for example a Decision Tree, will ask something like "does ts1 have a subsequence s within a distance of x of shape y?"

![[Pasted image 20240324185228 1.png]]


## Shapelets Algorithm
The steps of the whole algorithm are
1. shapelets extraction to create a dataframe of shapelets
2. trasform the input dataset of time series into a dataframe where each column is the distance between the record and a specific shapelet extracted
3. use the transformed dataset with a ML model, like a decision tree

## Shapelet distances
In the shapelet transformer, this is very important because we are computing the distance between a very small object (shapelet S) and a big one (ts T).
We use a distance function that computes the minimum distance between the shapelet and a subsequence of the ts, meaning that it looks for the best matching location for S in the ts to compute the distance.
It will be something like $SubsequenceDist(T,S)=min(Dist(S,S'))$ where S' is in the set of all possible subsequences of T.

![[Pasted image 20240420183245.png]]


## Shapelet Extraction
To use the shapelet transformer, first we need to extract the shapelets from the ts dataset.

Shapelets are ts subsequences maximally representative of a class.
They can provide interpretable results, which is really interesting for example in finance or medicine because it helps domain practitioners better understand their data.

Observe that while other ts classifiers consider global features, the shapelets are local features.

There are many different ways to extract shapelets.
1. random
	1. defined the length (works surprisingly well)
2. brute-force
	1. MORE IN DETAILS below
3. gradient-based
	1. minimizes a regularized loss function.
4. genetic
5. etc

### Brute-Force extraction
The steps are
1. extract candidate subsequences of all possible lengths
2. calculate the distance between each time series and each shapelet, keeping the minimum distance (best alignment)
	1. speed up this expensive calculation using Distance Early Abandon and Admissible Entropy Pruning
3. find the best split for each shapelet, like in decision tree features
4. evaluate the discriminatory effect of each shapelet through the information gain of the best split and return only the k-best shapelets with the highest information gain

![[Pasted image 20240722151229.png]]

*1. create candidate pool*
We create a candidate pool of shapelets, extracting all the subsequences of all possible lengths.

*2. calculate the distance from each shapelet*
It is like calculating the values for each feature (that is a shapelet).
In particular, we arrange the TSs dataset based on the distance from the candidate. We obtain a vector that contains all the distances from each specific shapelet.

Since the distance alculation can be very expensive, the speedup is Distance Early-Abandon, meaning abandon calculation if the current distance is larger than the best-so-far.

*3. find the best split for each shapelet*
Then we need to test the utility of a candidate shapelet, in order to select the ones that are really representative. We look for the shapelet that achieve the maximum information gain, similar to decision trees.

Exactly like decision trees, with compute the information gain using entropy to find the best splitting strategy and store the result.

In other words, we find the optimal split point that maximizes the information gain. The goal is to find the optimal splitting rule, which uses the distance between a ts and a shapelet, meaning the SubsequenceDist.

In this step, the speedup is Admissible Entropy Pruning (main speedup).
If after calculating the distance and the information gain for some samples, the upper bound of information gain < best candidate shapelet then the shapelet can be skipped avoiding unncessary calculation

NOTE that steps 2 and 3 are calculated together for each shapelet, meaning that the two speedups are used together to avoid calculating unncessary distances of not disciminant shapelets.

*3. select k-best shapelets with the highest information gain*
Given the optimal splitting strategy and the consequent information gain of each shapelet (remember that they are like features), select only the k-best.




![[Pasted image 20240422160902.png]]

![[Pasted image 20240420190637.png]]










> As we shall explain in Section 3, our algorithm needs some metric to evaluate how well it can divide the entire combined dataset into two original classes. Here, we use concepts very similar to the information gain used in the traditional decision tree [2]. The reader may recall the original definition of entropy which we review here: 
> 
> Definition 6: Entropy. A time series dataset D consists of two classes, A and B. Given that the proportion of objects in class A is p(A) and the proportion of objects in class B is p(B), the entropy of D is: I(D) = -p(A)log(p(A)) -p(B)log(p(B)). 
> 
> Each splitting strategy divides the whole dataset D into two subsets, D1 and D2. Therefore, the information remaining in the entire dataset after splitting is defined by the weighted average entropy of each subset. If the fraction of objects in D1 is f(D1) and the fraction of objects in D2 is f(D2), the total entropy of D after splitting is Î(D) = f(D1)I(D1) + f(D2)I(D2). This allows us to define the information gain for any splitting strategy: 
> 
> Definition 7: Information Gain. Given a certain split strategy sp which divides D into two subsets D1 and D2, the entropy before and after splitting is I(D) and Î(D). So the information gain for this splitting rule is Gain(sp) = I(D) - Î(D), Gain(sp) = I(D) - f(D1)I(D1) + f(D2)I(D2). 
> 
> As hinted at in the introduction, we use the distance to a shapelet as the splitting rule. The shapelet is a subsequence of a time series such that most of the time series objects in one class of the dataset are close to the shapelet under SubsequenceDist, while most of the time series objects from the other class are far away from it. 
> 
> To find the best shapelet, we may have to test many shapelet candidates. In the brute force algorithm discussed in Section 3.1, given a candidate shapelet, we calculate the distance between the candidate and every time series object in the dataset. We sort the objects according to the distances and find an optimal split point between two neighboring distances. 
> 
> Definition 8: Optimal Split Point (OSP). A time series dataset D consists of two classes, A and B. For a shapelet candidate S, we choose some distance threshold dth and split D into D1 and D2, such that for every time series object T1,i in D1, SubsequenceDist(T1,i, S) < dth and for every time series object T2,i in D2, SubsequenceDist(T2,i, S) ≥ dth. An Optimal Split Point is a distance threshold that Gain(S, dOSP(D, S)) ≥ Gain(S, d'th) for any other distance threshold d'th. 
> 
> So using the shapelet, the splitting strategy contains two factors: the shapelet and the corresponding optimal split point.
> (from article)

## Motif vs Shapelet
DO NOT confuse motifs/discords with shapelets.
They are different.

A motif is a repeated pattern (also subsequence) in ONE given TS.
It is extracted in a unsupervised way.

A shapelet is a pattern (also subsequence) which is maximally representative of a class with respect to a given dataset of TSs. It is extracted for a supervised task on a dataset.

Observe that motifs can be used as shapelets, because they are subsequences, but probably discords are much better shapelets since they characterize the sequence with respect to others.


# 3. NN-based Classification



