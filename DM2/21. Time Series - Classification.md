## How to classify Time Series
Generally assume that all ts have the same length m.
The goal is to find a function f that maps from the space of all ts to the space of class values. So, given a set X of n time series, each ts has m values and a class value c.

There are 2 main methods
1. KNN Classification
2. Shapelet-based Classification

# 1. KNN Classification
It is the most widely used and effective approach for time series classification (TSC).
It consists in using KNN on the raw ts.
DTW gives much better results than the ED.
However, it is very slow to calculate, given also the "lazyness" of the classifier.


# 2. Shapelet-based Classification
It is a better alternative compared to traditional KNN.
A shapelet is a subsequence of a predefined length n, such that the shape is characterizing one of the classes. So, a shapelet is a representative subsequence of the vector of distances that compose the ts.

## Shapelet Idea
The shapelets are used to transform a ts dataset and to use the transformed version as input for training a ml classifiers.
In other words, the shapelets extracted are used to obtain a transformed version of the initial dataset of time series, that will train a ML model, like a dt.

How to obtain the shapelet-tranformed dataset? Using a "shapelet transformer", which identifies candidate shapelets and then calculates the distance between each ts and each shapelets to select the most representative for each class.

Observe that the temporal information is not present anymore. This dataset will have a different dimension NxK, given the K shapelets found. It will have a highly discriminative power because, by design, this features are highly discriminative. It is tailored for ML classifiers.
Then, a ML classifer, for example a Decision Tree, will ask something like "does ts1 have a subsequence s within a distance of x of shape y?"

![[Pasted image 20240324185228 1.png]]


## Shapelets Algorithm
The steps of the whole algorithm are
1. shapelets extraction to create a dataframe of shapelets
2. trasform the input dataset of time series into a dataframe where each column is the distance between the record and a specific shapelet extracted
3. use the transformed dataset with a ML model, like a decision tree

## Shapelet distances
In the shapelet transformer, this is very important because we are computing the distance between a very small object (shapelet S) and a big one (ts T).
We use a distance function that computes the minimum distance between the shapelet and a subsequence of the ts, meaning that it looks for the best matching location for S in the ts to compute the distance.
It will be something like $SubsequenceDist(T,S)=min(Dist(S,S'))$ where S' is in the set of all possible subsequences of T.

![[Pasted image 20240420183245.png]]


## Shapelet Extraction
To use the shapelet transformer, first we need to extract the shapelets from the ts dataset.

Shapelets are ts subsequences maximally representative of a class.
They can provide interpretable results, which is really interesting for example in finance or medicine because it helps domain practitioners better understand their data.

Observe that while other ts classifiers consider global features, the shapelets are local features.

There are many different ways to extract shapelets.
1. random
	1. defined the length (works surprisingly well)
2. brute-force
	1. MORE IN DETAILS below
3. gradient-based
	1. minimizes a regularized loss function.
4. genetic
5. etc

### Brute-Force extraction
The steps are
1. extract candidate subsequences of all possible lengths
2. calculate the distance between each time series and each shapelet, keeping the minimum distance (best alignment)
	1. speed up this expensive calculation using Distance Early Abandon and Admissible Entropy Pruning
3. find the best split for each shapelet, like in decision tree features
4. evaluate the discriminatory effect of each shapelet through the information gain of the best split and return only the k-best shapelets with the highest information gain

![[Pasted image 20240722151229.png]]

*1. create candidate pool*
We create a candidate pool of shapelets, extracting all the subsequences of all possible lengths.

(for each shapelet 2 and 3)
*2. calculate the distance from each shapelet*
It is like calculating the values for each feature (that is a shapelet).
In particular, we arrange the TSs dataset based on the distance from the candidate. We obtain a vector that contains all the distances from each specific shapelet.

Since the distance alculation can be very expensive, the speedup is Distance Early-Abandon, meaning abandon calculation if the current distance is larger than the best-so-far.

*3. find the best split for each shapelet*
Then we need to test the utility of a candidate shapelet, in order to select the ones that are really representative. We look for the shapelet that achieve the maximum information gain, similar to decision trees.

Exactly like decision trees, with compute the information gain using entropy to find the best splitting strategy and store the result.

In other words, we find the optimal split point that maximizes the information gain. The goal is to find the optimal splitting rule, which uses the distance between a ts and a shapelet, meaning the SubsequenceDist.

In this step, the speedup is Admissible Entropy Pruning (main speedup).
If after calculating the distance and the information gain for some samples, the upper bound of information gain < best candidate shapelet then the shapelet can be skipped avoiding unncessary calculation

NOTE that steps 2 and 3 are calculated together for each shapelet, meaning that the two speedups are used together to avoid calculating unncessary distances of not disciminant shapelets.

*4. select k-best shapelets with the highest information gain*
Given the optimal splitting strategy and the consequent information gain of each shapelet (remember that they are like features), select only the k-best.



![[Pasted image 20240422160902.png]]

![[Pasted image 20240420190637.png]]


> As we shall explain in Section 3, our algorithm needs some metric to evaluate how well it can divide the entire combined dataset into two original classes. Here, we use concepts very similar to the information gain used in the traditional decision tree [2]. The reader may recall the original definition of entropy which we review here: 
> 
> Definition 6: Entropy. A time series dataset D consists of two classes, A and B. Given that the proportion of objects in class A is p(A) and the proportion of objects in class B is p(B), the entropy of D is: I(D) = -p(A)log(p(A)) -p(B)log(p(B)). 
> 
> Each splitting strategy divides the whole dataset D into two subsets, D1 and D2. Therefore, the information remaining in the entire dataset after splitting is defined by the weighted average entropy of each subset. If the fraction of objects in D1 is f(D1) and the fraction of objects in D2 is f(D2), the total entropy of D after splitting is Î(D) = f(D1)I(D1) + f(D2)I(D2). This allows us to define the information gain for any splitting strategy: 
> 
> Definition 7: Information Gain. Given a certain split strategy sp which divides D into two subsets D1 and D2, the entropy before and after splitting is I(D) and Î(D). So the information gain for this splitting rule is Gain(sp) = I(D) - Î(D), Gain(sp) = I(D) - f(D1)I(D1) + f(D2)I(D2). 
> 
> As hinted at in the introduction, we use the distance to a shapelet as the splitting rule. The shapelet is a subsequence of a time series such that most of the time series objects in one class of the dataset are close to the shapelet under SubsequenceDist, while most of the time series objects from the other class are far away from it. 
> 
> To find the best shapelet, we may have to test many shapelet candidates. In the brute force algorithm discussed in Section 3.1, given a candidate shapelet, we calculate the distance between the candidate and every time series object in the dataset. We sort the objects according to the distances and find an optimal split point between two neighboring distances. 
> 
> Definition 8: Optimal Split Point (OSP). A time series dataset D consists of two classes, A and B. For a shapelet candidate S, we choose some distance threshold dth and split D into D1 and D2, such that for every time series object T1,i in D1, SubsequenceDist(T1,i, S) < dth and for every time series object T2,i in D2, SubsequenceDist(T2,i, S) ≥ dth. An Optimal Split Point is a distance threshold that Gain(S, dOSP(D, S)) ≥ Gain(S, d'th) for any other distance threshold d'th. 
> 
> So using the shapelet, the splitting strategy contains two factors: the shapelet and the corresponding optimal split point.
> (from article)

## Motif vs Shapelet
DO NOT confuse motifs/discords with shapelets.
They are different.

A motif is a repeated pattern (also subsequence) in ONE given TS.
It is extracted in a unsupervised way.

A shapelet is a pattern (also subsequence) which is maximally representative of a class with respect to a given dataset of TSs. It is extracted for a supervised task on a dataset.

Observe that motifs can be used as shapelets, because they are subsequences, but probably discords are much better shapelets since they characterize the sequence with respect to others.


# 3. NN-based Classification

Approaches
1. Some approaches
	1. ResNet
	2. InceptionTime
	3. TapNet
	4. Canonical Interval Forest (CIF)
2. Rocket
	1. rocket
	2. minirocket
	3. comparison with CNN
3. ensembles
	1. COTE
	2. HIVE-COTE
	3. TS-CHIEF
4. shapelet-based
	1. MR-SEQL

## 1. ResNet
Composed by
1. 3 block of 3 convolutional layers each
2. in the end a global AvgPooling and a Softmax

## 2. InceptionTime
An ensemble of 5 Inception NN, each one composed by
1. 3 inception modules
	1. consists of convolutions with kernels of several sizes followed by batch normalization and the rectified linear unit activation function
2. global average pooling
3. fully connected layer with softmax

Note that a fully connected layer is made by neurons that receive information from all the neurons of the previous layer.

## 3. TapNet
Combination of traditional and neural networks approaches

## 4. Canonical Interval Forest (CIF)
Ensemble of time series tree classifiers built using Catch22, ie 22 canonical time series characteristics.
Build decision trees on feature-transformed random subsets.

## 5. Rocket
Means Random Convolutional Kernel Transform.
1. use a large number (like 100,000) of randomly generated convolutional kernels to transform the time series
2. the transformed features are used to train a linear classifier (like Logistic Regression)
3. the combination of rocket and logistic regression forms a single layer convolution with random weights

The differences with standard CNN are
1. the kernels in ROCKET are not trained using gradient descent as in CNN, but selected randomly and therefore more variable
2. the output of the steps kernel->featuremap->featureextraction is a single layer, meaning a vector with all the features (usually max and proportion of positive values) coming from the different parallel kernels.
3. even if kernels are more variable in ROCKET than CNN, the pooling is quite simple
4. the only parameter for ROCKET is the number of kernels since they are random

## 6. MiniRocket
Simplified version of rocket.
1. only kernels of length 9
2. only subset of 84 kernels
3. the only feature extraction is proportion of positive values
4. each kernel has the same fixed number of dilations

A dilation of 1 means that input values are contiguous, while a dilation of 2 means that when computing the dot product with the kernel one value is skipped each time, as there was an empty space in between the two values, and so on.

## 7. Ensembles



## 8. MR-SEQL
The time series is discretized using SAX.
The most discriminative symbols are extracted using a SEQuence learning algorithm.














