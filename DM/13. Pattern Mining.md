# Association Rules
## Market Basket Analysis (MBA)
Given a set of transactions where each transaction is a set of items.
The goal is to find groups of items frequently bought together.
It is an actionable insight. The seller can sell items in a single package. 

## Association Rules Introduction
Express how product/services relate to each other and tend to group together.
"If a customer purchases diapers then will also purchase beer"
Rule form: “Body → Ηead \[support, confidence]”.
buys(x, “diapers”) → buys(x, “beers”) \[0.5%, 60%]

Body/Head/Antecedent $X\rightarrow Y$ Head/Tail/Consequent

Rules can be
1. useful
2. trivial
3. unexplicable

chatgpt
> Association rule mining, a specific type of pattern mining, involves finding rules that describe how the presence of certain items in a dataset implies the presence of other items. It specifically aims to find relationships between variables in the form of "if-then" rules, typically using measures like support and confidence to evaluate these rules.

# Apriori
## Definitions
- association rule
	- rule that predict the occurrence of an item based on the occurrences of other items in the transaction (co-occurrence, not causality)
	- it is an implication expression of the form $X\rightarrow Y$ with $X,Y$ itemsets
- itemset
	- collection of one or more items
	- k-itemset contains k-items
- support count $\sigma$
	- frequency of occurrence of an itemset
- frequent itemset
	- itemset whose support is greater than or equal to a $minsup$ threshold
- rule evaluation metrics
	- support $s$
		-  fraction of transactions that contain an itemset, means  $\sigma / n\_transactions$
	- confidence $c$
		- how often items in Y appear in transactions that contain X

![[Pasted image 20240114182228.png|300]]

## Association Rule Mining

Given a set of transactions T, the goal of association rule mining is to find all rules having
1. $support \geq minsup$
2. $confidence\geq minconf$

What about a *brute-force* approach?
Listing all possible association rules and computing support and confidence for comparison with thresholds.
It is computationally prohibitive.

![[Pasted image 20240114185324.png]]

Given the Frequent Itemset *{Milk, Diaper, Beer}*, some examples of AR are

![[Pasted image 20240114185239.png]]

However, we observe that rules originating from the same itemset have identical support (because of the formula), but can have different confidence.
With this in mind, we can *decouple support and confidence requirements* based on threshold.
In fact, in apriori algorithm, we follow 2 steps
1. one generating frequent itemset with $support\geq minsup$
2. another generating, from each frequent itemset, rules with $confidence \geq minconf$

Methods to compute Association Rules.
1. Basic Apriori Algorithm
2. Optimizations of Apriori
3. Multi-Dimension AR
4. Quantititative AR

## Structure of Apriori Algorithm


Association Rule Mining follows a 2-step approach.
1. **Frequent Itemset Generation**
	1. generate itemsets
		1. whose $support\geq minsup$
		2. with cardinality from 1 to k (k-itemset)
		3. remember that *a subset of a frequent itemset is also a frequent itemset*
2. **Rule Generation**
	1. generate rules from each frequent itemset
		1. whose $confidence \geq minconf$
		2. each rule is a binary partitioning of a frequent itemset

Note that frequent itemset generation is still computationally expensive.
Given d items, there are $2^d$ possible candidate itemsets.

## 1. Frequent Itemset Generation

![[Pasted image 20240114185728.png]]

**Approaches**
1. brute-force
	1. Match each transaction against every candidate
	2. Complexity is O(NMw)
2. reducing number of candidates M
	1. complete search is $M=2^d$
	2. use pruning techniques
3. reducing number of transactions N
	1. reduce size of itemset
4. reducing number of comparisons MN
	1. use efficient data structures
	2. no need to match every candidate against every transaction

**Apriori Principle**
The *apriori principle* states that:
if an itemset is frequent, then all of its subsets must also be frequent.
> It looks like a necessary condition for an itemset to be frequent.
> If it is not respected, then an itemset cannot be frequent.
> Then, if at least one subset is not frequent, the itemset cannot be frequent.

It holds due to the *anti-monotone property* of support:
$$\forall X,Y:(X\subseteq Y)\Rightarrow s(X)\geq s(Y)$$
> In other words, $s(itemset)\leq s(subset)$
> And if $s(subset)\leq minsup$ then $s(itemset)\leq s(subset)\leq minsup$ , meaning that the itemset is below the threshold and therefore infrequent.

Basically, the support of an itemset never exceeds the support of its subsets.
For this reason, if an itemset is found to be infrequent, all the supersets that include it can be pruned because they are infrequent as well.
In other words, there is no need to generate candidates involving infrequent items or itemsets.

![[Pasted image 20240114190742.png|400]]

**Apriori Algorithm**
Set
- $F_f$ = frequent k-itemsets
- $L_k$ = candidate k-itemsets
Steps
1. Let $K=1$ and generate $F_1$ = {frequent 1-itemsets}
2. Repeat until $F_k$ is empty
	1. Candidate Generation
		1. generate $L_{k+1}$ from $F_k$
	2. Candidate Pruning
		1. prune candidate itemset in $L_{k+1}$ containing infrequent subsets of length k
	3. Support Counting
		1. count the support of each candidate in $L_{k+1}$ by scanning the DB
		2. expensive operation because must match every candidate itemset against every transaction
	4. Candidate Elimination
		1. eliminate candidates in $L_{k+1}$ that are infrequent
		2. leaving in $F_{k+1}$ only frequent candidates

**Method for Candidate Generation and Candidate Pruning**
For candidate generation, use the *$F_{k-1}\times F_{k-1}$ Method*.
Given two frequent (k-1)-itemsets, merge if their first (k-2) items are identical, means that they share the prefix of (k-1) elements.
Except when k=2, when of course we merge if they have a common element.

Example.
1. Candidate Generation.
F3 = {ABC,ABD,ABE,ACD,BCD,BDE,CDE} 
Merge.
(ABC, ABD) = ABCD 
(ABC, ABE) = ABCE
(ABD, ABE) = ABDE
Do not merge.
(ABD,ACD) because they share only prefix of length 1 instead of length 2 

2. Candidate Pruning
Given.
F3 = {ABC,ABD,ABE,ACD,BCD,BDE,CDE} be the set of frequent 3-itemsets.
L4 = {ABCD,ABCE,ABDE} is the set of candidate 4-itemsets generated.
Prune.
ABCE because ACE and BCE are infrequent (do not belong to F3)
ABDE because ADE is infrequent (do not belong to F3)
After candidate pruning
L4 = {ABCD}



![[Pasted image 20240114192225.png]]

## 2. Rule Generation

**Rule Generation with Apriori**



**Maximal vs Closed Itemsets**

![[Pasted image 20240114194037.png|400]]

## Pattern Evaluation



## Continuous and Categorical Data




## Multi-level Association Rules




# FP-Growth
It is about mining frequent patterns without candidate generation.


