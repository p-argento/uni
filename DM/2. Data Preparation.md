## Data Understanding vs Data Preparation
| **Data Understanding** | **Data Preparation** |
| -- | -- |
| existence of missing values | select attributes |
| existence of outliers | reduce data dimension |
| character of attributes | select records |
| dependencies between attributes | treat missing values |
|  | treat outliers |
|  | integrate, unify and transform data |
|  | improve data quality |
## Data Preparation Checklist
1. Aggregation
2. Data Reduction (Sampling)
3. Dimensionality Reduction
4. Feature Subset Selection
5. Feature Creation
6. Data Cleaning *
7. Discretization and Binarization
8. Attribute Transformation

## 1. Aggregation
Combining more attributes into one.
Why?
- Dimensionality reduction
- Change of scale (from seconds to minutes)
- Less variability (aggregated data is less variable)

## 2. Data Reduction
Reducing number of records
- sampling
- clustering
Reducing number of attributes
- selection of attributes subset
- generation of new set of attributes

**Sampling**
Must be representative of the dataset
Improve the processing time required.

Types of sampling
- simple random sampling (equal probability)
	- with replacement
	- without replacement
- stratified sampling (better)
	- draw random samples from partitions to keep structure

**Feature Selection**
Choose a subset that is as small as possible.
Remove features that are:
- irrelevant
- redundant

## 3. Dimensionality Reduction

**Curse of Dimensionality**
When dimensionality increases, data becomes increasingly sparse.
Then, density and distance between points become less meaningful.
And therefore, clustering and classifiers are less effective.

![[Pasted image 20240108212223.png|400]]

**Techniques for dimensionality reduction**
1. Principal Component Analysis (PCA)
2. Singular Value Decomposition (SVD)
3. Other supervised and non-linear techniques

## 4. Feature Selection

**Type of Methods for feature selection
1. Filter Methods
	1.  independently of dm task, selection after analyzing the significance and correlation with other attributes
2. Wrapper Methods
	1. based on dm task, incremental selection of best attributes in terms of statistical significance
3. Embedded Methods
	1. selection is part of the dm algorithm, which choose most useful attributes (decision tree)

**Focus on Wrapper Methods**
1. Selecting top-ranked features
	1. choose single features with highest evaluation
2. Selecting top-ranked subset
	1. requires exhaustive search through all combinations
3. Forward Selection
	1. start with empty and add each time the feature that yields the best improvement
4. Backward elimination
	1. start with full and remove each time the feature that yields to the least decrease in performance

## 5. Feature Creation
Create new attributes that can capture information more efficiently than the original dataset.

**Methods**
1. Feature construction (domain-dependent)
	1. density = mass / volume , efficiency, ...
2. Feature Projection
	1. from high dimensional space to lower dimensions
	2. approaches
		1. PCA
		2. SVD
		3. NMF (Non-Negative Matrix Factorization)
		4. LDA (Linear Discriminant Analysis)
		5. Autoencoder

## 6. Data Cleaning
**Anomalous Values**
1. Missing Values (null,?)
2. Unknown Values (no real meaning)
3. Invalid

**How to manage missing values**
1. elimination of record
2. substitution of values (it can influence the original distribution)
	1. use mean/median/mode
		1. without data segmentation
		2. with data segmentation
	2. estimate using probability distribution of existing values
		1. without data segmentation
		2. with data segmentation
	3. build a model of classification or regression for computing missing values

## 7. Discretization
It is the process of converting a continuous attribute into an ordinal attribute.
Each value is then replaced by the interval to which it belongs.

**Advantages**
1. Commonly used in classification, because classification algorithms works better if variables have few values.
2. Simpler interpretation
3. Less sparse data

**Unsupervised Discretization**
No label for the instances and unknown number of classes.
How to choose intervals:
1. fixed reasonable granularity
2. domain-dependent criterion
3. determined by data distribution or clustering

Techniques of binning:
1. natural binning
	1. equal width intervals
2. equal frequency binning
	1. same frequency intervals
3. statistical binning
	1. using statistical information like means, quartiles

*Natural Binning*
Subdivision in k parts of the same size after sorting values.
![[Pasted image 20240109114819.png|200]]
Can generate unbalanced distribution.

*Equal Frequency Binning*
Sort and count the number of elements in the sample N.
![[Pasted image 20240109115134.png|200]]
Not always suitable for highlighting interesting correlations.

**How many classes**
The optimal number of classes if a function of N elements. (Sturges,1929)
![[Pasted image 20240109115407.png|200]]
And the optimal width of classes? Depends also on variance. (Scott)
![[Pasted image 20240109115711.png|200]]

**Supervised Discretization**




## 8. Binarization




## Normalization



## Other Attribute Transformation