**Model Evaluation**
1. Metrics for Performance Evaluation -> how to evaluate model
	1. Confusion Matrix and Accuracy
	2. Cost-sensitive measures
	3. Cost matrix
2. Methods for Performance Evaluation -> how to obtain reliable estimates
	1. Parameter tuning
	2. Learning curve
	3. Methods of sampling
3. Methods for Model Comparison -> compare performance among competing models
	1. ROC Curve
	2. Lift Chart

![[Pasted image 20231117221520.png]]

---
# METRICS for performance evaluation
It focus on the predictive capability of the model (not the speed or scalability).

**Confusion matrix**
Compare the vector Y of actual class labels with the vector Y' returned by the trained model for the test set.

![[Pasted image 20231118104335.png|300]]

The relevant measures based on the confusion matrix are
1. accuracy (A)
2. precision (p)
3. recall (r)
4. F-measure (F)

**Accuracy**
The most important metrics is the *accuracy* (the opposite is the error).
$$Accuracy=\frac{correct\ instances}{total\ instances}=\frac{TP+TN}{TP+TN+FP+FN}$$
Limitation of accuracy.
Since true and false have the same importance, if the frequency of classes is very diverse, the accuracy might be misleading.
The model might predict always class 0 with a high accuracy, but we are actually looking for are those few records of class 1.

To avoid this limitation, we can give different importance to true and false, computing the *weighted accuracy*
$$Weighted\ Accuracy=\frac{w_1TP+w_4TN}{w_1TP+w_2TN+w_3FP+w_4FN}$$

**Cost-sensitive measures**
For example, in a factory, you need to be sure (TP) that there is a problem before stopping the machines, avoiding FP. So what you want is high precision.
*Precision* is biased towards C(Yes|Yes) & C(Yes|No).
$$Precision(p)=\frac{TP}{TP+FP}$$
In a hospital, you look for high recall for emergency calls. It means that you don't care about TN, they can be as many as you want, but it is crucial to identify the TP to evacuate the hospital when it is a real emergency. Basically, FN are dangerous and they will lower the value of recall / sensitivity.
*Recall* is biased towards C(Yes|Yes) & C(No|Yes)
$$Recall(r)=\frac{TP}{TP+FN}$$
Is it better to have a high precision or a high recall? It depends.  
If you don't know, do the F-measure, which is an harmonic mean.
*F-measure* is biased towards all except C(No|No).
$$F-measure(F)=\frac{2TP}{2TP+FN+FP}=\frac{2rp}{r+p}$$

**Cost Matrix**
In addition to the confusion matrix, we can compute the *cost matrix*.
It uses $C(i|j)$ as the cost of misclassifying class j example as class i.

![[Pasted image 20231117232752.png|300]]

It is used in combination with the confusion matrix to compute the total cost of the model, based on the goal we need to achieve in terms of costs for each event.
The cost matrix provides the weights for each frequency in the confusion matrix. The sum is the total cost.

**Multi-class evaluation**
TN does not exist. There are only correct or incorrect instances.

![[Pasted image 20231118104421.png|300]]

As a consequence, the accuracy is
$$Accuracy=\frac{correct\ instances}{total\ instances}=\frac{TPA+TPB+TPC}{N}$$
For the cost-sensitive measures with a multi-class, use both the separated and aggregated performance. In particular, build the confusion matrix for each class to calculate the separated performance on the cost-sensitive measures.

# METHODS for performance evaluation

**Parameter Tuning**
Learning scheme:
1. Build the basic structure
2. Optimize parameter settings
There are 3 sets of data:
1. training data
2. validation data ->  to optimize the parameter settings
3. test data -> to estimate the error
Observation:
1. the larger the training set the better the classifier
2. the larger the test set the more accurate the error estimate

**Factors that influence the model**
They are:
1. algorithm
2. class distribution
3. cost of misclassification
4. *size of training and set*
	1. how much data do I need to train the model? Evaluate the model with different percentages of the training set randomly selected.

## Learning Curve
To better investigate the *effect of the sample size* on the accuracy, it is useful to look at the learning curve.
It shows how accuracy changes with varying sample size.
It requires a sampling schedule.

A small sample size can bias the estimate.
The more data I have, the higher the accuracy and the lower the standard deviation.
However, if acquiring data is expensive and the result is similar, it is better to use a lower, but reasonable, amount of data.

![[Pasted image 20231118114244.png|400]]


## Methods of sampling for data partitioning

**Data partitioning**
To validate the parameter settings.
![[Pasted image 20231118115027.png|400]]

**Methods**
In order to validate the parameters used for training, a validation set must be extrapolated from the training set. There are many methods.
	1. Holdout
	2. Holdout with stratified sampling
	3. Repeated Holdout (random subsampling)
	4. Cross Validation
	6. Bootstrap (sampling with replacement)

*1. Holdout*
The holdout method reserves a certain amount for testing and uses the remaining for training.
Usually 1/3 for testing.
Disadvantages. For small or unbalanced datasets, the samples might not be representative for some classes. (see stratified sampling).

*2. Holdout with Stratified Sampling*
It is a particular type of holdout, with the goal to eliminate the disadvantages for unbalanced dataset.
It is based on balancing the data, so that each class is represented with approximately equal proportions in both subsets.
In this way, we avoid over-sampling and under-sampling for some classes.

*3. Repeated Holdout*
To get more reliable estimates with the holdout, we can repeat the process with different subsamples.
In each iteration, a certain proportion is randomly selected for training. It is better with stratified samples. (see stratified sampling)
The overall error rate is the average of the error on the different iterations.
The downside is in the overlap of different test.

*4. K-fold Cross Validation*
In order to avoid the overlap of different test sets, use k-fold Cross Validation.
Steps
1. split data into k subsets of equal size
2. use each subset in turn for testing (and the remaining for training)
Even better with stratified subsamples.
Even better with repeated cross-validation, taking the average of the k times.

**Evaluation of the model**

![[Pasted image 20231118115822.png]]

# COMPARING models

**Main Methods**
The methods to compare the models are
1. Receiver Operating Characteristic (ROC) Curve
2. Lift Chart

## ROC
It was originally developed in 1950s to analyze noisy signals.
It shows the trade-off between positive hits and false alarms.

**Graph**
The curve plots
- True Positive Rate (on x), also called 'recall' or 'sensitivity'
- False Positive Rate (on y), also called probability of false alarm

$$TPR = \frac{TP}{TP+FN}\qquad FPR=\frac{FP}{TN+FP} $$

![[Pasted image 20231118164535.png|400]]

**Interpretation**
The performance of each classifier is represented as a point on the ROC Curve.
Each parameter setting of the classifier provides a (_FP_, _TP_) pair and a series of such pairs can be used to plot an ROC curve. 
A non-parametric classifier is represented by a single ROC point, corresponding to its (_FP_,_TP_) pair.
Therefore, the parameters can be adjusted to increase _TP_ at the cost of an increased _FP_ or decrease _FP_ at the cost of a decrease in _TP_. 

The AUC (Area Under the Curve) represents the degree of measure of separability. How much a model is capable of distinguish between classes.
The greater the area under the curve, the better the model.
If AUC=1 the curve model can separate classes, if it is 0.5 it is like random.

A model with perfect discrimination passes through the upper left corner
(perfect discrimination means no overlap between the two classes). So the closer the ROC curve to the upper corner, the better the accuracy.

However, no model consistently outperform the other. In fact, a model can be better for FPR and another for high FPR.


![[Pasted image 20240113140202.png|400]]

**How to use the AUC ROC curve for the multi-class model?**

In a multi-class model, we can plot the N number of AUC ROC Curves for N number classes using the One vs ALL methodology. So for example, If you have three classes named X, Y, and Z, you will have one ROC for X classified against Y and Z, another ROC for Y classified against X and Z, and the third one of Z classified against Y and X.

The ROC Curve is also useful to understand which classes are recognized better.


**How to construct the ROC Curve.**
Steps
1. use classifier that produces posterior probability P(+|A) for each test instance
2. sort the instances according to P(+|A) in decreasing order
3. apply threshold at each unique value of P(+|A)
4. count the number of TP, FP, TN, FN at each threshold and calculate TPR and TNR

![[Pasted image 20240113140449.png]]

## Lift Chart

It is a technique popular in direct marketing.
It is used for binary classifiers.

![[Pasted image 20240113165735.png|400]]

The *cumulative lift chart* (or cumulative gains chart) is built as follows.
- on the x-axis, the cumulative number of cases
	- also called Predicted Positive Rate (or support)
	- (in descending order of probability)
- on the y-axis, the cumulative number of true positives
	- also called True Positive Rate (or recall, or sensitivity)
The dashed line is for reference, providing a benchmark. It represents the expected number of positives without a model, which means selected randomly.

![[Pasted image 20240113164859.png|400]]

Basically, it shows how much population we should sample to get the desired TPR (aka sensitivity or recall).
For example, if we want 40% of true positives (TPR), then we should select at least 20% of support (aka Predicted Positive Rate).


