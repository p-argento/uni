# K-Means
It is a Partitional clustering approach.
The number of clusters K must be specified.
## Algorithm
![[Pasted image 20240109180805.png]]

More in details, the algorithm stops when the difference between old and new centroids is less than a threshold.

## Details
Initial centroids are often chosen randomly. For this reason, clusters produced vary from one run to another.
The centroid is typically the mean of the points int the cluster.
The distance can be measured using Euclidean Distance, Cosine similarity, etc.
K-Means converge using common similarity measures.
![[Pasted image 20240109182357.png|300]]
## Evaluating K-Means Clusters
The most common measure is SSE.
For each point, the error is the distance to the nearest cluster.
![[Pasted image 20240109182623.png|300]]
- ! A good clustering with smaller K can have a lower SSE than a poor clustering with higher K

## Limitation of K-Means
Problems with
- outliers
- clusters differing by
	- sizes
	- densities
	- non-globular shapes

How to overcome limitations?
Increase the number of clusters.
## Pre-processing and Post-processing for K-Means
Pre-processing
- normalize data
- eliminate outliers
Post-processing
- eliminate small clusters that may represent outliers
- split cluster with high SSE ('loose' clusters)
- merge clusters with low SSE ('close' clusters)
## Initial Centroids Problem
Possible solutions
1. multiple runs (not ideal)
2. sample and use hierarchical clustering to determine initial centroids (BEST)
3. generate more clusters and post-process
4. generate more clusters and perform hierarchical clustering
5. bisecting k-means (less susceptible to initialization issues)
# Bisecting K-Means
Extension of K-Means.
## Description
It is a variant of K-Means that can produce a *hierarchical clustering*.
*Start with a unique cluster* containing all the points.
Then bisecting the cluster with highest SSE each time, using 2-Means.
Until there are K clusters.

![[Pasted image 20240109185235.png|400]]

## Limitations
The algorithm might terminate at a *singleton cluster* (cluster with one object), but this is meaningless. In fact, intermediate classes are more likely to correspond to real classes.
There is no criterion for stopping bisection before singleton clusters are reached.
# X-Means
Extension of K-Means.

## Bayesian Information Criterion (BIC)




# Expectation Maximization
K-Means Origins.
## Model-based Clustering




# K-Modes
K-Means Brother.

## Description


## Characteristics

**Distance**

**Mode**

**Algorithm**

# Mixture Gaussian Model
K-Means Brother.

## What is a Gaussian Distribution


## Model Description


## Maximum Likelihood Estimation (MLE)



## Mixture of Gaussians



## Mixture Model



---

# K-means
The idea is to divide the dataset in clusters so that the mean of each cluster has a meaning, represent something.
We're studying convex shapes, since with concavities we get weird results, since the mean would be in the hole. But we could also increase the number of clusters to obtain smaller convex shapes.
What is the best k? It is a post-processing choice, because you need to see the difference between the different results.

# Bisecting K-means
From a recursive algo to smaller sets. Starting from a unique cluster, split into 2.

# X-means
New idea for splitting. 
The SSE is always meaningful, it will always be meaningful to split. While with the BIC we can decide how many split at each iteration.
They're greedy algo, simply optimising the next move.

*K-means*
2 dimensions, look for 2 clusters.
The initial centroids are provided.
The idea is to calculate euclidean distance between points and centroid (don't need the distance between all the points).
2. assign each point to the nearest centroid
	1. solve it graphically
		1. draw a line between centroids and an orthogonal line intersecting in the middle.
		2. each side belongs to a different centroid.
Before the next iteration, calculate new centroids.
Calculate the mean of x and y of each cluster for the new centroids. The result is the coordinate for the new centroid. And same for the other centroid.