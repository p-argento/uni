
> Why naive? Because it's assuming that the attributes are conditionally independent (but in reality, they're not independent).

It is a probabilistic framework for solving classification problems.
## Bayes Theorem for classification
The Bayes Theorem states that the probability of getting the class variable $Y$,
given the attribute sets $X={X_1,X_2,...,X_d}$, is
![[Pasted image 20231117182459.png|350]]
In classification, by knowing the posterior probability P(Y|X) for every combination of X and Y, a test record X' can be classified by *finding the class Y' that maximizes the posterior probability P(Y'|X')*.
And for the Bayes Theorem, this is equivalent of choosing the value of Y' that maximizes P(X'|Y')P(Y').

## Naive bayes classifier
It assumes that the attributes are conditionally independent.
Then *it estimates the class-conditional probability given the class label y*.
Given each attribute set $X=\{X_1,X_2,...,X_d\}$,
the conditional independence is$$goal: P(X|Y=y)=\Pi^d_{i=1}P(X_i|Y=y)$$The classifier is based on the following *conditional independence assumption*.
Given $Y,X_1,X_2$ , we can say that $Y$ is independent from $X_1$ given $X_2$, if $$assumption: P(Y|X_1,  X_2)=P(Y|X_2)$$With this assumption, instead of computing the class-conditional probability for every combination of X, we *only have to estimate the conditional probability of each $X_i$ given $Y$*.
Thus, to classify a record, the naive Bayes classifier computes the posterior probability for each class Y $$bayes: P(Y|X)=\frac{P(Y)\ \Pi^d_{i=1}P(X_i|Y=y)}{P(X)}$$Then, it takes the class with the maximum posterior probability as result.

## How to estimate probability from data


## M-estimate of Conditional Probability

