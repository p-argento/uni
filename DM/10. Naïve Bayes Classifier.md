
> Why naive? Because it's assuming that the attributes are conditionally independent (but in reality, they're not independent).

It is a probabilistic framework for solving classification problems.

## Probability Review
![[Pasted image 20240113180220.png]]

## Bayes Theorem for classification
The Bayes Theorem states that the probability of getting the class variable $Y$,
given the attribute sets $X={X_1,X_2,...,X_d}$, is
![[Pasted image 20231117182459.png|350]]
In classification, by knowing the posterior probability P(Y|X) for every combination of X and Y, a test record X' can be classified by *finding the class Y' that maximizes the posterior probability P(Y'|X')*.
And for the Bayes Theorem, this is equivalent of choosing the value of Y' that maximizes P(X'|Y')P(Y').

## Naive bayes classifier
It assumes that the attributes are conditionally independent.
Then *it estimates the class-conditional probability given the class label y*.
Given each attribute set $X=\{X_1,X_2,...,X_d\}$,
the conditional independence is$$goal: P(X|Y=y)=\Pi^d_{i=1}P(X_i|Y=y)$$The classifier is based on the following *conditional independence assumption*.
Given $Y,X_1,X_2$ , we can say that $Y$ is independent from $X_1$ given $X_2$, if $$assumption: P(Y|X_1,  X_2)=P(Y|X_2)$$With this assumption, instead of computing the class-conditional probability for every combination of X, we *only have to estimate the conditional probability of each $X_i$ given $Y$*.
Thus, to classify a record, the naive Bayes classifier computes the posterior probability for each class Y $$bayes: P(Y|X)=\frac{P(Y)\ \Pi^d_{i=1}P(X_i|Y=y)}{P(X)}$$Then, it takes the class with the maximum posterior probability as result.


## How to deal with continuous attributes?
Possibilities
- discretize the range into bins
	- but it violates the independence assumption
- two-way split (X<v)
- probability density estimation
	- assuming the attribute follows a normal distribution, estimate the parameters like mean and standard deviation
	- then use it to estimate the conditional probability P(X|y)

**Example of classification using probability density estimation**
![[Pasted image 20240113181710.png]]



## M-estimate of Conditional Probability
![[Pasted image 20240113180538.png]]
M is a parameter, typically the number of classes.
If one of the conditional probability is zero, then the entire expression becomes zero. This happens if the combination is absent in the training data.
Therefore, both can be zero. How to decide?

*this part is not clear*
We use the M-estimate, which is
$$M\ estimate\rightarrow\ P(X|Y)=\frac{N_{xy}+mp}{N_y+m}$$
Where
- $N_{xy}$ is the number of training instances with $X_i=x$ and $Y=y$
- $N_y$ is the number of training instances with $Y=y$
- $m$ is a hyper-parameter that indicates the confidence in using p
- $p$ is some initial estimate of $P(x|y)$ known a priori

Or a Laplacian Estimation
$$Laplace\ estimate\rightarrow P(X|Y)=\frac{N_{xy}+1}{N_y+|X|}$$
where
- $|X|$ is the total number of values that $X_i$ can have
*maybe there is a mistake in the slide*

## Comments
NB has some advantages like
- it is robust to isolated noise points.
- it handles missing values by ignoring the instance
	- during probability estimate calculations.
- it is robust to irrelevant attributes
The problem is that
- the independence assumption may not hold for some attributes
	- there are better techniques like Bayesian Belief Networks
