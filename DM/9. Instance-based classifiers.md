[[DM Classification]]

# Intro
---
**Definition**
Instance-based learning (sometimes called *memory-based learning*) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory.

**Advantages**
- Adaptation -> for previously unseen data, it store a new instance or throw an old instance away.

**Disadvantages**
- "Lazy learners" -> computation is postponed until a new instance is observed, without building a model explicitly (model free)
	- in contrast, "eager learners" spend time in building the model, but then the classification is faster
- Expensive classification of unknown records -> given n training items, the complexity of classifying a single instance is O(n)
	- we need to compute every time the proximity values between the test instance and the training examples
- Suscetible to noise -> because it is based on local data

**Examples**
1. K Nearest Neighbor (KNN)
2. Kernel Machines
3. Radial Basis Function Network (RBF Networks)
4. Self-Organizing Map (SOM)
5. Learning Vector Quantization (LVQ)
6. Locally Weighted Learning (LWL)
7. Case-Based Reasoning

# K-NN
---
It's the Nearest-Neighbour Classifier.
Basic Idea:
> If it walks like a duck, quacks like a duck, then it's probably a duck.

**Requires**
1. Training Set of stored records
2. Distance metric between records
3. Value of k nearest neighbour to retrieve

**Instructions**, given a training set and a test set.
1. Compute distances from records
2. Identify k nearest records
3. Determine label
	1. (classification) by taking the majority labels of nn
	2. (regression) by computing the average between values

**Choosing the value of K**
- too small -> sensitive to noise points and leads to overfitting
- too large -> neighbourhood includes points from other classes
General practice ->  *$K=sqrt(N)$* where N is the number of samples in the training set.

**How to determine the class**
Majority Voting
-> take the majority vote of class labels among the k-nearest-neighours
Distance-weighted Voting (to reduce the impact of k)
-> use the weight factor $w=1/d^2$ to weight the vote according to the euclidean distance.

**Dimensionality issues**
High dimensional data (curse of dimensionality) -> normalize vectors to unit length.
High differences in the scale measure of attributes -> scale

**PEBLS**
Parallel Exemplar-Based Learning System is a KNN algorithm with K=1.
It is designed to handle nominal attributes.
It measures the distance between two values of a nominal attribute using the Modified Value Difference Metric (MVDM).

The distance between Nominal Attributes Values is: $$d\left(V_1, V_2\right)=\sum_{i=1}^k\left|\frac{n_{i 1}}{n_1}-\frac{n_{i 2}}{n_2}\right|$$$V_1,V_2$ -> the pair of nominal attributes values.
$i$ -> the class to label 
$n_{ij}$ -> number of examples from class $i$ with attribute value $V_j$
$n_j$ -> number of examples with attribute value $V_j$

![[Pasted image 20231117171143.png]]

**Distance between Records**
![[Pasted image 20231117181518.png]]


# TO BE ADDED
---

# KNN
---
It is the Nearest Neighbour Classification.
It is a instance-based classifier.
You compare the target value with the set of ducks which is nearest. This set is what defines the attributes of "a duck".
> If it walks like a duck, quacks like a duck, then it's probably a duck.

How to choose k?
If K is too small, it leads to overfitting (relying too much of a singular value).
If K is too large, the neighbourhood might include points from other classes.
Set $K=sqrt(N)$, as a general rule of thumb.

How does the voting work?
Weight the vote according to the distance ($w = 1/(d^2)$)

Dimensionality and scaling issues.
Curse of dimensionality -> normalize the vector to unit length
..


Memory-based learning.
..

To find the parameter, divide the train set in train' and validation. Apply the algorithm to find the parameter validated through the validation set. Now use parameter on the whole original train set.

PEBLS.
It is categorical KNN.
Until now, since we rely on Euclidean distance, we can work onyl with continuous attributes.
It uses a distance calculated on the target value.

Lazy-learner means calculus made at prediction time.

For classification you use the mode (majority voting).
For regression you use the average of the set of ducks.


# Naive Bayes
---
> Why naive? Because it's assuming that the attributes are conditionally independent.

In reality, they're not independent.

