# Decision Trees Introduction
---
Split until the splitting attributes directly leads to a specific class, in a sense that it does not need to be split again to find a specific class.
For example, if 'home owner=yes' then the class is for sure 'no'.

![[Pasted image 20231122113533.png|400]]

> Applied typically for tabular data.
	When the slip is not binary, who tells me what is the right split?
	There could be more than one tree that fits the same data.
	Use greedy algorithm, but it is not possible to grasp the real optimum.


# Decision Tree Induction
---
## Algorithms to build the tree
Many possible decision trees can be constructed from a particular data set.
Some trees are better than others, but finding an optimal one is computationally expensive due to the exponential size of the search space.
There are algorithms to find a reasonably accurate tree in a reasonable amount of time, even though these trees are still suboptimal.
These algorithms employ a **greedy strategy** to grow the decision tree top-down, by making *a series of locally optimal decisions*.
These decisions involves the choice of the attribute to use when partitioning the training data.

There are many algorithms to build the tree:
1. Hunt
2. CART
3. ID3, C4.5
4. SLIQ,SPRINT
## Hunt's Algorithm
How to split?
As long as a note has labels of more than one class, apply the splitting recursively.
If a node is associated with instances than more than one class, it is expanded using an attribute test condition, determined using a **splitting criterion**.
## Hunt's assumptions
*1. No attribute value -> empty children.*
If none of the training instances have the particular attribute value, some of the child nodes created can be empty.
Solution? Declare it as a leaf node, then assign to it the class label that occurs most frequently among the training instances associated with their parent nodes.
*2. Identical attributes but different labels-> node not expandable*
If all training instances associated with a node have identical attribute values but different class labels, the node cannot be expanded further.
Solution? Declare it as a leaf node, then assign to it the class label that occurs most frequently in the instances associated with the node.
## Hunt's Issues
Issues in the design of the algorithm.
*1. What is the splitting criterion?*

*2. What is the stopping criterion?*


## Different Types of Splits
Depends on attribute types
1. binary
2. nominal
3. ordinal
4. continuous
Depends on number of ways to split
1. 2-way split
2. multi-way split
## Finding the Best Split
1. Compute impurity measure (P) before splitting
2. Compute impurity measure (M) after splitting
	1. in particular, the weighted impurity of each children
3. Choose the attribute test condition that produces
	1. highest gain (G=P-M)
	2. (or) lowest impurity after splitting (M)

## Measures of Impurity
Which one
1. GINI
2. Entropy
	1. Information Gain
	2. Gain ratio
3. Misclassification Error

## Comparison among Impurity Measures

**GINI**
We want to determine the best feature to use as the root node for the decision tree. To do this, we will calculate the Gini impurity for each feature and select the feature with the lowest Gini impurity.


## Stopping Criteria for Tree Induction



