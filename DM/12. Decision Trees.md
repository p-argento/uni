# Decision Trees Introduction
---
Split until the splitting attributes directly leads to a specific class, in a sense that it does not need to be split again to find a specific class.
For example, if 'home owner=yes' then the class is for sure 'no'.

![[Pasted image 20231122113533.png|400]]

> Applied typically for tabular data.
	When the split is not binary, who tells me what is the right split?
	There could be more than one tree that fits the same data.
	Use greedy algorithm, but it is not possible to grasp the real optimum.


# Decision Tree Induction
---
## Algorithms to build the tree
Many possible decision trees can be constructed from a particular data set.
Some trees are better than others, but finding an optimal one is computationally expensive due to the exponential size of the search space.
There are algorithms to find a reasonably accurate tree in a reasonable amount of time, even though these trees are still suboptimal.
These algorithms employ a **greedy strategy** to grow the decision tree top-down, by making *a series of locally optimal decisions*.
These decisions involves the choice of the attribute to use when partitioning the training data.

There are many algorithms to build the tree:
1. Hunt
2. CART
3. ID3, C4.5
4. SLIQ,SPRINT
## Hunt's Algorithm
How to split?
As long as a note has labels of more than one class, apply the splitting recursively.
If a node is associated with instances than more than one class, it is expanded using an attribute test condition, determined using a **splitting criterion**.
## Hunt's assumptions
*1. No attribute value -> empty children.*
If none of the training instances have the particular attribute value, some of the child nodes created can be empty.
Solution? Declare it as a leaf node, then assign to it the class label that occurs most frequently among the training instances associated with their parent nodes.
*2. Identical attributes but different labels-> node not expandable*
If all training instances associated with a node have identical attribute values but different class labels, the node cannot be expanded further.
Solution? Declare it as a leaf node, then assign to it the class label that occurs most frequently in the instances associated with the node.
## Hunt's Issues
Issues in the design of the algorithm.
*1. What is the splitting criterion?*


*2. What is the stopping criterion?*
If all the records belong to the same class.
If all have identical attribute values.
If early termination has been set.

## Different Types of Splits
Depends on attribute types
1. binary
2. nominal
3. ordinal
4. continuous
Depends on number of ways to split
1. 2-way split
2. multi-way split

For continuous values?
Discretize, then use binary or multi-way split.
## Finding the Best Split
We prefer nodes with purer / homogeneous class distribution.
Therefore, we need a measure if node impurity.
It a greedy approach, finding local optima iteratively.

Steps
1. Compute impurity measure (P) before splitting
2. Compute impurity measure (M) after splitting
	1. in particular, the weighted impurity of each children
3. Choose the attribute test condition that produces
	1. highest gain (G=P-M)
	2. (or) lowest impurity after splitting (M)

## Measures of Impurity
Which one
1. GINI
2. Entropy
	1. Information Gain
	2. Gain ratio
3. Misclassification Error
![[Pasted image 20240112154915.png]]

## 1. GINI
We want to determine the best feature to use as the root node for the decision tree. To do this, we will calculate the Gini impurity for each feature and select the feature with the lowest Gini impurity.

![[Pasted image 20240112161722.png|300]]

For a given node t, where $p(j|t)$ is the relative frequency of class j at node t.
The Maximum is $(1-\frac 1 n_c)$  with $n_c$ number of classes. It happens when records are equally distributed among all classes and the impurity is maximum, implying least interesting information.
The Minimum us (0.0). It happens when all records belong to one class and the purity is maximum, implying most interesting information.



## 2. Entropy

![[Pasted image 20240115235336.png]]

**Gain Ratio**

![[Pasted image 20240115235407.png]]



## 3. Misclassification Error

![[Pasted image 20240115235317.png]]




## Comparing Impurity Measures

![[Pasted image 20240112155332.png]]


## Advanced Algorithms
![[Pasted image 20240112155357.png]]


## Advantages of Decision Trees

![[Pasted image 20240115235519.png]]

## Oblique Decision Trees

They overcome the limitation of splitting using only one attribute at a time, which is a limitation of Decision Trees.
The *Decision Boundary* is the border line between two neighboring regions of different classes.
It is parallel to axis because the split involves only a single attribute at a time.
In Oblique Decision Trees, we involve multiple attributes.

![[Pasted image 20240115235955.png]]

![[Pasted image 20240116000012.png]]

## Overfitting

![[Pasted image 20240116000150.png]]

![[Pasted image 20240116000210.png]]

**Occam's Razor**
Given two models of similar generalization errors, one should prefer the simpler model over the more complex model.
For complex models, there is a greater chance that it was fitted accidentally by errors in data.
Therefore, one should include model complexity when evaluating a model.

**Estimating the complexity of a DT**

![[Pasted image 20240116000456.png]]

![[Pasted image 20240116000548.png]]

1. Pessimistic Approach

![[Pasted image 20240116000604.png]]

2. Optimistic Approach

![[Pasted image 20240116000650.png]]

**How to address overfitting**

Methods
1. Pre-pruning
	1. early stopping with more restrictive conditions
2. Post-Pruning



