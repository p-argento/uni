# Hierarchical Clustering
## Dendrograms
It is used to visualize the nested clusters that results from the hierarchical clustering.
It is a tree-like diagram with the sequences of merges or splits.

![[Pasted image 20240111144355.png]]

## Strengths
The number of clusters doesn't need to be defined previously.
The desired number of clusters can be obtained cutting the dendrogram at the proper level.
The clustering may correspond to meaningful taxonomies like animal kingdoms.

## Types
Two main types:
1. Agglomerative
	1. start with points and merge
2. Divisive
	1. start with one cluster and split
Both uses a similarity or distance matrix.
## Agglomerative Clustering
It is the most popular hierarchical clustering technique.
The basic algorithm is:
1. Compute the proximity matrix
2. Let each data point be a cluster
3. Repeat until only one single cluster remains
	1. merge the two closest clusters
	2. update the proximity matrix

# How to define inter-cluster distance / similarity
How to update the proximity matrix in the algorithm?
This is the question we are trying to answer.

Methods for defining inter-cluster similarity:
1. MIN
2. MAX
3. Group Average
4. Distance Between Centroids
5. Other methods driven by an objective function (Ward)

## 1. MIN (or Single Link)
Proximity of two clusters is based on the two closes points in the two clusters.
The strength is that it can handle non elliptical shapes.
The limitation is that is can be sensitive to noise and outliers.

## 2. MAX (or Complete Linkage)
Proximity of two clusters is based on the two most distance points in the two clusters.
The strength is that it is less sensitive to noise and outliers.
The limitation is that is biased towards globular clusters and tends to break large clusters.

## 3. Group Average
Proximity of two clusters is the average of pairwise proximity between points in the two clusters.
![[Pasted image 20240111150657.png|300]]

It is a compromised between single and complete link.
It is less susceptible to noise and outliers, but still biased towards globular clusters.

## 4. Ward's Method
Similarity of two clusters is based on the increase in squared error when two clusters are merged.
It is similar to group average, but the distance is squared.
It is the hierarchical analogue of K-means. And can be used to initialize K-Means.
It is less susceptible to noise and outliers, but still biased towards globular clusters.

![[Pasted image 20240111151031.png]]

# Additional Notes
*hierarchical clustering*
Nested clusters organised as a hierarchical tree.
It is alway a bidimensional tree.
Limited scalability of the problem. So it is used in combination with other methods like k-means (with a large k).
Bottom-up costruction, merging cluster, is the most efficient way.
*exercises*
They provide the proximity matrix (pairwise).
> If it's a distance matrix, you look for the MIN distance.
> If it's a similarity matrix, you look for the MAX value.

1. if the distance merge is all the same (starting from the min), merge the points.
2. recalculate the distance matrix but using new clusters as columns
	1. in case of single-linkage, take the MIN distance between clusters
	2. in case of complete-linkage, take the MAX distance between clusters