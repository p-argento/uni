## Simple Attributes
![[Pasted image 20240109123032.png|500]]

## Euclidean distance
![[Pasted image 20231107154548.png|200]]
Standardisation is necessary.

## Manhattan Distance (city block)
Uses absolute value instead of square root.

## Minkowski Distance
It is a generalization of euclidean. Uses the abs instead of square.
![[Pasted image 20231107155238.png|200]]

Particular types of Minkowsky Distance
- euclidean distance
- manhattan distance
![[Pasted image 20240109122846.png|500]]

## Distance Matrix
(or proximity matrix). Clearly symmetric, because the distance xy=yx.


## Properties of similarity vs distance
| Distance | Similarity |
| ---- | ---- |
| 1. Positive Definiteness<br>-> (x, y) >= 0 for all x and y and d(x, y) = 0 only if x = y.<br><br>2. Symmetry<br>-> d(x, y) = d(y, x) for all x and y.<br><br>3. Triangle Inequality<br>-> d(x, z) <= d(x, y) + d(y, z) for all points x, y, and z. (Triangle Inequality) | 1. Maximum Similarity<br>-> s(x, y) = 1 (or maximum similarity) only if x = y. (does not always hold, e.g., cosine)<br><br>2. Symmetry<br>- > s(x, y) = s(y, x) for all x and y. |
Remember that distance = dissimilarity


## Similarity between Binary Data
How
- Simple Matching Coefficient (SMC)
- Jaccard Coefficients

**Simple Matching Coefficient**
![[Pasted image 20240109152721.png|400]]

**Jaccard Coefficient**
![[Pasted image 20240109152800.png|400]]

![[Pasted image 20240109154138.png|400]]

## Cosine Similarity
To be used for document vectors.
![[Pasted image 20240109153933.png|400]]
It basically uses the definition of dot product with the cosine.
![[Pasted image 20240109154052.png|400]]

## Combining Similarities using Weights
If we don't want to treat all attributes the same, we can use non-negative weights $\omega$.
![[Pasted image 20240109154301.png|300]]

## Correlation
Measures the linear relationship between objects, they must be binary or continuous. After standardization.

**Pearson Coefficient**
![[Pasted image 20240109153340.png|500]]

**Spearman Coefficient**
The Spearman correlation coefficient is defined as the Pearson coefficient between the rank variables.

![[Pasted image 20240115175834.png|400]]

While Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not).
Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of −1) rank between the two variables.
## Heterogenous Distance
What if we have data with both continuous and categorical attributes.
Options
1. discretize continuous attributes and use categorical distances like Jaccard or SMC
2. define a new heterogeneous distance