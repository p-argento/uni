# K-Means
It is a Partitional clustering approach.
The number of clusters K must be specified.
## Algorithm
![[Pasted image 20240109180805.png]]
Calculate the mean of x and y of each cluster for the new centroids. 
More in details, the algorithm stops when the difference between old and new centroids is less than a threshold.

## Details
Initial centroids are often chosen randomly. For this reason, clusters produced vary from one run to another.
The centroid is typically the mean of the points int the cluster.
The distance can be measured using Euclidean Distance, Cosine similarity, etc.
K-Means converge using common similarity measures.
![[Pasted image 20240109182357.png|300]]
## Evaluating K-Means Clusters
The most common measure is SSE.
For each point, the error is the distance to the nearest cluster.
![[Pasted image 20240109182623.png|300]]
$m_i$ is the representative point of a cluster, which is usually the mean, called centroid.
- ! A good clustering with smaller K can have a lower SSE than a poor clustering with higher K

## Limitation of K-Means
Problems with
- outliers
- clusters differing by
	- sizes
	- densities
	- non-globular shapes

How to overcome limitations?
Increase the number of clusters.
## Pre-processing and Post-processing for K-Means
Pre-processing
- normalize data
- eliminate outliers
Post-processing
- eliminate small clusters that may represent outliers
- split cluster with high SSE ('loose' clusters)
- merge clusters with low SSE ('close' clusters)
## Initial Centroids Problem
Possible solutions
1. multiple runs (not ideal)
2. sample and use hierarchical clustering to determine initial centroids (BEST)
3. generate more clusters and post-process
4. generate more clusters and perform hierarchical clustering
5. bisecting k-means (less susceptible to initialization issues)
# Bisecting K-Means
Extension of K-Means.
## Description
It is a variant of K-Means that produces a *hierarchical clustering*.
*Start with a unique cluster* containing all the points.
Then bisecting the cluster with highest SSE each time, using 2-Means.
Until there are K clusters.

![[Pasted image 20240109185235.png|400]]

## Limitations
The algorithm might terminate at a *singleton cluster* (cluster with one object), but this is meaningless. In fact, intermediate classes are more likely to correspond to real classes.
There is no criterion for stopping bisection before singleton clusters are reached.
# X-Means
Extension of K-Means.
## Bayesian Information Criterion (BIC)
It is a strategy  to stop the Bisecting Algorithm when meaningful clusters are reached to avoid over-splitting.
It can be used as splitting criterion to decide to split a cluster or not.
It measures the improvement of the cluster structure.
If the BIC of the children is more than the BIC of the parent, we accept the bisection.

## X-Means Algorithm
It is a mix of k-means and bisecting k-means.
The crucial difference is that it uses the BIC instead of SSE.
Steps
1. run k-means
2. split each cluster with bisecting 2-means
3. compare the local BIC of parent and children
4. keep only centroids with the higher local BIC
The algorithm stops if 
- k>max, means that we reached the number of desired clusters
	- with reference to the global BIC

![[Pasted image 20240113215433.png]]

## BIC formula in X-means

![[Pasted image 20240113220153.png]]

It is basically the adjusted "log-likelihodd" of the model.
The likelihood that the data is explained by the clusters, according to the spherical-gaussian assumption of k-means.
It estimates how closely the centroids are to the points of the cluster.


# Expectation Maximization
K-Means Origins.
## Model-based Clustering
The idea is that, in most cases, a single distribution is not good enough to describe all data points: different parts of the data follow a different distribution.
It is the generalization of the structure of k-means.

## Algorithm
Steps
1. Initialize the parameters to some random values
2. Repeat until convergence
	1. Estimation-Step
		1. given the parameters, estimate probabilities 
	2. Maximization-Step
		1. given the probabilities, calculate new parameters that maximize the data likelihodd

In the K-means
1. initialize centroids
2. repeat
	1. given the centroids, assign points to clusters based on distance with centroids
	2. given the assigments of points to clusters (100% probability), compute new centroids


# K-Modes
K-Means Brother.
## Description



## Characteristics
**Distance**
Instead of the Euclidean Distance, K-Modes uses the number of mismatches between the attributes of two objects.

![[Pasted image 20240113221445.png|300]]

**Mode**
Instead of the centroid, K-Modes uses the mode as representative object of a cluster. Given the points in the cluster C, the mode is the max frequency of each attribute.
$$f(A_j=c_{l,j}|X_l)=\frac {n_{c_{l,k}}}{n}$$

**Algorithm**
Steps
1. Randomly select the initial objects as modes
2. Repeat until no object change the assigned cluster
	1. Scan the data to assign each object to the closer cluster identified by the mode
	2. Compute the new mode of each cluster

# Mixture Gaussian Model
K-Means Brother.

## What is a Gaussian Distribution


## Model Description


## Maximum Likelihood Estimation (MLE)



## Mixture of Gaussians



## Mixture Model



---

# K-means
The idea is to divide the dataset in clusters so that the mean of each cluster has a meaning, represent something.
We're studying convex shapes, since with concavities we get weird results, since the mean would be in the hole. But we could also increase the number of clusters to obtain smaller convex shapes.
What is the best k? It is a post-processing choice, because you need to see the difference between the different results.

# Bisecting K-means
From a recursive algo to smaller sets. Starting from a unique cluster, split into 2.

# X-means
New idea for splitting. 
The SSE is always meaningful, it will always be meaningful to split. While with the BIC we can decide how many split at each iteration.
They're greedy algo, simply optimising the next move.

*K-means*
2 dimensions, look for 2 clusters.
The initial centroids are provided.
The idea is to calculate euclidean distance between points and centroid (don't need the distance between all the points).
2. assign each point to the nearest centroid
	1. solve it graphically
		1. draw a line between centroids and an orthogonal line intersecting in the middle.
		2. each side belongs to a different centroid.
Before the next iteration, calculate new centroids.
Calculate the mean of x and y of each cluster for the new centroids. The result is the coordinate for the new centroid. And same for the other centroid.