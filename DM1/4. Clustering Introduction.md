# Cluster Analysis
## What is Clustering
A clustering is a set of clusters.
What is NOT clustering?
- NOT Simple Segmentation (like dividing alphabetically)
- NOT the result of a query
- NOT class label information (supervised learning),..

Two main ideas.
1. Intra-cluster distances are minimized
2. Inter-clusters are maximized

Applications of Cluster Analysis
1. Understanding
2. Summarization

## Types of clustering set
Main Types
- **Partitional Clustering** -> non-overlapping clustering
- **Hierarchical Clustering** -> nested clusters (dendrogram)
![[Pasted image 20240109162003.png|400]]
Other distinctions between sets of clusters
1. exclusive vs non-exclusive
2. fuzzy vs non-fuzzy
	1. a point belongs to a cluster with some weights between 0 and 1 (total=1)
3. partial vs complete
	1. cluster only some data
4. heterogenous vs homogenous
## Types of clusters
Main types
- center-based
	- an object is closer to the centroid of the cluster than any other centroid
	- the centroid is the average of all points in the cluster
- contiguity-based (nearest neighbour)
	- ? each point is closer to at least one point in its cluster than to any point in another cluster
	- have trouble with noise
- density-based
	- cluster is a dense region of points
	- better for outliers or irregular shapes
-  defined by objective function
	- minimize or maximize the objective function
	- typically hierarchical algorithms has local objective, while partitional algorithms have global objectives
Other types
- well-separated
	- if every point is closer to every other point in the cluster than to other points not in the cluster
- property or conceptual
# Cluster validation
## How to validate Clusters
To compare clustering algorithms results.

Why to evaluate?
1. avoid finding patterns in noise
2. compare clustering algorithms
3. compare two sets of clusters
4. compare two clusters

Determine the correct number of clusters.

## Measure of Cluster Validity
Criteria:
- external index
	- to measure the extent to which cluster labels match externally supplied class labels
	- ie entropy
- internal index
	- without external information
	- ie sse, cohesion and separation, silhouette
- relative index
	- to compare different clusters
## Correlation using the Similarity Matrix
Compute the correlation between the similarity matrix and the ideal similarity matrix.
The ideal similarity matrix is a binary matrix where 1 indicates that two points belong to the same cluster and 0 otherwise.
High correlation indicates that points that belong to the same cluster are close to each other.

![[Pasted image 20240109164833.png|400]]
## Internal measures
Internal Index: Used to measure the goodness of a clustering structure without respect to external information

1. SSE
2. Cohesion
3. Separation
4. Silhouette Coefficient

**Cohesion**
It is measured by the SS within a cluster.
Called Within Sum of Squares.
![[Pasted image 20240113222351.png]]
Cluster Cohesion is the sum of the weight of all links within a cluster.

**Separation**
Separation is measured by the SS between clusters.
Called Between Sum of Squares.
![[Pasted image 20240113222406.png]]
Where $|C_i|$ is the size of the cluster i.
Cluster separation is the sum of weights between nodes in the cluster and outside.

**Silhouette coefficient**
The silhouette coefficient combines the ideas of cohesion and separation.
It can be used for points, clusters or clusterings.

For an individual point in a cluster, 
$$s=\frac{(b-a)}{max(a,b)}$$
Where
- $a$ is the avg distance of i to the points of its cluster (cohesion)
- $b$ is the avg distance of i to the points of another cluster (separation)

That simplified is
$$s = \frac {separation\ â€“\ cohesion} {max(of\ the\ two)}$$
It is typically between 0 and 1. Where 1 is better.

## Assessing the significance of cluster validity measures

Statistics as a framework to interpret any measure.
The more atypical a clustering is, the more likely it represents valid structure in the data. It means that more atypical index values means valid results.
Look at the position on the histogram, comparing the value obtained with those resulting from random data.

For SSE.
For example, compare SSE of three cohesive clusters against three clusters in random data.

![[Pasted image 20240111193830.png]]

For Correlation.



# Additional Notes during Clustering lab
It is mandatory to normalize data for clustering, since it is based on distances. It might be numbers expressed in different scales, and then the computation of the distance would be biased.
Normalise all the columns in the same way.
Use MINMAX Normalisation for clustering.

Add random_state parameter in KMeans to confront the results in a group, otherwise we get different results.

Silhoutte. (slide) 
A is average distance between points in the same class.
B is minimum average distance ...
It becomes negative if there's something wrong (->-1).
Repeat the computation for all the points.
We're looking for the 'knee'.

How to calculate the centroids?
With df.groupby.mean

If you wnat to use categorical attributes for clustering you need to define a different distance (not euclidean).
If you mix categorical and continuous attributes using euclidean distance is a mistake.
You can use categorical attributes to explore.

![[clustering-comparison.png]]