## The Sorting Problem
Input: sequence `A[1,n]` of n elements
Goal: permutation `A'[1,n]` of A (without creating a new element) such that $$
A'[i]\leq A'[i+1]\qquad\forall i\in[1,n-1]
$$All the books about algorithms are 1-based (NOT 0-based like python).
Keep this in mind when turning pseudo-code into real-code.

What is the idea for ordering a vector?
Never use an external number to replace a number which was supposed to be removed. It is a patch and it is wrong.
Swap the number to be removed with the last one and remove the last one.

> Try to implement a sorting algorithm with the min.

Instead of creating a new array, it is a better idea to divide the array in sorted and unsorted sub-arrays. Each time a min is computed, the number goes to the left and the index of the unsorted sub-array is increased by 1 (started from 0).
This algorithm is called *selection sort*.

Iterating the array n times and swapping each time the consecutive element if smaller. It is called *bubble sort*. Never use this, it is one of the worst.

Another approach is place each number in its correct position. Start with a one-number array and increasingly add items in the right position.
This is 

Which one is the best?
> Try to implement this in python.


+++21.02.24+++

The complexity of an algorithm is the number of steps required in order to find a solution.

## Insertion Sort
It is an "inplace sort", meaning that modifies the original array.
It doesn't need extra space for creating a new array.
We process the array and when the i-th term is not sorted, it will be moved backwards to its correct position, comparing with each antecedent until we found the smaller or equal.

Pseudocode.
i is the antecedent, j is the item to be moved.
```pseudocode
InsertionSort(A)
	for j=2 to length
		key = A[j]
		// insert key in already sorted A[1,i-1]
		i = j - 1
		while i > 0 and A[i] > key
			A[i+1] = A[i]
			i = i - 1
		A[i+1] = key
```

> Always simulate. Design an example and run the algorithm step by step.
> I strongly encourage you to try more examples.

Running Example.
> Do not believe in comments


Let's prove the *correctness of this algorithm*.
There are no automatic tools to check correctness,  we must use a proof.
"unit testing" means defining some examples, especially with marginal cases like empty array.
The larger the set for testing is, the better is the confidence in the correctness of the algorithm.

## Loop invariant
Idea
1. find a property which is valid for every iteration of the algorithm.
2. using this property you should be able to check correctness.
3. the goal is to prove that after the loop the array is sorted.
Definitions
1. loop invariant: a property that holds for every iteration of the algorithm
2. initialization: the property holds before the loop starts
3. maintenance: the property holds after every iteration
4. termination: the property holds after the loop terminates

*Loop invariant for Insertion Sort*
At iteration j, `A[1,...,j-1]` is sorted.

## Stability
Idea
1. if we look at the array before and after being sorted, and we focus on different occurrences of the same values, then those occurrences *preserve their relative order*
	1. $...n_1...n_2...n_3... \rightarrow ...n_1n_2n_3...$

## Analysis of Algorithm
What does it mean? It means finding a mathematical formula which is able to tell us how many steps the algorithm requires. What does a step mean?
We need to agree on a *computer model* that should be
1. simple enough to make analysis possible
2. precise enough to give meaningful analysis
There a lot of computational models, each one analysing a different aspect like memory. We focus on the easiest, which is called RAM Model.
It allows to remove algorithms that are very inefficient. But to compare two algorithms that looks efficient, the only way is testing.

*Ram Model*
CPU takes data from a memory. Afterwards the result will be stored again in mamory.
In our model, all the operations have the same cost (like sum and square root) and access from memory has always the same cost. But in real life this is far from being true.
Then, there are registers (increasingly bigger):
1. CPU registers of memory
2. L1 cache (KB)
3. L2 cache (MB)
4. L3 cache (MB)
5. RAM (GB)
6. Disks (TB)
Accessing data in different registers costs differently, increasingly more expensive.
CPU performs operations between (at most 2) values in memory and saves the result back in memory.

What are the *operation* the CPU is allowed to perform?
Set of elementary operations: $+, -, *, /, \sqrt{}, log, \lceil\ \rceil, \lfloor\ \rfloor, and, or$
The cost of any operations is 1.
But remember that this is irrealistic because $\sqrt{}$  cost is much more than $+$
### Analyzing Insertion Sort
*Constant* means independent of the size of the input. But depends on the machine.
How many times are we executing every constant instruction?
In general, we evaluate a loop condition one time more (when we stop and do not enter the loop).
f(n) is the worst case , among all possible inputs, meaning the worst possible input for the algorithm.
Remember that this type of analysis can be problematic, because most cases can be actually efficient, and the worst case can be marginal.

The magic formula is $cost\times times$
But cost is constant, meaning that depends on the computer.

> try to calculate the f(x) of insertion sort for some arrays.
> What is the best case? What is the worst case?

+++23.02.24+++

Being a single line doesn't mean costant operation.
For example, `if 10 in array` costs the length of array.

*What is the running time?*
```pseudocode
InsertionSort(A)
	for j=2 to length
		key = A[j]
		// insert key in already sorted A[1,i-1]
		i = j - 1
		while i > 0 and A[i] > key
			A[i+1] = A[i]
			i = i - 1
		A[i+1] = key
```
$$\displaylines{T(n)=\\
=C_1n+C_2(n-1)+C3(n-1)+C_4\sum_{j=2}^nt_j+C_5\sum_{j=2}^n(t_j-1)+C_6\sum_{j=2}^n(t_j-1)+C_7(n-1)}$$
This is the complexity, the running time.
We always want an overestimation.
But the goal is not giving an estimation of the running time of an algorithm.
The main goal is to compare two solutions. And with this formula is quite hard to compare. Let's try to simplify, by approximating stuff, keep in mind that we want to overestimate.

We declare that there is a $C\geq C_i$ so that $$\begin{align}
T(n)&=Cn+3C(n-1)+C\sum_{j=2}^N+2C\sum_{j=2}^N(t_j-1) \\
&=3C\sum t_j+4Cn-3C
\end{align}$$In bubble sort, we do n iterations, but the cost of swapping is constant. Does the running time depends on the (different) input? No, because it depends only on n and not on swapping or not. The array must be scanned anway.
How can we compare this bubble sort idea with the $T(j)$ of insertion sort? It is still difficult because we do not have $t_j$ in bubble sort.

What we want is the worst time possible. Now, we have one single target.
Observe that $T(n)$ depends on the particular input we use because of $t_j$s.
If we know the typical $t_j$ of your application, you can fix it and calculate.
Otherwise, we use best case and worst case.

*1. Best Case: already sorted*
$t_j=1$ because it is number of times we were evaluating the while loop.
With this fixed input, $$\begin{align}
T(n)&=3C\sum t_j+4Cn-3C \\
&=3Cn+4Cn-3C \\
&=7Cn-3C
\end{align}$$This is a linear function, so the running time of the function is linear.
**Linear time** are really good, in sorting you cannot do better than linear time.
Of course, we must check the array at least once.

*2. Worst Case: reverse order*
> There could be many different worst cases, try to find the easiest

$t_j=j$ because every time we enter is $j-1+1$.
Now, we simplify the formula with the fixed $t_j$ $$\begin{align}
T(n)&=3C\sum t_j+4Cn-3C \\
&=3C\sum_{j=2}^nj+4Cn-3C \\
&=3C(\frac{(n+1)n}2-1)+4Cn-3C \\
&=\frac3 2C(n+1)n+4Cn-6C \\
&=\frac3 2Cn^2+\frac3 2Cn+4Cn-6C \\
&\approx\frac3 2Cn^2
\end{align}$$We used the formula for the sum of N natural numbers (called Gauss formula?)$$
\sum_{j=1}^nn=\frac{(n+1)n}2
$$And selected the dominant term in the last step, because we are interested in large input sizes because those are the inputs that can make the algorithm unfeasible. This means that the algorithm runs **Quadratic Time**.

Now, we can compare algorithms, without using many different constants for being precise. It was impossible.

*A better approach*
Is there an easier way avoiding all these calculations?
Yes.

## Order of growth
Large enough input means $$
\lim_{n\rightarrow\infty}f(n)
$$*Idea*.
1. forget about lower order terms
2. forget about constants factors
It means that we can do $$\begin{align}
T(n)&=\frac3 2Cn^2+\frac3 2Cn+4Cn-6C \\
&\approx\frac3 2Cn^2 \\
&\approx n^2
\end{align}$$In every algorithm we use a different factor.
This is enough most of the time to compare efficient vs inefficient algorithms.
This is true for large enough input size.
For example, every quadratic algo is slower than any linear algo.

We will use the *notation*$$\begin{align}
T(n)&=3C\bigg(\frac{(n+1)n}2-1\bigg)+4Cn-3C \\
&=\Theta(n^2)
\end{align}$$
## Some examples
Most useful functions in growing order.$$\begin{align}
&\Theta(1) \\
&\Theta(log\ n)=100\ log\ n+3 \\
&\Theta(n\ log\ n) \\
&\Theta(n^2) \\
&\Theta(n^3)
\end{align}$$It is not possible to sort an array in $\Theta(log\ n)$ because it is less than $\Theta(n)$ which is the minimum to check every element of the array.

If you compare two linear time algos, it might be useful to compare also constant factors.

*Time considerations*
If you have logn, it means that there was a preprocessing on the input data.
If you have a linear time algo, it is a super good algo that will finish pretty soon.
If you have nlogn, it will be fast, because logn is small.
NOTE that a nlogn might be faster than n because the constant factor in the linear can be bigger than the (usually) small value of logn.
If you have quadratic, it might take years if n is bigger than one million.
If you have a cubic, it will work only for small n.


> Try to analyze insertion sort by yourself



--27.02--
1. analyzing selection sort
2. designing algorithm
3. optimal sorting algorithm


Scanning an array takes $\Theta(n)$ time.
There are two possibilities:
1. scan over elements
2. scan over indexes (better because more general)

If the goal is to compute the sum(A) where `A[1,n]`
```pseudocode
s=0
for e in A: # c(n+1)
	s += e # c(n)
```
$T(n)=2cn+c=\Theta(n)$

In a simpler way.
1. number of time we iterate
2. cost of each iteration
Because we need only the bigger term.

$Log\ n$ is usually around 32.
$n^2$ is really slow already at 100,000
$n^3$ usually unfeasible, unless very efficient for most of the inputs.


## Analyzing Selection Sort 

The idea is check for the smallest and move it in front of the array.
Now the prefix of the array contains the sorted array.

At each iteration i, find the min in the suffix `A[i,n]` and say its position is j.
Swap `A[i]` and `A[j]`.

```pseudocode
SelectionSort(A):
	for i: 1 to A.length
		min_pos = i
		for j: i+1 to A.length
			if A[j] < A[min_pos]
				min_pos = j
		swap(A[i], A[min_pos])
```
Why computing the min in this way? Because I need the index and because I want to inizialize it with an element of the array.

Properties we want
1. inplace
	1. we are sorting the array without using extra space 
2. stable
	1. if two identical values keep the order in the final array
Selection sort is inplace, but NOT stable.
To prove that is not stable is enough to provide a counterexample like `A=[551]`.

How to check correctness?
Carefully design a set of test. Marginal cases included.

Let's analyze the complexity of Selection Sort
```pseudocode
SelectionSort(A):
	for i: 1 to A.length // c(n+1)
		min_pos = i // c(n)
		for j: i+1 to A.length // sum n times of c(n-i) 
			if A[j] < A[min_pos] // sum n times of c(n-i-1)
				min_pos = j // cost dominated by if
		swap(A[i], A[min_pos]) // c(n)
```

$$T(n)=3cn+2\sum_{i=1}^N c(n-1)=n+\frac{n(n-1)}2=n^2+... \Longrightarrow\Theta(n^2)$$
The running time is quadratic regarding the input. So, it is both best and worst case.
Why? There is nothing depending on the input in the final formula (i disappeared).
This is very bad. No matter what, the time is very slow.
While for insertion sort, the best case was linear (or close to linear).
When the input is quite small or the input is almost sort, even python libraries use insertion sort. Nobody uses selection sort.

## Shorthands for analyzing
The hard part is to analyze loops.
Be careful with the difference between parallel and sequential loops.
The trick is starting from the most inner thing and then go to the outer.

If the inner loop is `for j: i to n` it is not better than `1 to n`, it is still quadratic because of the gauss formula. See Selection Sort analysis.

> The most important thing of the course is to avoid parallel loops.

## Exercise
Write a function to compute the distinct elements in a list A.
```python
def DistinctPythonic(A):
	list(set(A)) # this is the most pythonic way

def Distinct(A):
	B=[]
	for e in A:
		if e not in B: # here there is an hidden loop
			B.append(e) 
	return B
```
$$T(n)=\Theta(n^2)$$
Solve the same assuming A is sorted.
```python
def Distinct(A): # there is a bug
	B=[]
	for e in A:
		if e not in B[-1]:
			B.append(e) 
	return B
```
It is enough to check the last element of B because B is sorted.
Now, the running time is linear.
But append does not always cost constant time. It is not fully correct.



