 UniversitÃ  di Pisa - Registro Lezioni

Pagina principale | Registri delle lezioni per anno accademico | Login
Antonio Frangioni

Registri a.a. 2024/2025

Dati registro

insegnamento
    OPTIMIZATION FOR DATA SCIENCE (cod. 784AA) 
corso di studi
    WDB-LM - DATA SCIENCE AND BUSINESS INFORMATICS 
periodo
    Primo semestre
responsabile
    ANTONIO FRANGIONI
docenti
    ANTONIO FRANGIONI 
totale ore
    46
    ( Lezione: 30 ore , Esercitazione: 16 ore ) 

    Dettaglio ore

Lezioni

Mar 17/09/2024 16:00-17:00 (1:0 h) lezione: A brief introduction to the course: contents, structure, grading (the two tracks). (ANTONIO FRANGIONI)
Mar 17/09/2024 17:00-18:00 (1:0 h) esercitazione: Learning as a mathematical process = creating a (simple) model of a (complex) process = fitting a function. Didactic example: estimating the hourly production of an energy community. Polynomial interpolation, "because c = y / X works". All the rest on top of polynomial interpolation: estimating the quality of the result via k-fold cross validation. The bias / variance dilemma, all the corresponding idea, and why this is *not* what this course is about. This is what you'll see in DM, BDA and ML and *one* of the fundamental reasons for this course, but *not* what this course is about. What this course *is* about: optimization. The questions we will be interested in. The two roles of optimization in Data Science: as a servant, and as a master. (ANTONIO FRANGIONI)
Ven 20/09/2024 09:00-10:00 (1:0 h) lezione: On the other role of optimization in data science: using the fitted function to construct a (simple, but combinatorial) analytical model of optimal sizing of a battery in an energy community. The many reasons why it is approximate (but you have to start somewhere). The mathematical description: variables, constraints, objective. Tricks of the trade: "steady-state" daily operations (energy at the beginning = energy at the end), variable upper bounds with the design variable. In the end: a Mixed-Integer Linear Problem. Putting the process in perspective: similarities and differences between analytic and synthetic models, a plea for the pacific co-existence of both. (ANTONIO FRANGIONI)
Ven 20/09/2024 10:00-11:00 (1:0 h) esercitazione: Solving the design problem with a modelling language and general-purpose solvers: still quite fast. Analysing the effect of having "good" or "bad" data on the decisions. Conclusion: you need a good data analysis phase, but then you may need to optimize upon it. Pointers to the follow-up courses (Model-Driven Decision-Making methods). (ANTONIO FRANGIONI)
Mar 24/09/2024 16:00-18:00 (2:0 h) lezione: General optimization problems: objective function, feasible region, optimal solution, optimal value. Problems with "no solution", how to deal with them (approximated solutions). Issue: optimization is impossible / very hard unless the problem is very kind on you and/or is very small. "Know your enemy" and "choose wisely the battles you fight". Starting (too) simple: optimising univariate linear and quadratic functions, possibly on an interval. An excessively involved proof of optimality conditions for the nonhomogeneous univariate quadratic case (but a sign of things to come). First words about multivariate optimization: be aware that \R^n gets "very large" as n grows. (ANTONIO FRANGIONI)
Ven 27/09/2024 09:00-11:00 (2:0 h) lezione: Preliminaries to (unconstrained) multivariate optimization, the necessary concepts in \R^n: vector space, scalar product, norm, distance, matrices, matrix-vector products. An aside: vector-valued optimization and why we *don't* do it (save via tricks). Picturing functions in \R^n: graph, level sets and tomography. Graph, level sets and tomography of multivariate linear functions. When multivariate optimization is easy: the separable cases. The linear case is always separable, but the quadratic one may not be. Characterising graphs and level sets of quadratic (homogeneous) functions out of the spectral decomposition of Q: the several different cases. Consequence: characterising when a quadratic (homogeneous) function has minimum and maximum and where it lies. Extension to the box-constrained case and why it is *not* a good idea. (ANTONIO FRANGIONI)
Mar 01/10/2024 16:00-17:00 (1:0 h) lezione: Computing the "center" of the level sets of a quadratic non-homogeneous function: the simple (nonsingular) case and the less-than simple (singular) one. All in all: our first example of complete optimality conditions for a family of (simple) optimization problems. From the optimality condition to the gradient method: the important property (the anti-gradient is a descent direction). Theory breeds algorithms: the gradient method for quadratic functions, definition and discussion. (ANTONIO FRANGIONI)
Mar 01/10/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of the gradient method for quadratic functions. Running the code on different test instances, observing the properties: convergence rate dependent on shape of ellipsoids. (ANTONIO FRANGIONI)
Ven 04/10/2024 09:00-10:00 (1:0 h) lezione: More about the gradient method for quadratic functions. Our first convergence and efficiency proofs, what it reliably tells and what it does not (only a bound on the convergence speed, so the practice may be different). Naming names: convergence rates (linear, sublinear, superlinear), what they mean, what they look like on the log-linear plot. This is not empty theory: the case of \lambda_n = 0 and our first sublinear convergence result. The first case of what is coming: (much) different convergence properties for (apparently minor) differences in the properties of the problem. Cautionary tales about the stopping criteria: different tolerances for input and output error, what the norm of the gradient really measures. (ANTONIO FRANGIONI)
Ven 04/10/2024 10:00-11:00 (1:0 h) esercitazione: Running the MATLAB code "with the lenses of the convergence rate": log-linear convergence plots, checking that the rate respects the bound, dependence on the starting point (it can go much faster than the bound), the effect of eccentricity. Crossing a boundary: running on a semidefinite case and seeing sublinear convergence "in the flesh". (ANTONIO FRANGIONI)
Mar 08/10/2024 16:00-17:00 (1:0 h) lezione: Towards faster algorithms. One useful trick: warping the space. Orthogonal directions in the warped space become Q-conjugate directions in the original one. Proving the power of Q-conjugate directions: n steps always suffice (in exact arithmetic). The Conjugate Gradient Method: pseudocode and discussion. Theoretical results: linear convergence, vague hints to the results depending on multiplicity and clustering of eigenvalues. Improving the Conjugate Gradient Method: "forcing the eigenvalues to cluster" = another example of "space warping" = Preconditioning. The simplest preconditioner: diagonal. Vague hints to extensions (extracting an easily invertible sparse sub-matrix, incomplete Cholesky, exploiting the structure). (ANTONIO FRANGIONI)
Mar 08/10/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of the Conjugate Gradient Method. Practical speed on a non-completely-toy example, the good and the bad. Theoretical results: linear convergence, vague hints to the results depending on multiplicity and clustering of eigenvalues. Improving the Conjugate Gradient Method: "forcing the eigenvalues to cluster" = another example of "space warping" = Preconditioning. The simplest preconditioner: diagonal. Vague hints to extensions (extracting an easily invertible sparse sub-matrix, incomplete Cholesky, exploiting the structure). Practical effect of the diagonal preconditioner: the good (very effective on highly diagonal predominant matrices) and the bad (useless otherwise). (ANTONIO FRANGIONI)
Mar 15/10/2024 16:00-17:00 (1:0 h) lezione: Last step in solving quadratic problems: using the heavy guns = directly solving the system. "Non-symmetric perfect preconditioner = Cholesky factorization", because "triangular matrices are simple". The algorithmic side: backsolve (upper and lower triangular), the Cholesky factorization (using backsolves at each step). Executing backsolve and Cholesky factorization by hand on small examples. Final words on "simple" problems (why they are useful), time to move to "the simplest of non-simple ones". (ANTONIO FRANGIONI)
Mar 15/10/2024 17:00-18:00 (1:0 h) lezione: Towards more general optimization algorithms: "oracles" for "black box" functions. Yet, even restricted to a single variable optimization is impossible: the objective can be an adversarial function. Issue: optimization is impossible / very hard unless the problem is very kind on you and/or is very small. "Know your enemy" and "choose wisely the battles you fight". Imposing conditions on the objective and the constraints makes it possible: the "obvious" algorithm and its (easy) correctness proof. But global optimization still is "too hard" (even in one variable, not to mention many). (ANTONIO FRANGIONI)
Ven 18/10/2024 09:00-10:00 (1:0 h) lezione: Local optimization to the rescue: local minima, attraction basins, unimodal functions. Fast (local, univariate) optimization requires more information: first and second derivatives, the first-order model. Continuous differentiability (the C^1 and C^2 classes) and Lipschiz continuity. Stationary points (maxima, minima, saddle points). Local minima ==> stationary point (but <== not true). Finding a root of the derivative via the mean value theorem = the Dichotomic Search algorithm. (ANTONIO FRANGIONI)
Ven 18/10/2024 10:00-11:00 (1:0 h) esercitazione: Computing derivatives is easy (because someone else does it for us): hands-on with symbolic systems, pointers to Automatic Differentiation tools. Hands-on with the Dichotomic Search algorithm: it really has linear convergence. (ANTONIO FRANGIONI)
Mar 22/10/2024 16:00-17:00 (1:0 h) lezione: Fast can be faster: derivatives can be used, or used really well. First example of model-based search: quadratic interpolation. What went wrong with quadratic interpolation: the model can be wrong, hence never unconditionally trust it. "regularization" = safeguarding. Fastest (univariate) optimization requires second derivatives: Newton's Method, the (very) good and the (rather) bad. A quick look at the convergence proof of Newton's method, why it matters. A nod towards global optimization, a quick introduction to (univariate) convex functions, convex ==> local == global, an hand-waving introduction to global optimization algorithms. Closing thoughts on univariate optimization. (ANTONIO FRANGIONI)
Mar 22/10/2024 17:00-18:00 (1:0 h) esercitazione: Hands-on with the Dichotomic Search algorithm with quadratic interpolation, what goes very well and what can go wrong. Hands-on with the complete version of Dichotomic Search (quadratic interpolation + safeguarding), and it really is faster (with the right safeguard). Hands-on with Newton's Method, different convergence (but always fast "in the tail" depending on the starting point). The role of the second-order model, that can be very bad (but when it is good, is very good). (ANTONIO FRANGIONI)
Ven 25/10/2024 09:00-10:00 (1:0 h) esercitazione: Towards unconstrained multivariate optimization, motivation: Neural Networks, how they are done, what their tomographies look like (nothing like convex, unless perhaps in the large scale). The good part: computing the gradient is still easy in practice, although it's a vector. Computing directional derivatives of simple functions with the symbolic package, computing the gradient of any test function (comprised the Neural Network) with ADGator, what back propagation would be (but we don't see it). (ANTONIO FRANGIONI)
Ven 25/10/2024 10:00-11:00 (1:0 h) lezione: Why global optimization is much harder in \R^n (because there is a lot more ground to cover), the curse of dimensionality. What local multivariate optimization will give us (if we are lucky) instead. Some necessary concepts in \R^n: topology, limits, continuity. Derivatives in R^n: partial derivatives, directional derivatives, gradient. The scary part: in (the vector) space (\R^n), no-one can hear you scream (when you discover derivatives are in fact vectors). Differentiability and the multivariate first-order model. Geometric interpretation of the gradient with sublevel sets. Towards the theoretical results: the many ways in which a function can be non differentiable. Jacobians and Hessians. (ANTONIO FRANGIONI)
Mar 29/10/2024 16:00-17:00 (1:0 h) lezione: Why do we care about the gradient: directions of descent. First-order local optimality conditions. The proof of a theorem, for the once, because theorems breed algorithms: the steepest descent direction. Second-order local optimality conditions: a glimpse of the proof for another important concept (directions of negative curvature). Towards global optimization: multivariate convex functions, convexity and continuity, first- and second-order order conditions for convexity, the "grammar of convexity". Quasi-convex functions and why they are harder to construct. (ANTONIO FRANGIONI)
Mar 29/10/2024 17:00-18:00 (1:0 h) lezione: Multivariate unconstrained optimization algorithms. Choice of the direction: the obvious one = steepest descent ==> the gradient method for general nonlinear functions. The crucial part: stepsize selection. How *not* to choose a stepsize, the search for the Goldielocks step. Fixed stepsize: why it might work (the gradient is hopefully going to zero), what conditions are needed for it to really do. MATLAB implementation of the gradient method for nonlinear functions (and fixed stepsize, for the moment). Example with the Neural Network: fixed stepsize may work, but it's very delicate and the results are not great anyway. A sneak peek: this is not because of the NN, but because the optimization is not effective: what you get with a better (unspecified) method. (ANTONIO FRANGIONI)
Mar 05/11/2024 16:00-17:00 (1:0 h) lezione: Why fixed stepsize can be possible, what you need for that. Lipschitz continuity of the function and the gradient (L-smoothness), relationships with gradient / Hessian boundedness. The result: fixed stepsize is possible with 1 / L ... if one knows L. Convergence and efficiency results: sublinear, which should be expected because "\lambda_n = 0" may happen. (ANTONIO FRANGIONI)
Mar 05/11/2024 17:00-18:00 (1:0 h) lezione: To be "fast" you also need \tau-convexity. The proof of linear convergence because it shows where the optimal stepsize comes from. Back to MATLAB implementation of the gradient method for quadratic functions, activating the Fixed Stepsize option: works as advertised, but (exact) Line Search is better. Towards dynamic stepsizes: convergence analysis for approximate line search to an approximate stationary point. Recall: stopping conditions, what you would like to have is (often) what you can have. (ANTONIO FRANGIONI)
Ven 08/11/2024 09:00-10:00 (1:0 h) lezione: Inexact line search: Armijo-Wolfe conditions, what they mean, why they work. Theory (points satisfying the A-W conditions always exist, L-smoothness then gives convergence) and practice (how to find them). The theoretical side: efficiency estimates for the general case (basically the same as for the quadratic case with exact line search, close to with inexact one). Take away: a good inexact LS can improve the practical behaviour of the method, even counting function evaluations, insomuch as the steepest descent direction allows ... which is not too much. (ANTONIO FRANGIONI)
Ven 08/11/2024 10:00-11:00 (1:0 h) esercitazione: MATLAB implementation of the gradient method for nonlinear functions. Testing on a "nasty" function, where the gradient is slow: nonetheless, the LS is remarkably efficient especially if the initial stepsize is chosen well. Tuning the other parameters and why it is not important. Testing on "nicer" functions with well-conditioned Hessians, and where the gradient direction is good the Armijo-Wolfe line search rounds the circle to an overall effective algorithm. Being "not too smart" can even be an advantage since it helps not to be trapped on shallow local minima. But no line search will ever make the algorithm efficient in the ill-conditioned case, so something different will have to be done. (ANTONIO FRANGIONI)
Mar 12/11/2024 16:00-17:00 (1:0 h) lezione: The next step: changing the direction ... and there are very many to choose from: the Twisted Gradient Method, Zoutendijk's result. The "most obvious" one: Newton's method. Basic ideas for the (strongly) convex case. Convergence properties, expected behaviour, interpretations (linearization of the nonlinear system on the gradient). Newton's method for the nonconvex case: Hessian modifications. A glimpse to trust region: changing everything for not changing much. Take away: you cannot work with the true Hessian anyway, but you also don't need to, which paves the way to quasi-Newton methods. Initial presentation of the quasi-Newton theory, the secant condition and the minimization problem: the DFG formulae. (ANTONIO FRANGIONI)
Mar 12/11/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of Newton's method (basic version), first look at the behaviour on some test cases. Take away: convergence is very good even far from the local minimum and indeed exceptionally good "in the tail". However, there can be too much of a good thing: Newton's method can converge to the closer, but not to the better, local minimum. A "nasty" application on a Neural Network, better than Gradient, but iterations are very slow. Hessian modifications, the role of the \delta parameter: almost always (but not always) 0 for "simple" functions, but rather large for the NN. (ANTONIO FRANGIONI)
Ven 15/11/2024 09:00-10:00 (1:0 h) lezione: Quasi-Newton methods: the details of DFG, then BFGS. Towards the very-large-scale: limited-memory BFGS. Towards the very-very-large scale: deflecting as the alternative to twisting, introduction to the Conjugate gradient for nonlinear functions. (ANTONIO FRANGIONI)
Ven 15/11/2024 10:00-11:00 (1:0 h) esercitazione: MATLAB code for BFGS; same as Newton's + black magic. Behaviour on relevant test cases. Quasi-Newton methods in practice: almost as fast a Newton's method at lesser cost, possibly even less prone to fall into local minima. BFGS for Neural Network: even better than Newton, the effect of stabilization (win-win: better predictions and better convergence since the function is "more \tau-convex"). (ANTONIO FRANGIONI)
Mar 19/11/2024 16:00-17:00 (1:0 h) lezione: Conjugate gradient for nonlinear functions: different beta formulae, theoretical results (sketch). "Poorman's" deflection-type methods: heavy ball gradient. A hint at the (overly complex) convergence theory of the heavy ball gradient, what the results are: faster than gradient with the right alpha and beta (and it typically is), nonmonotone at the beginning but close so at the end (and it typically is). A quick hint at accelerated gradients, why they may matter (or not). I love heavy ball / ACCG because it's an arrow shot between Newton's and the subgradient, an arrow shot across the abyss (of nondifferetiability). (ANTONIO FRANGIONI)
Mar 19/11/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of the conjugate gradient, impact of algorithmic parameters (the beta formula and the restarts), behaviour on relevant test cases (quadratic and not). Take away: better than gradient, but do not expect Newton-type convergence. "Poorman's" deflection-type methods: heavy ball gradient. MATLAB implementation of the heavy ball gradient. Impact of algorithmic parameters: alpha is the usual mess, but beta is also important. The lucky (quadratic case) when the optimal alpha and beta are known, and they really work. Take away: better than gradient with the right alpha and beta, but not really fast. More examples of heavy ball: worse conditioned quadratics, Neural Networks (here the starting point counts as well). (ANTONIO FRANGIONI)
Ven 22/11/2024 09:00-10:00 (1:0 h) lezione: Towards less-than-gradient methods, motivation: incremental approaches (the gradients exist but is to costly, so it is approximated), Lasso regularization, Support Vector Machine / Regression. Subgradients and subdifferentials (in \R^{n + 1} and \R^n), the good and the bad. Several crucial properties are lost, but some are kept (mostly due to convexity) and they can be used to construct descent methods. The fundamental property of subgradient (they "tell where the optimum is"). Why descent methods don't work in the nondifferentiable case. Complexity results for nondifferentiable optimization. Negative results for nondifferentiable optimization: fixed stepsize just won't work. Wrap up: why nondifferentiable optimization is (much) harder than differentiable optimization. Subgradient methods. Basic ideas, DSS stepsizes. (ANTONIO FRANGIONI)
Ven 22/11/2024 10:00-11:00 (1:0 h) esercitazione: MATLAB implementation of a (very simple) nondifferentiable function. Testing all smooth methods seen so far on it and seeing all them break: they all fail differently, but fail they do. MATLAB implementation of the subgradient method: DSS stepsize. Behaviour in practice. Take away: with the right parameter tuning you can do it, but it's not fast and you don't really know when to stop. (ANTONIO FRANGIONI)
Mar 26/11/2024 16:00-17:00 (1:0 h) lezione: More on subgradient methods: Polyak stepsizes, efficiency estimates, target-level stepsize. Better nondifferentiable optimization methods in theory (and, relatively speaking, in practice): smoothed gradient method ... if you can cheat. Truly better methods in practice; the cutting plane method and its variants (Bundle methods). (ANTONIO FRANGIONI)
Mar 26/11/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of the subgradient method: Polyak stepsize, and it somehow helps ... but not too much. MATLAB implementation of the proximal bundle method: accruing information does work (much faster convergence speed and effective stopping criterion) ... if you can solve the master problem. (ANTONIO FRANGIONI)
Mar 03/12/2024 16:00-18:00 (2:0 h) lezione: Generalities on constrained optimization (feasible and unfeasible solutions, global and local optima, emptiness). Optimality conditions for general constrained optimization: the difference between boundary and interior. Tangent cone, cone of feasible directions, necessary and sufficient local optimality conditions geometrically. Global optimality requires (as usual) convexity: convex sets, convexity and global optimality. Verifying the Tangent Cone Condition depends on how the feasible set is specified. Representing sets via functions: the general case, a glimpse of reformulations. Constraints and convexity. Constraints via functions, the linear case: how a polyhedron looks like. (ANTONIO FRANGIONI)
Ven 06/12/2024 09:00-11:00 (2:0 h) lezione: Algebraic version of optimality conditions: the case of equality constraints. Take away: a strange new set of (unconstrained) multipliers enters into play. The inequality constraints case: cone of first-order feasible directions, constraint qualifications. First-order optimality conditions: Farkas' Lemma, KKT (what complementary slackness means). Take away: optimization is solving (nonlinear) systems of equalities and inequalities. A glimpse to second-order optimality conditions. From the Lagrangian function to the dual function: Lagrangian duality. Duality and convexity, economic interpretation of the dual solution. The special cases of linear, quadratic and conic (SOCP, SDP) problems. Two important application of duality: the dual of the proximal bundle master problem (constructing the primal optimal solution even when unicity does not hold), the dual formulations of SVM/SVR and the kernel trick. Duality in action: MATLAB implementation of SVR and the (Gaussian) kernel trick. (ANTONIO FRANGIONI)
Gio 12/12/2024 09:00-10:00 (1:0 h) lezione: Constrained optimization algorithms. The linearly constrained quadratic case = (structured) linear algebra (a hint to reduced KKT vs Null Space methods). The Active-set method: general case, convergence properties. Specialization of the Active-Set method to the box-constrained case: what "\mu < 0" becomes, how to exploit all the structure. Again "heavy guns" vs "quick & dirty", "conjugate gradient" vs "gradient": the projected gradient method. Specialization for the box-constrained case. (ANTONIO FRANGIONI)
Gio 12/12/2024 10:00-11:00 (1:0 h) esercitazione: Generating "interesting" box-constrained convex Quadratic Programs. MATLAB implementation of the Active-Set method for box-constrained convex Quadratic Programs, its behaviour on instances of different type (effect of active set size and eccentricity). MATLAB implementation of the projected gradient method (standard) and for box-constrained convex Quadratic Programs, their behaviour on instances of different type. Take away: different methods have very different trade-offs and are best on different instances. (ANTONIO FRANGIONI)
Lun 16/12/2024 16:00-17:00 (1:0 h) lezione: The Frank-Wolfe method, with stabilization. A fleeting glimpse to extensions of the Frank-Wolfe method: Sequential Quadratic Programming (second-order models), constrained Bundle method for non-C^1 functions (cutting plane model). Dual methods, the easy case (quadratic with strictly positive definite Hessian). Primal-dual interior-point methods for linear and quadratic problems: motivating idea (barrier function and the central path), KKT systems, linear algebra aspects, structured matrices. Wrap-up of the course. (ANTONIO FRANGIONI)
Lun 16/12/2024 17:00-18:00 (1:0 h) esercitazione: MATLAB implementation of the Frank-Wolfe method: a disaster without stabilisation, a bit better with it (but it depends a lot on the instance). MATLAB implementation of the dual method box-constrained convex Quadratic Programs: it works quite well, but it also depends on the instance (ill conditioning and size of active set) and it is especially because it has the Lagrangian heuristic. MATLAB implementation of the interior-point method for box-constrained convex Quadratic Programs: the Ferrari of constrained optimization (fast but costly). Comparison between the all the presented methods on a large instance: many different trade-offs. Conclusions. (ANTONIO FRANGIONI)


