(following the wikipedia rabbit hole, mainly [here](https://en.wikipedia.org/wiki/Numerical_linear_algebra))


Calculations that can be proven not to magnify approximation errors are called _numerically stable_.

In the [mathematical](https://en.wikipedia.org/wiki/Mathematics) discipline of [linear algebra](https://en.wikipedia.org/wiki/Linear_algebra), a **matrix decomposition** or **matrix factorization** is a [factorization](https://en.wikipedia.org/wiki/Factorization) of a [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.

In [numerical analysis](https://en.wikipedia.org/wiki/Numerical_analysis "Numerical analysis"), different decompositions are used to implement efficient matrix [algorithms](https://en.wikipedia.org/wiki/Algorithm "Algorithm").
For instance, when solving a [system of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations "System of linear equations") Ax=b![{\displaystyle A\mathbf {x} =\mathbf {b} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/45d894430af69e29d6dda5aacbf4bb19336226a0), the matrix _A_ can be decomposed via the [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition "LU decomposition"). The LU decomposition factorizes a matrix into a [lower triangular matrix](https://en.wikipedia.org/wiki/Lower_triangular_matrix "Lower triangular matrix") _L_ and an [upper triangular matrix](https://en.wikipedia.org/wiki/Upper_triangular_matrix "Upper triangular matrix") _U_. The systems L(Ux)=b![{\displaystyle L(U\mathbf {x} )=\mathbf {b} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/e55c9cfdfe772f24e24bb4ba0307adc167681d77) and Ux=L−1b![{\displaystyle U\mathbf {x} =L^{-1}\mathbf {b} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/254570218cd039cd14aaf3bb030de8a21e8124c4) require fewer additions and multiplications to solve, compared with the original system Ax=b![{\displaystyle A\mathbf {x} =\mathbf {b} }](https://wikimedia.org/api/rest_v1/media/math/render/svg/45d894430af69e29d6dda5aacbf4bb19336226a0), though one might require significantly more digits in inexact arithmetic such as [floating point](https://en.wikipedia.org/wiki/Floating_point "Floating point").

While systems of three or four equations can be readily solved by hand (see [Cracovian](https://en.wikipedia.org/wiki/Cracovian "Cracovian")), computers are often used for larger systems. Small systems are solved with algorithms such as variable substitution, Gaussian elimination, Cramer's rule, inverse of the matrix. Larger systems are computed with machines with different methods that are now described.

Firstly, it is essential to avoid division by small numbers, which may lead to inaccurate results. This can be done by reordering the equations if necessary, a process known as [_pivoting_](https://en.wikipedia.org/wiki/Pivot_element "Pivot element"). Secondly, the algorithm does not exactly do Gaussian elimination, but it computes the [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition "LU decomposition") of the matrix _A_. This is mostly an organizational tool, but it is much quicker if one has to solve several systems with the same matrix _A_ but different vectors **b**.

If the matrix _A_ has some special structure, this can be exploited to obtain faster or more accurate algorithms. For instance, systems with a [symmetric](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix") [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix "Positive-definite matrix") matrix can be solved twice as fast with the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition "Cholesky decomposition"). [Levinson recursion](https://en.wikipedia.org/wiki/Levinson_recursion "Levinson recursion") is a fast method for [Toeplitz matrices](https://en.wikipedia.org/wiki/Toeplitz_matrix "Toeplitz matrix"). Special methods exist also for matrices with many zero elements (so-called [sparse matrices](https://en.wikipedia.org/wiki/Sparse_matrix "Sparse matrix")), which appear often in applications.

Main Decomposition methods related to solving systems of linear equations:
1. LU Decompostion
	1. Traditionally applicable to: square matrix _A_, although rectangular matrices can be applicable.
	2. Decomposition: $\displaystyle A=LU$, where _L_ is lower triangular and _U_ is upper triangular
2. Cholesky Factorization
	1. Applicable to: [square](https://en.wikipedia.org/wiki/Square_matrix "Square matrix"), [hermitian](https://en.wikipedia.org/wiki/Symmetric_matrix "Symmetric matrix"), [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix "Positive-definite matrix") matrix A![{\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3)
	2. Decomposition: ${\displaystyle A=U^{*}U}$ where U is upper triangular with real positive diagonal entries

A completely different approach is often taken for very large systems, which would otherwise take too much time or memory. The idea is to start with an initial approximation to the solution (which does not have to be accurate at all), and to change this approximation in several steps to bring it closer to the true solution. Once the approximation is sufficiently accurate, this is taken to be the solution to the system. This leads to the class of [iterative methods](https://en.wikipedia.org/wiki/Iterative_method "Iterative method"). For some sparse matrices, the introduction of randomness improves the speed of the iterative methods.[[10]](https://en.wikipedia.org/wiki/System_of_linear_equations#cite_note-10) One example of an iterative method is the [Jacobi method](https://en.wikipedia.org/wiki/Jacobi_method "Jacobi method"), where the matrix A![{\displaystyle A}](https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3) is split into its diagonal component D![{\displaystyle D}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f34a0c600395e5d4345287e21fb26efd386990e6) and its non-diagonal component L+U![{\displaystyle L+U}](https://wikimedia.org/api/rest_v1/media/math/render/svg/f553df6d71bde4d8c86baa5ffe6705a412e86e95). An initial guess x(0)![{\displaystyle {\mathbf {x}}^{(0)}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/a6ebbfa25788095285f0675b14021cdb6b95ea6c) is used at the start of the algorithm. Each subsequent guess is computed using the iterative equation:
${\displaystyle {\mathbf {x}}^{(k+1)}=D^{-1}({\mathbf {b}}-(L+U){\mathbf {x}}^{(k)})}$
When the difference between guesses x(k)![{\displaystyle {\mathbf {x}}^{(k)}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/690e84b53186e51bc6432fb1024e0c3a3b6e5041) and x(k+1)![{\displaystyle {\mathbf {x}}^{(k+1)}}](https://wikimedia.org/api/rest_v1/media/math/render/svg/832f466e73981a249e41975aa77919734cfedd9d) is sufficiently small, the algorithm is said to have _converged_ on the solution.[[11]](https://en.wikipedia.org/wiki/System_of_linear_equations#cite_note-11)

> So there are two main approaches to solve large linear systems
> 1. decompositions (LU and Cholesky mainly)
> 2. iterative methods (Gradient and Conjugate Gradient)

